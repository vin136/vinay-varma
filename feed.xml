<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://vinayvarma.work/feed.xml" rel="self" type="application/atom+xml" /><link href="https://vinayvarma.work/" rel="alternate" type="text/html" /><updated>2022-01-11T13:54:16-06:00</updated><id>https://vinayvarma.work/feed.xml</id><title type="html">Home</title><subtitle>Thoughts on Science and Life.</subtitle><entry><title type="html">Linear Algebra - (Chapter 0.1)</title><link href="https://vinayvarma.work/research/2022/01/10/Linear-Algebra.html" rel="alternate" type="text/html" title="Linear Algebra -  (Chapter 0.1)" /><published>2022-01-10T00:00:00-06:00</published><updated>2022-01-10T00:00:00-06:00</updated><id>https://vinayvarma.work/research/2022/01/10/Linear-Algebra</id><content type="html" xml:base="https://vinayvarma.work/research/2022/01/10/Linear-Algebra.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/Linear Algebra.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;These are not meant to be used for self-teaching the subject. My objective is to collect the summary of all the ideas that a working &lt;code&gt;Machine Learning Researcher&lt;/code&gt; should be aware of. Use this as a reference to check or deepen your understanding. At the start of each post I will provide an opinionated learning strategy to teach yourself the subject from scratch in the &lt;strong&gt;shortest possible time&lt;/strong&gt;. In most cases the post is based off those resources, thus the credit is due to the orginal authors.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Pointers to learn from scratch (Go through these resources in the given order.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Read and workthrough the first 4 chapters of the book &lt;a href=&quot;https://mml-book.github.io/book/mml-book.pdf&quot;&gt;mml&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Watch through this series of videos by 3Blue1Brown - &lt;a href=&quot;https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&quot;&gt;Essence of linear algebra&lt;/a&gt; along the way.&lt;/p&gt;
&lt;p&gt;For all practical purposes this should suffice. But, I would also suggest to read through &lt;a href=&quot;https://linear.axler.net&quot;&gt;Linear Algebra Done Right&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Central-Objects-:-Vectors-and-linear-maps&quot;&gt;Central Objects : Vectors and linear maps&lt;a class=&quot;anchor-link&quot; href=&quot;#Central-Objects-:-Vectors-and-linear-maps&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we will define the lead actors - vectors(objects of study) and linear maps(mappings between these objects)&lt;/p&gt;
&lt;p&gt;Vector Space : Set of all objects called &lt;strong&gt;vectors&lt;/strong&gt; that satisfy some intuitive constraints. Consider the following vector spaces.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la2.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Note the commonality between the following objects. For illustration let's use geometric vectors, that can be seen as pointed arrows within a coordinate axis, represented as a list of numbers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la1.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;
&lt;p&gt;All these satisfy some intuitive properties once we define addition and scalar multiplication. Let's call all such objects &lt;strong&gt;Vectors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la3.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Linear Algebra studies the emergent properties of objects once we impose simple properties like commutitivity,associativity etc. Note that we are just formalizing the familiar notions. This helps us discover some very interesting properties as we shall see.&lt;/p&gt;
&lt;p&gt;In short, we introduced the notion of &lt;code&gt;vector space&lt;/code&gt; which consists of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A set &lt;code&gt;X&lt;/code&gt; of vectors&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A set &lt;code&gt;C&lt;/code&gt; of scalars&lt;/p&gt;
&lt;p&gt;such that vectors are closed that are closed under addition,subtraction and scalar multiplication and satisfies natural associative and distributive laws.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Linear Maps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Linear Maps are the functions that map between the elements of the vector space. But not all mappings, only those that satisfy linearity conditions as outlined below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la4.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now,If you have studied &lt;strong&gt;Linear Algebra&lt;/strong&gt;, you might have jumped directly at &lt;strong&gt;geometric vectors&lt;/strong&gt; and &lt;strong&gt;matrices&lt;/strong&gt;. Below we will take a slightly more abstract approach that will pay dividends when we study arguably the most used decorated area the subject &lt;strong&gt;Matrix Decompositions - SVD and Eigen Value decomposition.&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Idea-1:-Spanning-Vectors&quot;&gt;Idea 1: Spanning Vectors&lt;a class=&quot;anchor-link&quot; href=&quot;#Idea-1:-Spanning-Vectors&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Big Idea 1&lt;/strong&gt;: All the elements of a vector space can be represented succintly as a linear combination of few  a &lt;strong&gt;vectors&lt;/strong&gt;. In other words,Irrespective of the vector space (geometric vectors,polynomials, functions etc), there exists a set of few vectors that can be used to generate all the elements of that set. This section builds up the material to prove this fact.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Subspace&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There exist a subset of the vector space that also satisfy the properties of the vector space. This will be more clear via examples below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la4.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M16 8.5l-6 6-3-3L8.5 10l1.5 1.5L14.5 7 16 8.5zM5.7 12.2l.8.8H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h7c.55 0 1 .45 1 1v6.5l-.8-.8c-.39-.39-1.03-.39-1.42 0L5.7 10.8a.996.996 0 000 1.41v-.01zM4 4h5V3H4v1zm0 2h5V5H4v1zm0 2h3V7H4v1zM3 9H2v1h1V9zm0-2H2v1h1V7zm0-2H2v1h1V5zm0-2H2v1h1V3z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;To check if a set forms a valid &lt;strong&gt;Subspace&lt;/strong&gt; check if the elements of the set are closed under a.&lt;code&gt;addition&lt;/code&gt; and b.&lt;code&gt;zero&lt;/code&gt; element belongs to the set c. closed under scalar multiplication. Rest of the properties(distributivity,associativity etc) are anyways satisfied by definition(as these are subset of the vector space).
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la5.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now it is sensible to ask whether by combining two subspaces(UNION,INTERSECTION) can we get another valid subspace. Convince yourself that the &lt;code&gt;INTERSECTION&lt;/code&gt; of two subspaces is always a subspace which is not the case for &lt;code&gt;UNION&lt;/code&gt;. If we define the notion &lt;code&gt;sum of two subspaces&lt;/code&gt; as the collection all elements that can be written as &lt;code&gt;sum of an element from each subspace&lt;/code&gt;, we note that this set is also a vector space thus a subspace of the original. Also this is the smallest subspace containing both the subspaces (just like how union of two sets in set theory is the smallest set containing both the sets).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la6.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/la7.jpg&quot; alt=&quot;&quot; title=&quot;Credit:&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 10 16&quot; version=&quot;1.1&quot; width=&quot;10&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10 7H6l3-7-9 9h4l-3 7 9-9z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;Understand the above claims before proceeding.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Research notebook</title><link href="https://vinayvarma.work/research/2021/11/01/WorkBook.html" rel="alternate" type="text/html" title="Research notebook" /><published>2021-11-01T00:00:00-05:00</published><updated>2021-11-01T00:00:00-05:00</updated><id>https://vinayvarma.work/research/2021/11/01/WorkBook</id><content type="html" xml:base="https://vinayvarma.work/research/2021/11/01/WorkBook.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-11-01-WorkBook.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;Note&lt;/code&gt; : The following is my working notes,only to aid my thiniking. Likely not useful for others.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;NeuroSymbolic-AI&quot;&gt;NeuroSymbolic AI&lt;a class=&quot;anchor-link&quot; href=&quot;#NeuroSymbolic-AI&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;explicit variable manipulation(local/discrete) vs distributed representations.
It is now accepted that learning takes place on a continuous search spaceof (sub)differentiable functions; reasoning takes place in general on a discretespace as in the case of goal-directed theorem proving&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Applications:&lt;/p&gt;
&lt;p&gt;Planning(RL)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A common thread across the above examples and applications is the needfor modellingcause and effectwith the use of implicit information&lt;/p&gt;
&lt;p&gt;Once a symbolic description of the formif A thenBhas been associated with a neural network, surely the idea ofintervention[9] and counterfactual reasoning become possible&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Lesson learnt while building a Deep Reinforcement learning based automated stock trading service.</title><link href="https://vinayvarma.work/research/2021/02/21/What-I-Learned-at-Niveshi.html" rel="alternate" type="text/html" title="Lesson learnt while building a Deep Reinforcement learning based automated stock trading service." /><published>2021-02-21T00:00:00-06:00</published><updated>2021-02-21T00:00:00-06:00</updated><id>https://vinayvarma.work/research/2021/02/21/What-I-Learned-at-Niveshi</id><content type="html" xml:base="https://vinayvarma.work/research/2021/02/21/What-I-Learned-at-Niveshi.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-21-What-I-Learned-at-Niveshi.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As of today, I have been working at &lt;code&gt;Niveshi&lt;/code&gt; for nearly two-and-half years. We Started out in late 2018, with nothing but a rough idea at best to automate &lt;code&gt;strategy generation&lt;/code&gt; using DeepRL. I will spare other details but I believe we have succeeded in building an automated pipeline from getting raw data to ready-to-deploy startegies. Here I'm logging some lossons learned along the way. These will be useful for folks trying to make research ideas in Artificial Intelligence work in the real world, especially Deep RL. I will take &lt;code&gt;stock trading&lt;/code&gt; as a running example,to illustrate some general principles. Some familiarity with RL is assumed.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;Objective&lt;/code&gt; : What are we trying to solve ?&lt;/p&gt;
&lt;p&gt;Find statistical regularities in the historical data and identify profitable trading rules. For simplicity, let's assume the data includes only historical price and volume. From RL perspective, given a set of time-series features learn to trade profitably. This amounts to outputting appropriate actions at every time-stamp, in order to maximize profits.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Usually techniques are sampled from traditional machine learning,signal processing and statistics. The recipe is as follows :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hire a bunch of smart folks. &lt;/li&gt;
&lt;li&gt;Give them the data and ask them to make money for the fund.&lt;/li&gt;
&lt;li&gt;Incentivize them appropriately. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apart from the laundry list of problems that come with managing people, this approach is quite costly, and doesn't scale well. Markets quickly absorb any obvious edges. Consequently,any successful pattern,however sophisticated, will have a short shelf-life. This keeps the team busy continuosly searching for novel ways to leverage the data and extract profitble stratagies. DeepRL, atleast in theory, promises to automate all this.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Notes to my-self&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;How to gaurentee success with Neural Networks ?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect Large , Clean and Varied ( interesting samples,edge cases) data.&lt;/li&gt;
&lt;li&gt;Train a large enough network neural network on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For whatever reason if your problem can't meet the above conditions, get ready for a painful ride.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;How to deal with less data ?&lt;/p&gt;
&lt;p&gt;signal = (amout of data) * (signal from each data point).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Roughly,the amount of signal provided per data-point follow &lt;code&gt;Self-supervision &amp;gt; Supervised &amp;gt; Reinforcement Learning&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Consider learning the structure of the data before attempting to formulate as the RL problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add auxillary losses. These might either fasten learning or can be used as standalone signals to improve the robustness of the base signal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you are limited by the scale of data, ensemble of simple architectures can beat the effort over novel architectures.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Don't Fiddle with network-architectures yet.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When You begin workin on a new problem, Start with the simplest yet bespoke architucture you can find. At the outset,you have too many variables to deal with and the &lt;code&gt;network-architecture&lt;/code&gt; is the least important of all.
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Best architecture is not only function of the domain but also the scale of data. The inductive bias of CNN&amp;#8217;s and RNN&amp;#8217;s might not be necessary or even optimal under the limit of infinite data. This also explains the recent surge in Multi-head Attention based architectures, which are closer to Multi-Layer Perceptrons than CNN&amp;#8217;s. These architectures work better with lots of data. Otherwise, you better stick with your CNN&amp;#8217;s.
&lt;/div&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Don't do AutoML.&lt;/p&gt;
&lt;p&gt;We have never done blind hyper-parameter search. Imagine working with a peculiar dataset where almost there is no useful published work to start with. And you just managed to train the model. How would you go from here to betting your capital on its outputs ?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ablation studies and Hypothesis-testing is the way to go.&lt;/p&gt;
&lt;p&gt;Based on our understanding of the algorithm and the domain we made hypotheses and tested them on at a time. Note that at this point we only managed to fit on &lt;code&gt;training data&lt;/code&gt;. Performance on out of sample is out of question. To give an example, We have spent inordinate amounts of time thinking and empirically testing out the effects of say changing any particular hyperparameter like &lt;code&gt;gamma&lt;/code&gt; or &lt;code&gt;entropy factor&lt;/code&gt; would effect some aspect of the resulting strategies like &lt;code&gt;Average holding time&lt;/code&gt;. This would not only develop your understanding of the problem but gives you &lt;code&gt;levers&lt;/code&gt; to control the specific atttributes of your &lt;code&gt;strategies&lt;/code&gt; at the later stage.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Many useful things are not useful in practice.&lt;/p&gt;
&lt;p&gt;In theory any marginal improvement is useful. In practice, marginal improvement cannot be justified at the cost of adding algorithmic/implementation complexity to the system. This throws majority of reasearch papers out-of-the window.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Don't Learn from pixels. Please consider adding any auxillary information about the dynamics.State Representation is really important.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reduce the dimensionality of &lt;code&gt;Action&lt;/code&gt; and &lt;code&gt;State&lt;/code&gt; space.&lt;/p&gt;
&lt;p&gt;Focus on explicitly reducing the noise from the &lt;code&gt;state&lt;/code&gt;. If you don't have a good set of minimal features required for the task, which is often the case, focus on building a pipeline to choose relevant features from you global set. Classical techniques may be of little use here, It is worthwhile to see if we can use the orginal task and the model to glean feature importance.&lt;/p&gt;
&lt;p&gt;Here &lt;code&gt;state&lt;/code&gt; simply refers to the features fed as input to the network. Theoritically,it seems network should be able to do feature selection and engineering implicitly. But, We have found drastic improvements by reducing noise from the input features before feeding to the network. In stock markets, many patterns found through training data don't generalize. It is intuitive to rely on &lt;code&gt;Occam's Razor&lt;/code&gt;- Among two similarly performing strategies, prefer the one that used less features. Many RL algorithms relies on having access to a hashtable storing visitation frequencies and other metrics of the state-space. A neural network acts as a drop-in replacement in modern DeepRL methods with potentially infinite state-spaces. This might also contribute to why the performance is improved by drastically reducing the dimensionality of the state.&lt;/p&gt;
&lt;p&gt;And reducing Action space has more to do with decreasing the complexity of the modeling problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;Engineering Environment&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;Reward Engineering&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Develop checks for checking robustness.&lt;/p&gt;
&lt;p&gt;For other domains it translates to developing techniques of accessing overfitting apart from checking run-on-test.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;You can use machine learning to understand ML models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No single metric to optimize&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Carefully Engineer the environment.
Reward engineering and desigining environment are intimately tied. Eg: Agent got stuck in high portfolio states. For instance if the global trend is up..it learned to get some high portfolio and stay there. Correlation vs causation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Technical-Notes&quot;&gt;Technical Notes&lt;a class=&quot;anchor-link&quot; href=&quot;#Technical-Notes&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Focus on gathering Large amounts of Clean Data. This should be the first priority.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;Focus on solving the most general instantiation of the problem. This system can later be used as a backbone to specialize.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Some Philosophical musings&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Why-not-to-join-in-a-startup-?&quot;&gt;Why not to join in a startup ?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-not-to-join-in-a-startup-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here I'm assuming you are an employee, not the founder.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Lack of proper mentorship. Startups hire just enough people to get things done. And often times you are the only one working on an area. During my time at Niveshi, I was the only machine learning engineer at the company. To grow, you need fast feedback loops. Nothing replaces learning from an experienced mentor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You won't get rich by working at a startup.You will get rich by deploying large sums of capital over multiple startups. Capital is cheaper than time. You can only work in so many startups over your career.Given abysmal success rate, it is stupid to diversify your time this way. Moreover,as an employee you will own 10 to 30 times lower equity than the founders. This is true even if you are the founding engineer of an early stage startup. Considering that you are just starting out your career,your market value is low. You will be paid minimal equity and compensation possible, even if they are generous. A better alternative is to work towards increasing your reputation early in your career. Earn a safety net of Career capital and money by working in big companies. Then,later start your own company or even work at a startup when you are potentially overvalued. Diversifying your bets among several low-risk and high-risk has better return profile than  medium-risk ones.[&lt;a href=&quot;https://en.wikipedia.org/wiki/Barbell_strategy&quot;&gt;https://en.wikipedia.org/wiki/Barbell_strategy&lt;/a&gt;]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You will likely learn some bad engineering practices. Believe it or not we managed to get away without having any kind of unittesting. Our code get's pushed only by means of peer review.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are no established practices to deal with common painpoints. Resolving conflict.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Deep Reinforcement Learning - Theory (Chapter 1)</title><link href="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" rel="alternate" type="text/html" title="Deep Reinforcement Learning - Theory (Chapter 1)" /><published>2021-02-21T00:00:00-06:00</published><updated>2021-02-21T00:00:00-06:00</updated><id>https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas</id><content type="html" xml:base="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-21-Deep-Learning-Useful-Ideas.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I plan on to write series of posts explaining key ideas in Modern Reinforcement Learning(RL). Current post covers basics of deep learning,followed by an introduction to RL Theory.(Thourougly proves two key algorithms for learning in RL setting - Value Iteration and Policy Iteration.). If you prefer a more practical introduction refer &lt;a href=&quot;https://vinayvarma.work/reinforcement%20learning/2020/05/06/Q-Learning.html&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;Note&lt;/code&gt; : This is not intended to be the first introduction to deep learning. Here I just provide a self-contained summary. Only hard prerequisite is to have good intuitions for &lt;code&gt;matrix multiplication&lt;/code&gt; and the notion of &lt;code&gt;taking a derivative&lt;/code&gt;. Otherwise refer to &lt;a href=&quot;https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&quot;&gt;Essence of linear algebra&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&quot;&gt;Calculus&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Basics&quot;&gt;Basics&lt;a class=&quot;anchor-link&quot; href=&quot;#Basics&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Many deep learning models follow a simple recipe:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. Gather the data.
2. Define learnable parameters. And specify how they will interact with the data.(architecture)
3. Define a loss function to minimize.
4. Adjust the parameters until satisfied.&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Train-a-linear-regression-model-using-gradient-descent.&quot;&gt;Train a linear regression model using gradient-descent.&lt;a class=&quot;anchor-link&quot; href=&quot;#Train-a-linear-regression-model-using-gradient-descent.&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we will see how we can perform all the above steps starting with the most barebones implementation. Note that the procedure outlined here is general purpose - meaning the way we adjust &lt;code&gt;parameters&lt;/code&gt; is going to remain same irrecpective of the modality of the data, details of the loss function or the architecture.&lt;/p&gt;
&lt;p&gt;Step 1. Gather the data&lt;/p&gt;
&lt;p&gt;Let's generate some fake data.Let's assume that the data is coming from $y = 2*x1 - 4.2*x2 + 1 + noise(measurement error)$. This can be more succintly represented in vector notation :

$$y = \begin{bmatrix} x1 \\ x2 \end{bmatrix} . \begin{bmatrix} 2 \\ -4.2 \end{bmatrix} + 1$$
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#number of features in the input&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;const&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;((1000, 2), (1000, 1))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Step 2. Define learnable parameters. And specify how they will interact with the data.(architecture)&lt;/p&gt;
&lt;p&gt;Now we aim to learn the right coefficients to approximate the data generation process. First let's look at some code.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Start with a random guess that respects the sanctity of the data.i.e our inputs are of dimension 1000*2 &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# outputs are 1000*1. Multiplying inputs by a 2*1 matrix(weights) and adding a constant(bias) is the simplest way&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# to ensure an output of 1000*1. &lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# initial guess&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#shape -&amp;gt; 1*2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#expected output&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;give_expected_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inpt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inpt&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;give_expected_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#shape -&amp;gt; 1000*1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expected_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;get_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# IF WE CAN DRIVE THIS NUMBER DOWN TO ZERO VIA A GENERAL PURPOSE PROCESS,WE ARE GOOD TO GO&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;2.121630476687219&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;squared_loss&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Just taking the mathematical gradient as defined by the model.&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;squared_loss&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;weights_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bias_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Sorry I&amp;#39;m not yet scalable enough for arbitrary loss functions&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_grad&lt;/span&gt;
        


&lt;span class=&quot;n&quot;&gt;grad_init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;squared_loss&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;give_expected_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#print(f&amp;#39;initial error, epoch 0: {error}&amp;#39;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pres_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;weight_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pres_lr&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias_grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pres_lr&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;give_expected_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pres_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pres_lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;
        
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;matplotlib&lt;/span&gt; inline
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;[&amp;lt;matplotlib.lines.Line2D at 0x7f2be3ea9df0&amp;gt;]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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
&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_bias&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(array([[-0.82604061, -1.82604061]]), array([0.97582166]))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the above code what happens if 'lr' is not dynamically adjusted ? Now, imagine an arbitrary architecture (interaction between parameters and data). Can we have a general purpose &lt;code&gt;get_grads&lt;/code&gt; function that efficiently evaluates the gradient for this network ?. For many such practical concerns, I suggest going through this free course from &lt;a href=&quot;https://course.fast.ai/&quot;&gt;fast.ai&lt;/a&gt;. Modern libraries like Pytorch provide simple abstractions to ignore all such details. Now some general intuitions towards coming up with architectures for learning more complex functions.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Adding-Inductive-biases---convolutions-and-recurrent-networks.&quot;&gt;Adding Inductive biases - convolutions and recurrent networks.&lt;a class=&quot;anchor-link&quot; href=&quot;#Adding-Inductive-biases---convolutions-and-recurrent-networks.&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the below feedforward network each neuron is connected to all the neurons in the previous layer. No inductive biases in the connectivity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/fc.png&quot; alt=&quot;FeedForward&quot; title=&quot;Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Locality&lt;/code&gt; : For almost all natural signals it is easier to predict the future using recent past compared to any earlier versions. &lt;code&gt;Locality&lt;/code&gt; allows for the sparsity of weights. We can put faraway weights to zero. In the below figure the &lt;code&gt;15&lt;/code&gt; weights of the first layer is reduced to &lt;code&gt;9&lt;/code&gt;. It's also important to be aware of the concept of &lt;code&gt;receptive field&lt;/code&gt;(RF). &lt;code&gt;RF&lt;/code&gt; of layer &lt;code&gt;a&lt;/code&gt; w.r.t &lt;code&gt;b&lt;/code&gt; is simply the number of neurons in &lt;code&gt;a&lt;/code&gt; that influence the outputs of layer b.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/sparsity.png&quot; alt=&quot;&quot; title=&quot;Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Stationarity&lt;/code&gt; : Same patterns are repeated again and again.
We don't need connections from the inputs far down. &lt;code&gt;Stationarity&lt;/code&gt; implies weight sharing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/parashare.png&quot; alt=&quot;&quot; title=&quot;Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Compositionality&lt;/code&gt;: There are hierarchies. Letters make up words,words make up sentences and so on. &lt;code&gt;Compisitionality&lt;/code&gt; implies deeper networks.&lt;/p&gt;
&lt;p&gt;Now We will see how popular building blocks like CNN'S and RNN'S leverage these properties. &lt;code&gt;CNN&lt;/code&gt;'s are a linear layer with lots of &lt;code&gt;weight sharing&lt;/code&gt; and &lt;code&gt;sparsity&lt;/code&gt;. RNN's just use &lt;code&gt;weight sharing&lt;/code&gt; but BPTT takes the &lt;code&gt;locality&lt;/code&gt; into account.&lt;/p&gt;
&lt;p&gt;CNN  = linear layer + weight sharing + sparsity.&lt;/p&gt;
&lt;p&gt;Consider convolving over a 4 &lt;em&gt; 4 inputs with a 3 &lt;/em&gt; 3 kernel with a unit stride as presented below.
&lt;img src=&quot;/images/copied_from_nb/my_icons/cnn.png&quot; alt=&quot;&quot; title=&quot;Credit:https://arxiv.org/pdf/1603.07285.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;If I stack the 2-d input into a 1-d vector by unrolling left to right and top to bottom, the convolution can be represented as a matrix multiplication.
&lt;img src=&quot;/images/copied_from_nb/my_icons/conv_mat.png&quot; alt=&quot;&quot; title=&quot;Credit:https://arxiv.org/pdf/1603.07285.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;zeros&lt;/code&gt; along the columns encode &lt;code&gt;locality&lt;/code&gt; while the replication of same weights along the rows account for &lt;code&gt;stationarity&lt;/code&gt;. If the above properties doesn't make sense for your input , then CNN's aren't the right choice.&lt;/p&gt;
&lt;p&gt;RNN = linear layer + weight sharing + BPTT(Back-prop through time)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/rnn.png&quot; alt=&quot;&quot; title=&quot;Credit:https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-3/&quot; /&gt;&lt;/p&gt;
&lt;p&gt;RNN's are used for sequence data. In the above figure the &lt;code&gt;arrow&lt;/code&gt; indicates matrix multiplication. The hidden state &lt;code&gt;h(t)&lt;/code&gt; at time &lt;code&gt;t&lt;/code&gt; is equal to Affine_transform(x(t)) + Affine_transform(h(t-1)).(&lt;code&gt;Note&lt;/code&gt;: Affine_transform refers to matrix multiplication). The following code will makes it all clear.&lt;/p&gt;
&lt;p&gt;Consider the following sequence :&lt;/p&gt;
&lt;p&gt;'Hey Jude, don't make it bad.&lt;/p&gt;
&lt;p&gt;Take a sad song and make it better.&lt;/p&gt;
&lt;p&gt;Remember to let her into your heart,&lt;/p&gt;
&lt;p&gt;Then you can start to make it better.
'&lt;/p&gt;
&lt;p&gt;Now we would want to classify it into either positive or negative sentiment. Ideally we would have a collection of such sequences with their corresponding labels. Here note that the number of words in each sequence need not be same. A naive approach would be to string all the words in a sequence to a single column,and run it through a fullyconnected network.(variable sequence length still poses a problem.) Let's see how an RNN can accomplish this with much less parameters.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;F&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i_sz&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_sz&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emb_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# This just adds the hidden representation which is a function of all the words fed until t-1 to the &lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;#word at t&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:])&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Stores the hidden representation for next  word in seq.&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;101&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vector_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seq_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([101, 1])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This can be seen as a two layer network.The last layer converts &lt;code&gt;h_sz&lt;/code&gt; to the output dimension, while the first layer maps &lt;code&gt;i_sz&lt;/code&gt; to &lt;code&gt;h_sz&lt;/code&gt;. But, the first layer only uses weight matrices of size 100 &lt;em&gt; 20,20 &lt;/em&gt; 20. Totalling of around 2400 parameters. Our naive version would have seq_len &lt;em&gt; i_sz &lt;/em&gt; h parameters.(around 20000). Moreover, In RNN number of paramenters is independent of
sequence length.&lt;/p&gt;
&lt;p&gt;The above model is just an instantiation of &lt;code&gt;weight sharing&lt;/code&gt; for sequential data. We haven't still leveraged the &lt;code&gt;locality&lt;/code&gt; aspect (&lt;code&gt;sparsity&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Moreover,eventhough we only have 3 different weight matrices,the actual number of layers is proportional to the &lt;code&gt;size&lt;/code&gt; of the &lt;code&gt;for loop&lt;/code&gt;. Aside from being very slow and memory intensive, gradients of loss w.r.t initial operations( i = 0) very unlikely to be stable.(According to the chain rule of derivatives the gradient of loss w.r.t first matrix multiplication would involve multiplying atleast &lt;code&gt;seq_len&lt;/code&gt; of partial derivatives. The resultant can easily explode or vanish.) What if we only take gradients for the last &lt;code&gt;n&lt;/code&gt; operations. This is also called &lt;code&gt;Truncated BPTT&lt;/code&gt;. Here's the modified &lt;code&gt;forward&lt;/code&gt; function.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emb_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# This just adds the hidden representation which is a function of all the words fed until t-1 to the &lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;#word at t&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:])&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Stores the hidden representation for next  word in seq.&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;3&lt;/span&gt; == 0:
                &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This just flushes the memory for the backward pass after every 3 steps. Aside from solving obvious practical problems,this also has regularizing effects. We are implicitly encoding our bias - you need not look past the last 3 points in the sequence - a.k.a &lt;code&gt;locality&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Self-Attention&quot;&gt;Self-Attention&lt;a class=&quot;anchor-link&quot; href=&quot;#Self-Attention&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Consider a sequence of vectors ($x_1$,$x_2$..$x_n$). It helps to imagine them as vectors corresponding to sequence of words. If you can bear with me,the following operation converts them into another sequence of vectors ($y_1$,$y_2$..$y_n$) of same dimension.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Two column vectors(each with dimension of 3*1)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.tensor&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensor&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;6.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;7.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;9.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;11.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;12.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# a random matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(torch.Size([3, 4]), torch.Size([4, 4]), torch.Size([3, 4]))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now,let's generate the same &lt;code&gt;V&lt;/code&gt; with a fancier set of operations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@X&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_new&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([3, 4])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here &lt;code&gt;temp.T&lt;/code&gt; is acting as &lt;code&gt;matrix&lt;/code&gt;. This also removes the need for additional initialization. This operation also lends to the following intution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;temp&lt;/code&gt; is the dot product of each column vector in U with all the vector within it. The captures the measure of similarity between the vectors.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Y_new&lt;/code&gt; is just a linear combination of U with the corresponding weights from the &lt;code&gt;temp&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In other words, $y_i =\sum_{j} w_{ij}.x_{j}$ where &lt;code&gt;w&lt;/code&gt;'s are taken frow the rows of &lt;code&gt;temp&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the above figure each vector $x_{i}$ is used three times. Let's take $x_{2}$ for illustration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To get $w_{22}$&lt;/li&gt;
&lt;li&gt;Similarly to get weight's required for all the other outputs $y_1$,$y_3$ and $y_4$ &lt;/li&gt;
&lt;li&gt;$x_2$ is also used in linear weighting with &lt;code&gt;w&lt;/code&gt;'s to get $y_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But, In the whole compution we are not learning any weights. Everything is being generated from the input. We can introduce three different set of &lt;code&gt;x&lt;/code&gt;'s for each of the above operations. Let's initialize three square matrices $W_k$,$W_q$,$W_v$, each of size (4,4).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# learnable parameters&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@w_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@w_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@w_v&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# This is the naming convention used in the literature.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@keys&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_new&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([3, 4])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;That's it. &lt;code&gt;Self-attention&lt;/code&gt; refers to performing above operations. We additionally normalize the weights in the &lt;code&gt;temp&lt;/code&gt; with &lt;code&gt;softmax&lt;/code&gt;. Further, We can also use sets of matrices ($W_k$,$W_q$,$W_v$),essentially replicationg self-attention with different weight matrices. The resulting outputs can be concatenated and be passed through a linear layer to get back the orginal dimension.(This is called &lt;code&gt;multi-head attention&lt;/code&gt;)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;F&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SelfAttention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# for n_heads we need the corresponding number of weight matrices of size feat_sz*feat_sz to get new&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#set of (keys,queries,values),Computationally this can be fused inside a single linear operation.&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_queries&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comb_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# typically data is fed with features along the `columns`.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# `torch.bmm` performs matrix multiplication for a given batch.It is efficient to squeeze n_heads along&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# with batches and perform the calculation at once.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#Rescaling the elements to control the scale&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;#reshaping&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# passing through a linear layer to combine all the heads&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comb_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# gives bs,n_seq,n_heads&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# input with features arranged in columns(shape = [1, 4, 3])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sa&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SelfAttention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# simple attention&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SelfAttention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(torch.Size([1, 4, 3]), torch.Size([1, 4, 3]))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Attention is first introduced to deal with sequences in the context of natural language. But, nothing in the above implementation handles &lt;code&gt;order&lt;/code&gt;. It just maps a &lt;code&gt;set of vectors&lt;/code&gt; to another. To enforce order, we can simply add a &lt;code&gt;position vector&lt;/code&gt; to our inputs.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Above ideas mark the end of the theoretical minimum. These will be needed only when you are trying to solve any of the interesting applications discussed below with RL.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Deep-RL---Theory&quot;&gt;Deep RL - Theory&lt;a class=&quot;anchor-link&quot; href=&quot;#Deep-RL---Theory&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Motivation&quot;&gt;Motivation&lt;a class=&quot;anchor-link&quot; href=&quot;#Motivation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Deep Learning allows for learning generalizable mappings between input and output. In supervised setting we are given a &lt;code&gt;fixed dataset&lt;/code&gt; D = {$(x_{i},y_{i})$},and are tasked to predict $y_{i}$ using $x_{i}$. Thus, we know the groud truth for all input data. This let's us formulate a &lt;code&gt;loss&lt;/code&gt; that reflects our dissatisfaction with the predicted &lt;code&gt;outputs&lt;/code&gt; and optimize over it. But, consider how we humans learn to perform any new task ? The dataset and the learning signal comes sequentially and is also dependent on our actions. We don't have prepared datasets in real life. We learn from experience. &lt;code&gt;RL&lt;/code&gt; deals with learning under this natural setting. The key difference here is that the dataset is not &lt;code&gt;constant&lt;/code&gt;, It changes everytime, contingent on your actions and the stochasticity in the environment. To make this clear, consider learning to play a video game. The pixels (call it &lt;code&gt;state&lt;/code&gt;) and the score you receive ( call it &lt;code&gt;reward&lt;/code&gt;) cannot be predetermined until you actually go through the experience. Here the dataset D = {$(state_{i},reward_{i})$} is not same everytime you play the game. &lt;code&gt;RL&lt;/code&gt; provides a formalism for learning optimal decision making. This will become clear when we see some concrete examples and code. But combing these techniques with &lt;code&gt;Neural networks&lt;/code&gt; has given us general algorithms that &lt;code&gt;learn to play atari games&lt;/code&gt;, &lt;code&gt;beat world champions at Go&lt;/code&gt; and &lt;code&gt;train robots to learn simple tasks&lt;/code&gt;.
&lt;img src=&quot;/images/copied_from_nb/my_icons/rlsuc.png&quot; alt=&quot;&quot; title=&quot;Credit: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Problem&quot;&gt;Problem&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Consider the Following optimization problem. Find the shortest path from state $s_0$ to goal $g$,where the edges indicate the cost/distance ?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/sp.png&quot; alt=&quot;&quot; title=&quot;Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Here taking a greedy approach will fail as actions will have long-term consequences. Solving the above problem efficiently requires realizing that &lt;code&gt;the distance to any node along the shortest path from source to destination is also shortest path&lt;/code&gt;. In the above example the shortest path to &lt;code&gt;g&lt;/code&gt; should either be through &lt;code&gt;d&lt;/code&gt; or &lt;code&gt;f&lt;/code&gt;.(one of the incoming edges). Let the shortest distance from a node &lt;code&gt;s&lt;/code&gt; to &lt;code&gt;g&lt;/code&gt; be given by $v^{*}(s)$. Then the last edge of the shortest path should come from evaluating $v^{*}(f))$ and $v^{*}(d)$, where $v^{*}(f) = 1 + v^{*}(g)$ and $v^{*}(d) = min(3+v^{*}(g),1+v^{*}(f))$. This backtracking or dynamic programming approach of finding one edge at a time is illustrated below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/op.png&quot; alt=&quot;&quot; title=&quot;Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Now, for each action we take, let us also add a &lt;code&gt;transition probability&lt;/code&gt; that defines the likelihood of ending up in any of the available states given the action. Here, for states &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;e&lt;/code&gt; we added some randomness. This will let us model more realistic scenarios. Consider an &lt;code&gt;RL agent&lt;/code&gt; driving your car. The consequences of any action (eg: Turning the steering to the right given the visual view of the road.) can only be modeled probabilistically.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/ssp.png&quot; alt=&quot;&quot; title=&quot;Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Here by weighting w.r.t the transition probabilities we can recover $v^{*}()$ for all states. Optimal policy is again achieved by acting greedily w.r.t $v^{*}()$. For example , $v^{*}(c) = min(4+0.3*v^{*}(e)+0.7*v^{*}(d),2+v^{*}(e))$. In RL, we call this &lt;code&gt;Bellmann Equation&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/sspb.png&quot; alt=&quot;&quot; title=&quot;Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/be.png&quot; alt=&quot;&quot; title=&quot;Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Modern &lt;code&gt;Deep RL&lt;/code&gt; deals with solving this stochastic version of the problem when the transition probabilities are not available and the number of possible states is very large. Consider the following video game playing scenario. The pixels on the screen at any timestamp can be taken as &lt;code&gt;state&lt;/code&gt;. The game rules provides list of &lt;code&gt;possible actions&lt;/code&gt; and the corresponding &lt;code&gt;reward&lt;/code&gt;(increase in score).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/vid.png&quot; alt=&quot;&quot; title=&quot;Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf&quot; /&gt;&lt;/p&gt;
&lt;p&gt;By the end of this module we will develop all the machinery to understand the algorithms that learn optimal(or good) policies for this general case.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Bellmann-Equation-and-MDP's&quot;&gt;Bellmann Equation and MDP's&lt;a class=&quot;anchor-link&quot; href=&quot;#Bellmann-Equation-and-MDP's&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In this section we will devolop some theory. First let's define an object called Markov Decision Process (MDP),given by the tuple of $(S,A,P,R,\gamma)$. Here discounting factor $\gamma$ is largely a mathematical convenience as it helps to bound the &lt;code&gt;total reward&lt;/code&gt;. Any reward $r_t$ received at timestamp &lt;code&gt;t&lt;/code&gt; is multiplied by $\gamma^{t-1}$, that is we prefer immediate rewards over faraway ones. The term &lt;code&gt;Markov&lt;/code&gt; refers to the fact that given present state &lt;code&gt;s&lt;/code&gt; and action taken from there &lt;code&gt;a&lt;/code&gt;, next state is independent of the past trajectory. Consider the example of &lt;code&gt;stochastic shortest path&lt;/code&gt; but with the goal state &lt;code&gt;g&lt;/code&gt; infinitely far away. In that setting we want to maximize the average reward we will get starting from any state.&lt;/p&gt;
&lt;p&gt;The only &lt;code&gt;knob&lt;/code&gt; agent has is policy $\pi : S \rightarrow A$.(which of the possible actions to take). The environment dynamics (P and R) are not under agent's control. For simplicity,let's assume that rewards are bounded and positive. Let's say that the agent has a policy $\pi$ and starts to intreract with the environment. The value function &lt;code&gt;v(s)&lt;/code&gt; refers to the average reward starting from state &lt;code&gt;s&lt;/code&gt; and following policy $\pi$. Then &lt;code&gt;v(s)&lt;/code&gt; for all the states satisfies a recursive definition as shown below. 
&lt;img src=&quot;/images/copied_from_nb/my_icons/bellmann.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Thus finding $v^{\pi}(s)$ amounts to solving system of linear equation or in matrix notation finding the inverse of a matrix. But, we still need the proof for the existence of the inverse for $I - \gamma.P^{\pi}$. Before going further, it's important to thoroughly understand the bellmann equation : $V^{\pi} = R^{\pi} + \gamma.P^{\pi}.V^{\pi}$. The deriviation involves observing the fact that once the agent takes initial action from state &lt;code&gt;s&lt;/code&gt;, the transition probability will dictate it's next state &lt;code&gt;s'&lt;/code&gt;. From there, the average reward by definition is  given by &lt;code&gt;v(s')&lt;/code&gt;, leaving a recursive definition.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/matform.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Below i have give the proof for the existence of inverse. If we recall, a matrix A($n * n$) multiplied by a column vector X ($n * 1$) merely takes a linear combination of all the column vectors in A. Existence of inverse to &lt;code&gt;A&lt;/code&gt; means that there doesnot exist a non-zero vector X, that can collapse A to a null vector. Using this fact and the traingular inequality on $I - \gamma.P^{\pi}$ completes the proof.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/proof.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Search-for-optimal-State-Values.&quot;&gt;Search for optimal State Values.&lt;a class=&quot;anchor-link&quot; href=&quot;#Search-for-optimal-State-Values.&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Now let's define $v^{*}(s)$ as the maximum expected reward that we can get from state &lt;code&gt;s&lt;/code&gt; under &lt;code&gt;any policy&lt;/code&gt;. 
Note that the above equation is defined on a particular policy or when the action from each state is fixed or if state transition probabilities are independent of &lt;code&gt;action taken&lt;/code&gt;. Once we are able to extract these values, optimal policy becomes obvious. &lt;code&gt;Value Iteration&lt;/code&gt; methods try to apprimate this $V^{*}(s)$. They start with some arbitrary function like $f(s) = 0 \forall s$ and iteratively bring $f$ closer to $V^{*}$. Let's also define $Q^{\pi}(s,a)$ as the expected reward under the policy $\pi$,when we take action &lt;code&gt;a&lt;/code&gt; at state &lt;code&gt;s&lt;/code&gt; and subsequently sample actions according to $\pi$. Here $\pi$ is just a function that takes state &lt;code&gt;s&lt;/code&gt; and action &lt;code&gt;a&lt;/code&gt; and returns the corresponding probability &lt;code&gt;a&lt;/code&gt; for taking that action. Similarly $Q^{*}(s,a)$ is also defined as the maximum Q that can be achieved under any policy. We often want $Q^{*}$ values over $V^{*}$ as simply choosing greedily w.r.t $Q^{*}$ gives the optimal policy. To see the advantage more clearly, Imagine an oracle that can give you $V^{*}$ for any &lt;code&gt;s&lt;/code&gt;. Now, the agent starts at state &lt;code&gt;s&lt;/code&gt; and is looking to take optimal action. It first has to take all possible actions from that state to see where it would end up. And for each subsequent state &lt;code&gt;s'&lt;/code&gt; , it has to query the oracle to get $V^{*}(s')$. Only after this we can determine the best action from initial state s as $max_{a\in A}(r_1 + v^{*}(s'))$. But having $Q^{*}$ values let's us choose this action  directly by evaluating $Q_{a\in A}^{*}(s,a)$. There exists a proof that for discounted infinite horizon MDP's there exists a stationary and deterministic optimal policy for all states simultaneously. Let's call this $\pi^{*}$. It is easy to see that both $V^{*}$ and $Q^{*}$ will also satisfy a similar recursive relation called &lt;code&gt;Bellmann Optimality equations&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/b.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Value-Iteration&quot;&gt;Value Iteration&lt;a class=&quot;anchor-link&quot; href=&quot;#Value-Iteration&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Till now we have seen how we can solve for &lt;code&gt;V&lt;/code&gt; for all states given a policy. But, Our objective is to find &lt;code&gt;V*&lt;/code&gt; - the maximum return that can be achieved under any policy. The claim is that if we start with random values of &lt;code&gt;Q&lt;/code&gt; and then do the following -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Act greedily with respect to these values.&lt;/li&gt;
&lt;li&gt;Update these &lt;code&gt;Q&lt;/code&gt; values using bellmann update rule.&lt;/li&gt;
&lt;li&gt;Repeat the process for some large number of times - H &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Will give us a policy $Q^{*,H}$(greedy policy w.r.t Q values after H Steps) that is close to true Optimal Policy. In what follows is the proof of the claim.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/b2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/vi1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/vi2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/vi3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Policy-Iteration-(PI)&quot;&gt;Policy Iteration (PI)&lt;a class=&quot;anchor-link&quot; href=&quot;#Policy-Iteration-(PI)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We also have an alternative algorithm that also converges to optimal policy called Policy Iteration.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;Note&lt;/code&gt; : PI strictly converges to optimal policy after some steps which is not true for VI as it only get's closer but never quite equals.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The following derivation requires some explanation. First Here's the claim of policy iteration algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start out with random policy.&lt;/li&gt;
&lt;li&gt;Evaluate $Q^{\pi_{0}}$ using bellmann update rule. (BOX 1). &lt;/li&gt;
&lt;li&gt;Greedy policy w.r.t to new Q values becomes your new policy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performing above steps for large number of iterations would give us Optimal Policy  - &lt;code&gt;Claim.&lt;/code&gt; (In fact the claim is more subtle as given by &lt;code&gt;Policy Improvement Theorem&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Policy Improvement Theorem&lt;/code&gt;: As given in below slide suggests that after every round of PI, the new policy has higher(or equal) &lt;code&gt;V&lt;/code&gt; compared to old &lt;code&gt;for all states&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The proof involves manipulating $\tau^{\pi}$ (bellmann operator). We start out by consicely writing down PI algorithm -&lt;/p&gt;
&lt;p&gt;$Q^{\pi_{k}} = \tau^{\pi_{k}}.Q^{\pi_{k}}$&lt;/p&gt;
&lt;p&gt;Note that bellmann optimality operator $\tau$(involves taking &lt;code&gt;max&lt;/code&gt; in next state) is defferent from bellmann operator $\tau^{\pi}$. And substituting the latter with former would always result in a value higher or equal by definition. Since according to PI algorithm the new policy is the Greedy one w.r.t updated Q values, we have  - 
$\tau.Q^{\pi_{k}} = \tau^{\pi_{k+1}}.Q^{\pi_{k}}$. Later by recursively expanding $Q^{\pi_{k}}$, we reach the fixed point of the &lt;code&gt;bellmann operator&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/pi1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Alternative and more Intuitive proof:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/pia1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/pia2.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/pia3.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/pia4.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Estimating Distributions</title><link href="https://vinayvarma.work/reinforcement%20learning/2020/11/10/Estimating-Distributions.html" rel="alternate" type="text/html" title="Estimating Distributions" /><published>2020-11-10T00:00:00-06:00</published><updated>2020-11-10T00:00:00-06:00</updated><id>https://vinayvarma.work/reinforcement%20learning/2020/11/10/Estimating-Distributions</id><content type="html" xml:base="https://vinayvarma.work/reinforcement%20learning/2020/11/10/Estimating-Distributions.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-10-Estimating-Distributions.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Here, I will cover methods that leverage deep learning to estimate distributions. Ideally, they should also allow us to sample new data-points aka generative models. Supervised learning has already shown great success. But, when labels are not available we still want to be able to use raw perceptual data (videos,images,raw text etc) to capture rich patterns in the data. Broadly, I will cover the basics of the following approaches :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Autoregressive Models&lt;/li&gt;
&lt;li&gt;Flow Based Models&lt;/li&gt;
&lt;li&gt;Latent Variable Models&lt;/li&gt;
&lt;li&gt;Generative Adversarial Networks (GAN's)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Consider a distribution X, of dog images (24*24 images). We want a model that can give&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a. high probability values for any dog image in the training data &lt;/li&gt;
&lt;li&gt;b. high probability for any other similar dog image (Generalization) &lt;/li&gt;
&lt;li&gt;c. Can generate novel dogs (Generative model). &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The recipe for satisfying conditions a.,b. seems straight forward - train on the training data maximizing log-likelihood of each image. An architecture like CNN would encode good prior(translational invariance and parameter sharing) to enable generalization. To be clear, what i'm suggesting is for each image, we will have a CNN processing followed by the output of size 256*24*24. ( For each pixel,we will have 256 possible values and the label comes from the actual value of that pixel in that image.). So,we are essentially maximizing the log probability of each pixel corresponding to all the images in the training data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;But, Once we have trained on our data what we end up is 24*24 histograms. What exactly will be P(x_i) ? Can we say that the sigma(p(xi)) = 1.(definition of probability distribution). Let's say we define P(x_i) as product of all the outputs. Here,all that we are ensuring is each histogram corresponding to each pixel sums to 1.(via softmax over outputs). So, each of the methods tries to approach this problem in different ways with some tradeoff in ease of  sampling new values and training.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Accept all your drives</title><link href="https://vinayvarma.work/think%20on%20these/2020/09/08/Accept-your-drives.html" rel="alternate" type="text/html" title="Accept all your drives" /><published>2020-09-08T00:00:00-05:00</published><updated>2020-09-08T00:00:00-05:00</updated><id>https://vinayvarma.work/think%20on%20these/2020/09/08/Accept-your-drives</id><content type="html" xml:base="https://vinayvarma.work/think%20on%20these/2020/09/08/Accept-your-drives.html">&lt;p&gt;Acknowledge and accept all human drives. The tension between Culturally-accepted and evolutionarily-primed behaviors create unnecessary fatigue and misery. Evolutionary forces create strong drives that are to be suppressed or accepted based on the imaginative reality of the culture. Breaching those norms leaves us feeling ashamed and guilty. We seek to adjust our thoughts and behaviors to fit into the culture.This often happens in very subtle ways. Mentally note or record in a journal the instances you felt the slightest tinge of guilt or indulged in behaviors that you are not happy. Now, accept that this is by definition is part of human nature. Sometimes mere social baggage creates unnecessary friction against evolutionary drives. Maybe the advantage of sticking to cultural imagination is not worth it. Even, noticing vast differences in the cultural norms across the globe can give a different perspective.Guilt is a bad feeling and it cannot be endured for long. So, our brains invent reasons to escape this feeling.&lt;/p&gt;

&lt;p&gt;All behaviours that are against our wellbeing are sustained either by this escape-mechanism or through our complete unawareness of their effects. Only an inquisitive and open mind has the luxury of ever discovering the behaviours in the second category. The creation of the ego is an apt example. Raising our awareness is the only way to open the possibility of improving our well-being.&lt;/p&gt;</content><author><name></name></author><summary type="html">Acknowledge and accept all human drives. The tension between Culturally-accepted and evolutionarily-primed behaviors create unnecessary fatigue and misery. Evolutionary forces create strong drives that are to be suppressed or accepted based on the imaginative reality of the culture. Breaching those norms leaves us feeling ashamed and guilty. We seek to adjust our thoughts and behaviors to fit into the culture.This often happens in very subtle ways. Mentally note or record in a journal the instances you felt the slightest tinge of guilt or indulged in behaviors that you are not happy. Now, accept that this is by definition is part of human nature. Sometimes mere social baggage creates unnecessary friction against evolutionary drives. Maybe the advantage of sticking to cultural imagination is not worth it. Even, noticing vast differences in the cultural norms across the globe can give a different perspective.Guilt is a bad feeling and it cannot be endured for long. So, our brains invent reasons to escape this feeling.</summary></entry><entry><title type="html">A different dimension</title><link href="https://vinayvarma.work/think%20on%20these/2020/09/05/different-dimension.html" rel="alternate" type="text/html" title="A different dimension" /><published>2020-09-05T00:00:00-05:00</published><updated>2020-09-05T00:00:00-05:00</updated><id>https://vinayvarma.work/think%20on%20these/2020/09/05/different-dimension</id><content type="html" xml:base="https://vinayvarma.work/think%20on%20these/2020/09/05/different-dimension.html">&lt;p&gt;Heightened experience of life doesnt come from grabbing better beliefs and ideologies. Beliefs offer certainty. Our brains are ill-equipped to deal with probabilities. So, a well-formed belief or opinion might serve as a good heuristic. But, Beliefs concerning oneself create an identity.It only creates angst and unnecessary dissatisfaction. It also has the side-effect of pushing life in the background. It feeds on the stability of its beliefs. It works only in the realm of thoughts  It strives to engross and fit everything in its tiny bubble. Any amount of brooding or theorization over this only can burst one to creates newer and more subtle ones. Non-judgemental awareness of this process loosens the hold. At this heightened level of perception, life happens on a different dimension&lt;/p&gt;</content><author><name></name></author><summary type="html">Heightened experience of life doesnt come from grabbing better beliefs and ideologies. Beliefs offer certainty. Our brains are ill-equipped to deal with probabilities. So, a well-formed belief or opinion might serve as a good heuristic. But, Beliefs concerning oneself create an identity.It only creates angst and unnecessary dissatisfaction. It also has the side-effect of pushing life in the background. It feeds on the stability of its beliefs. It works only in the realm of thoughts  It strives to engross and fit everything in its tiny bubble. Any amount of brooding or theorization over this only can burst one to creates newer and more subtle ones. Non-judgemental awareness of this process loosens the hold. At this heightened level of perception, life happens on a different dimension</summary></entry><entry><title type="html">Continual Learning</title><link href="https://vinayvarma.work/research/artificial%20general%20intelligence/2020/08/08/Continual-learning.html" rel="alternate" type="text/html" title="Continual Learning" /><published>2020-08-08T00:00:00-05:00</published><updated>2020-08-08T00:00:00-05:00</updated><id>https://vinayvarma.work/research/artificial%20general%20intelligence/2020/08/08/Continual-learning</id><content type="html" xml:base="https://vinayvarma.work/research/artificial%20general%20intelligence/2020/08/08/Continual-learning.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-08-Continual-learning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The objective of this work is to provide a coherent picture of &lt;code&gt;Continual Learning&lt;/code&gt; in the context of Neural Networks. I hope that this would serve as a launch pad for further investigations of the reader.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Continual-Learning&quot;&gt;Continual Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Continual-Learning&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Modern Deep Learning systems,in general, deal exclusively with i.i.d. data. If our grand objective is to build General Intelligence,then it is mandatory for a single system to learn multiple tasks. Here , We will start with the general setting of continual learning and then begin to investigate how various features of the problem are solved by current methods.
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;None of the known methods satisfactorily solve Continual Learning problem in it&amp;#8217;s General Setting.
&lt;/div&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Continual Learning : The General Setting

Optimize a defined loss function under Infinite Stream of Data,without any explicit distinction between tasks. 

Note : Here 'task' is artificially introduced to represent variation in data distribution and even loss function.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following are some of the desirable features of the learning scheme &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Fixed Memory&lt;/code&gt; : Memory requirements cannot grow with each additional task.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;No Task Boundary&lt;/code&gt; : Learn from the input data without explicitly defining task boundary increases the flexibility of the method and also reflects real-world setting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Online Learning&lt;/code&gt; :  No offline storing of data.(i.e training with large batches for multiple epochs)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Forward Transfer&lt;/code&gt; : Previous tasks should ease the learning on new tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Backward Transfer&lt;/code&gt; : Upon Learning future tasks, performance on correspondingly related past tasks should improve.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Problem Agnostic&lt;/code&gt; : Should be flexible enough to work with different loss functions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 10 16&quot; version=&quot;1.1&quot; width=&quot;10&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10 7H6l3-7-9 9h4l-3 7 9-9z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;Humans do not exactly operate in the General setting of &amp;#8217;Continual Learning&amp;#8217;. We often have local control over the stream of tasks. We decide,apriori the sequence of tasks to perform in order to reach a desired goal. We also assign ourselves tasks(thus access to task labels) in order to optimize our goals. 
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Implicit in the above definition,we expect the learning procedure to learn flexible representations. Traditional machine learning paradigm aims to learn a specifit task - machine translation,Image recognition,Speech synthesis - end to end,often from scratch. This is facilitated by training on large amounts of data,specific to the task.Consider Classical MNIST and CIFAR-10 Datesets,that have propelled progress in Computer vision. Each of these have 60,000 Images split equally between ten classes. This is in sharp contrast to how a human experiences learning. Our learning involves learning large number of classes,with few examples from each class.(say 6000 classes with 10 examples for each). Thus,our learning procedure is optimized for this setting and we are forced to learn concepts and abstractions that let us learn quickly and efficiently with less data. Later in this section,we will see techniques that tries to alleviate this problem in modern neural networks.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let's consider three scenarios,which will better help us to better parse the existing literature in the field.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;$e^{i\pi} + 1 = 0$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;
$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Approaches&quot;&gt;Approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#Approaches&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/deep-learning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The approaches taken can be broadly categorized into three groups &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Modify the Update Rule&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Replay methods&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use Semi-distributed representations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Meta-Approaches&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here we will cover some representative works from each of the above categories.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;1.-Modify-the-Update-Rule&quot;&gt;1. Modify the Update Rule&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Modify-the-Update-Rule&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Instead of updating all the weights on a new task, the most important weights are retained.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.-Meta-Approaches&quot;&gt;4. Meta-Approaches&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Meta-Approaches&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;First, We will try to understand the various incarnations of these approaches and finally conclude with some recent works that try to leverage Meta-Approaches for Continual Learning.&lt;/p&gt;
&lt;p&gt;Meta-Learning is a general paradigm that aims to condition the learning rule itself based on pre-defined properties of the learned representations. This will become clear when we see concrete examples. But,for our task of continual learning, Meta-Approaches offer a way away from hand-engineered approaches to learning-based solutions. Let us explore few foundational works in this field,before we see a concrete example for Continual Learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;I will not cover the complete spectrum of Meta-Learning approaches,just enough to appreciate how this paradigm allows for a more flexible approach towards solving Continual Learning. But,for a good strating point check &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html&quot;&gt;Lil&amp;#8217;Log&amp;#8217;s Blog&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;MODEL-AGNOSTIC-META--LEARNING-(MAML)-&amp;lt;sup id='fnref--3' class='footnote-ref'&amp;gt;&amp;lt;a href='#fn--3'&amp;gt;-3&amp;lt;/a&amp;gt;&amp;lt;/sup&amp;gt;&quot;&gt;MODEL-AGNOSTIC META- LEARNING (MAML) &lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;a class=&quot;anchor-link&quot; href=&quot;#MODEL-AGNOSTIC-META--LEARNING-(MAML)-&amp;lt;sup id='fnref--3' class='footnote-ref'&amp;gt;&amp;lt;a href='#fn--3'&amp;gt;-3&amp;lt;/a&amp;gt;&amp;lt;/sup&amp;gt;&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Notation-:&quot;&gt;Notation :&lt;a class=&quot;anchor-link&quot; href=&quot;#Notation-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;T : The task, corresponds to the objective or loss function,domain,environment.&lt;/p&gt;
&lt;p&gt;p(T): The distribution of tasks from which a new task is sampled.&lt;/p&gt;
&lt;p&gt;f&lt;sub&gt;&amp;theta;&lt;/sub&gt;   : A model(neural net) parametrized by $\theta$&lt;/p&gt;
&lt;p&gt;{T&lt;sub&gt;i&lt;/sub&gt;} ~ p(T) : Tasks used for meta training&lt;/p&gt;
&lt;p&gt;{T&lt;sub&gt;j&lt;/sub&gt;} ~ p(T) : Tasks used for meta testing&lt;/p&gt;
&lt;p&gt;{D&lt;sub&gt;$T_i$&lt;/sub&gt;} : Meta-training set&lt;/p&gt;
&lt;p&gt;{D&lt;sub&gt;$T_j$&lt;/sub&gt;} : Meta-testing set&lt;/p&gt;
&lt;p&gt;{D&lt;sub&gt;T&lt;/sub&gt;&lt;sup&gt;tr&lt;/sup&gt;} : Traning data for task T&lt;/p&gt;
&lt;p&gt;{D&lt;sub&gt;T&lt;/sub&gt;&lt;sup&gt;test&lt;/sup&gt;} : Test data for task T&lt;/p&gt;
&lt;p&gt;L&lt;sub&gt;T&lt;sub&gt;i&lt;/sub&gt;&amp;lt;/sub&amp;gt; : Loss or feedback from the sampled task T&lt;sub&gt;i&lt;/sub&gt;&amp;lt;/p&amp;gt;
&lt;p&gt;Given that we want our model to quickly adapt to a wide range of tasks, can we incorporate this inductive bias  explicitly into our training ?&lt;/p&gt;
&lt;h4 id=&quot;PROBLEM-STATEMENT-:&quot;&gt;PROBLEM STATEMENT :&lt;a class=&quot;anchor-link&quot; href=&quot;#PROBLEM-STATEMENT-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Learn the weights of a neural net that can quickly learn to perform well on a new task.&lt;/p&gt;
&lt;h4 id=&quot;Method-:&quot;&gt;Method :&lt;a class=&quot;anchor-link&quot; href=&quot;#Method-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The intuition is to learn the right set of initial parameters that which upon few gradient steps on a new task,would result in good performance.&lt;/p&gt;
&lt;p&gt;Consider a task T&lt;sub&gt;i&lt;/sub&gt; drawn out of task distribution p(T). Now, from this T&lt;sub&gt;i&lt;/sub&gt; we can sample training dataset D&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;tr&lt;/sup&gt;,the loss/feedback on which is given by L&lt;sub&gt;T&lt;sub&gt;i&lt;/sub&gt;&amp;lt;/sub&amp;gt;. This loss is parameterized by f&lt;sub&gt;&amp;theta;&lt;/sub&gt;. Suppose we take few gradient steps on this $\theta$ to give $\phi$. The error on test data set ,D&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;test&lt;/sup&gt; sampled from T&lt;sub&gt;i&lt;/sub&gt;,acts as training signal to update orginal f&lt;sub&gt;&amp;theta;&lt;/sub&gt;.&amp;lt;/p&amp;gt;
&lt;p&gt;What makes this process different from traditional training of neural networks is that the gradient is taken over meta-parameters $\phi$ through $\theta$. Consider the case of taking one gradient over L&lt;sub&gt;T&lt;sub&gt;i&lt;/sub&gt;&amp;lt;/sub&amp;gt; to arrive at $\phi$.&amp;lt;/p&amp;gt;

&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;
$$ \phi_i  = \theta - \alpha \nabla_\theta \mathscr{L}(\theta,D_{T_i}^{tr})$$
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now,the loss on $\phi$&lt;sub&gt;i&lt;/sub&gt; (taken on  $ {D_{T_i}^{test}} $ ) is backpropagated through $\theta$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;
$$ min_{\theta} \Sigma_{{T_i}\thicksim p(T)} L(\phi_i,D_{T_i}^{test}) = min_{\theta} \Sigma_{{T_i}\thicksim p(T)} \mathscr{L}(\theta - \alpha \nabla_\theta \mathscr{L}(\theta,D_{T_i}^{tr}),D_{T_i}^{test}) $$
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;$\alpha : Learning rate(hyper-parameter)$&lt;/p&gt;
&lt;p&gt;Note that the meta-optimization is performed over the model parameters $\theta$, whereas the objective is computed using the updated model parameters $\phi$. In effect, the method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/my_icons/maml.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now, We will see how this approach can be applied to &lt;code&gt;Continual learning&lt;/code&gt; by just tweaking the problem statement.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;$\mathscr{L}$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For example, here is a footnote &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.
And another &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. This is the footnote.&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. This is the other footnote. You can even have a &lt;a href=&quot;https://arxiv.org/abs/1905.12588&quot;&gt;link&lt;/a&gt;!&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = &amp;theta;&lt;sub&gt;o&lt;/sub&gt; x + &amp;theta;&lt;sub&gt;1&lt;/sub&gt;x&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;h&lt;sub&gt;&amp;theta;&lt;/sub&gt;(x) = &amp;theta;&lt;sub&gt;o&lt;/sub&gt; x + &amp;theta;&lt;sub&gt;1&lt;/sub&gt;x&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &lt;a href=&quot;https://arxiv.org/abs/1909.08383&quot;&gt;arXiv:1909.08383&lt;/a&gt;!&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &lt;a href=&quot;https://arxiv.org/abs/1905.12588&quot;&gt;arXiv:1905.12588&lt;/a&gt;!&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. &lt;a href=&quot;https://arxiv.org/abs/1703.03400&quot;&gt;arXiv:1703.03400&lt;/a&gt;!&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. &lt;a href=&quot;https://arxiv.org/abs/1905.12588&quot;&gt;arXiv:1905.12588&lt;/a&gt;!&lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&amp;lt;/div&amp;gt;
 

&lt;/sub&gt;&lt;/p&gt;&lt;/sub&gt;&lt;/p&gt;&lt;/sub&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Basics of Deep Learning</title><link href="https://vinayvarma.work/jupyter/2020/07/02/fastai-book-walkthrough.html" rel="alternate" type="text/html" title="Basics of Deep Learning" /><published>2020-07-02T00:00:00-05:00</published><updated>2020-07-02T00:00:00-05:00</updated><id>https://vinayvarma.work/jupyter/2020/07/02/fastai-book-walkthrough</id><content type="html" xml:base="https://vinayvarma.work/jupyter/2020/07/02/fastai-book-walkthrough.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-02-fastai-book-walkthrough.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;About&quot;&gt;About&lt;a class=&quot;anchor-link&quot; href=&quot;#About&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This notebook is a walkthrough of the amazing book by &lt;code&gt;Jeremy howard&lt;/code&gt; and &lt;code&gt;Slyvian Gugger&lt;/code&gt;. To buy the orginal book :
&lt;code&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD&lt;/code&gt;. I am using their free &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;git-hub version&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Download-files-and-unzip&quot;&gt;Download files and unzip&lt;a class=&quot;anchor-link&quot; href=&quot;#Download-files-and-unzip&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://vinayvarma.work/images/chart-preview.png" /><media:content medium="image" url="https://vinayvarma.work/images/chart-preview.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Title</title><link href="https://vinayvarma.work/2020/05/06/thoughts.html" rel="alternate" type="text/html" title="Title" /><published>2020-05-06T00:00:00-05:00</published><updated>2020-05-06T00:00:00-05:00</updated><id>https://vinayvarma.work/2020/05/06/thoughts</id><content type="html" xml:base="https://vinayvarma.work/2020/05/06/thoughts.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-05-06-thoughts.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;trying neil gaiman's technique of blocking out fixed periods of time daily where i'm allowed to not write if i wish but not allowed to do anything else.(30 mins).For the time being the objective of this practice in improve my personal writing skills - i.e write a readable prose,extra points if also delightful.Hoping to eventually tease out more concrete practice session...aka with some measurable metrics.Some important areas to work on:&lt;/li&gt;
&lt;li&gt;1.Clarity in writing.(as it reflects,clarity of thought)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary of the day:&lt;/p&gt;
&lt;p&gt;I have tried out a more engaging approach to learning mathematics - solve problems.I have found a nice book on analysis and an accompanying problem book.I seem to have enjoyed teasing out some problems from the first chapter.Surely,this is more engaging.&lt;/p&gt;
&lt;p&gt;Personal interview :
How my philosophy of health has changed over the years?&lt;/p&gt;
&lt;p&gt;Ok, let me reel back to as far into past I can. I think I don't have the slightest clue during my high school for sure. I used to eat a typical south Indian diet which involves feeding in copious amounts of refined flour(rice, wheat.).And the protein intake is laughable at best. As expected, I used to feel hungry right after eating. I felt good about it. I somehow felt that it translates as growth. During my high-school, especially in the later years, after class 10th, I hardly had time to even surf the internet casually. We were busy preparing for the entrance exam called IIT-JEE(consider them the ivy league of Indian universities just 10 times more competitive). The psychological turmoil during that period is for some other time. I have always had an addiction to reading. I read compulsively. Well, on anything with the slight influence of my mood to steer. That's when I first read Tim Ferriss and some folks on his podcast like peter Attia. I don't remember how I stumbled on the bulletproof guy, but I kind of fancied his books as they seem to have the right amount of 'look I'm different and also right' kind of air about them. So, I picked up on keto and read further. (David Perlmutter-Author of Grain Brain).Well, presently I take two meals a day, with no grains or flours. Unlimited amount of unadulterated foods. (meat, veggies, and fruit) conditioned on availability and taste. Some times I randomly skip meals all day. The next day, I ensure to start with an intense workout followed by a ravenous feeding session. One good rule of thumb is don't eat anything that came into existence in the last 100 years or so and hasn't been before - all packaged and processed foods, sugar.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;weight&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;&amp;#39;weight&amp;#39;&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;s2&quot;&gt;&amp;quot;hi{str(i)}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_text output_error&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;ansi-red-fg&quot;&gt;---------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;ansi-red-fg&quot;&gt;KeyError&lt;/span&gt;                                  Traceback (most recent call last)
&lt;span class=&quot;ansi-green-fg&quot;&gt;&amp;lt;ipython-input-8-8197ad1f9fec&amp;gt;&lt;/span&gt; in &lt;span class=&quot;ansi-cyan-fg&quot;&gt;&amp;lt;module&amp;gt;&lt;/span&gt;
&lt;span class=&quot;ansi-green-intense-fg ansi-bold&quot;&gt;      1&lt;/span&gt; i&lt;span class=&quot;ansi-blue-fg&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;ansi-cyan-fg&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;ansi-green-fg&quot;&gt;----&amp;gt; 2&lt;/span&gt;&lt;span class=&quot;ansi-red-fg&quot;&gt; &lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;&amp;#34;hi{str(i)}&amp;#34;&lt;/span&gt;&lt;span class=&quot;ansi-blue-fg&quot;&gt;.&lt;/span&gt;format&lt;span class=&quot;ansi-blue-fg&quot;&gt;(&lt;/span&gt;i&lt;span class=&quot;ansi-blue-fg&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;ansi-red-fg&quot;&gt;KeyError&lt;/span&gt;: &amp;#39;str(i)&amp;#39;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;weig&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>