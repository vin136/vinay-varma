{
  
    
        "post0": {
            "title": "Lesson learnt while building a Deep Reinforcement learning based automated stock trading service.",
            "content": "As of today, I have been working at Niveshi for nearly two-and-half years. We Started out in late 2018, with nothing but a rough idea and a non-working model at best to automate strategy generation using DeepRL. I will spare other details but I believe we have succeeded in building an automated pipeline from getting raw data to ready-to-deploy startegies. Here I&#39;m logging some lossons learned along the way. These will be useful for folks trying to make research ideas in Artificial Intelligence work in the real world, especially Deep RL. I will take stock trading as a running example,to illustrate some general principles. Some familiarity with RL is assumed. . Objective : Find statistical regularities in the historical data and identify profitable trading rules. For simplicity let&#39;s assume the data includes only historical price and volume. From RL perspective, given a set of time-series features learn to trade profitably. This amounts to outputting appropriate actions at each of the time-stamps,in order to maximize profits. . Note :Usually techniques are sampled from traditional machine learning,signal processing and statistics. The recipe is as follows - Hire a bunch of smart kids -&gt; Give them the data-&gt; Ask them to make money for the fund -&gt; Incentivize them appropriately. Apart from the laundry list of problems that come with managing people, this approach is quite costly, and doesn&#39;t scale well. Markets quickly absorb any obvious edges. Consequently,any successful pattern,however sophisticated,have a short shel-life. DeepRL, if can successfully be employed can replace the hiring and continuously finding sophisticated patterns part. . The following is the form of notes that I would hope my past self have known when starting out with the problem. Some of these we got it right the first time , while others learned the hardway.But neverthless, here are the lessons. . Don&#39;t Learn from pixels. Please consider adding any auxillary information about the dynamics.State Representation is really important. . | Reduce the dimensionality of Action and State space. . Focus on explicitly reducing the noise from the state. If you don&#39;t have a good set of minimal features required for the task, which is often the case, focus on building a pipeline to choose relevant features from you global set. Classical techniques may be of little use here, It is worthwhile to see if we can use the orginal task and the model to glean feature importance. . Here state simply refers to the features fed as input to the network. Theoritically,it seems network should be able to do feature selection and engineering implicitly. But, We have found drastic improvements by reducing noise from the input features before feeding to the network. In stock markets, many patterns found through training data don&#39;t generalize. It is intuitive to rely on Occam&#39;s Razor- Among two similarly performing strategies, prefer the one that used less features. Many RL algorithms relies on having access to a hashtable storing visitation frequencies and other metrics of the state-space. A neural network acts as a drop-in replacement in modern DeepRL methods with potentially infinite state-spaces. This might also contribute to why the performance is improved by drastically reducing the dimensionality of the state. . And reducing Action space has more to do with decreasing the complexity of the modeling problem. . | Don&#39;t Fiddle with network-architectures yet. . When You begin workin on a new problem, Start with the simplest yet bespoke architucture you can find. At the outset,you have too many variables. When we started, apart from some inspiration of watching AlphaGo beating.. We don&#39;t have a reason to believe that it would work. And network architecture is the least important piece of the puzzle. . | Don&#39;t do AutoML We have never done blind hyper-parameter search. Imagine working with a peculiar dataset where almost there is no useful published work to rely on for guidance. And you just managed to train the model. How would you go from here and start building confidence on yourself and the model to discover something useful. Based on our understanding of the algorithm and our domain we made hypothesis and tested them by running the experiments. Note that at this point we just managed to fit train.Performance on out of sample is out of question. To give an example, We would spend some time thinking how would changing any particular hyperparameter say gamma or entropy would effect some metrics of the resulting strategies like Average holding time. This would not only develop your understanding of the problem but gives you levers to control various attributes of the strategies. . Engineering Environment | | Reward Engineering | How to deal with less data ? . signal = amout of data + signal from each data point. . | Many useful things are not useful in production. The advantage of any feature is proportional to differential advantage/(maintenance overload or complexity) . | Develop checks for checking robustness. . For other domains it translates to developing techniques of accessing overfitting apart from checking run-on-test. . | You can use machine learning to understand ML models. . | No single metric to optimize . | Carefully Engineer the environment. Reward engineering and desigining environment are intimately tied. Eg: Agent got stuck in high portfolio states. For instance if the global trend is up..it learned to get some high portfolio and stay there. Correlation vs causation. . | # Some Philosophical musings . Why not to join in a startup ? . Here I&#39;m assuming you are an employee, not the founder. . Lack of proper mentorship. Startups hire just enough people to get things done. And often times you are the only one working on an area. During my time at Niveshi, I was the only machine learning engineer at the company. To grow, you need fast feedback loops. Nothing replaces learning from an experienced mentor. . | You won&#39;t get rich by working at a startup.You will get rich by deploying large sums of capital over multiple startups. Capital is cheaper than time. You can only work in so many startups over your career.Given abysmal success rate, it is stupid to diversify your time this way. Moreover,as an employee you will own 10 to 30 times lower equity than the founders. This is true even if you are the founding engineer of an early stage startup. Considering that you are just starting out your career,your market value is low. You will be paid minimal equity and compensation possible, even if they are generous. A better alternative is to work towards increasing your reputation early in your career. Earn a safety net of Career capital and money by working in big companies. Then,later start your own company or even work at a startup when you are potentially overvalued. Diversifying your bets among several low-risk and high-risk has better return profile than medium-risk ones.[https://en.wikipedia.org/wiki/Barbell_strategy] . | You will likely learn some bad engineering practices. Believe it or not we managed to get away without having any kind of unittesting. Our code get&#39;s pushed only by means of peer review. . | There are no established practices to deal with common painpoints. Resolving conflict. . | .",
            "url": "https://vinayvarma.work/research/2021/02/21/What-I-Learned-at-Niveshi.html",
            "relUrl": "/research/2021/02/21/What-I-Learned-at-Niveshi.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Learning",
            "content": "Deep Learning is a collection of techniques for finding functions that can approximate arbitrary input-output mappings, while capturing enough structure of the problem to be able to use them in practical applicatons.language-translation systems,Image classification,Movie Recommendation systems are Some notable applications where they have become defacto-choice. This is by no means an exhaustive treatment of the field. Here I provide concise summaries,followed by a simple implementation of some of the most interesting ideas in the field. I believe that many things in the field are unnecessarily complicated by lengthy treatment where a readable code and short explanation will suffice. . Note : This is not intended to be the first introduction to deep learning. Here I wanted to succintly catalogue some the latest advancements in the field. But, respecting tradition the first module starts from the basics. Only hard prerequisite is to have a good intuition of matrix multiplication notion of taking a derivative. . Basics . Many deep learning models follow a simple recipe: . 1. Gather the data. 2. Define learnable parameters. And specify how they will interact with the data.(architecture) 3. Define a loss function to minimize. 4. Adjust the parameters until satisfied. . Train a linear regression model using gradient-descent. . Here we will see how we can perform all the above steps starting with the most barebones implementation. Note that the procedure outlined here is general purpose - meaning the way we adjust parameters is going to remain same irrecpective of the modality of the data, details of the loss function or the architecture. . Step 1. Gather the data . Let&#39;s generate some fake data.Let&#39;s assume that the data is coming from $y = 2*x1 - 4.2*x2 + 1 + noise(measurement error)$. This can be more succintly represented in vector notation : $$y = begin{bmatrix} x1 x2 end{bmatrix} . begin{bmatrix} 2 -4.2 end{bmatrix} + 1$$ . import numpy as np def get_data(*params,const=None,rows=1000): #number of features in the input dim = len(params) x = np.random.normal(0,0.3,(rows,dim)) y = x@np.array([params]).T if const: y += np.array([const]) return x,y x,y = get_data(2,-4.2,const=1) x.shape,y.shape . ((1000, 2), (1000, 1)) . Step 2. Define learnable parameters. And specify how they will interact with the data.(architecture) . Now we aim to learn the right coefficients to approximate the data generation process. First let&#39;s look at some code. . # Start with a random guess that respects the sanctity of the data.i.e our inputs are of dimension 1000*2 # outputs are 1000*1. Multiplying inputs by a 2*1 matrix(weights) and adding a constant(bias) is the simplest way # to ensure an output of 1000*1. # initial guess init_weights = np.array([[0.,-1.]]) #shape -&gt; 1*2 init_bias = np.array([0.]) #expected output def give_expected_output(inpt,weights,bias): return ((inpt@weights.T) + bias) out = give_expected_output(x,init_weights,init_bias)#shape -&gt; 1000*1 def get_error(out,expected_out): return np.mean((expected_out - out)**2) get_error(out,y) # IF WE CAN DRIVE THIS NUMBER DOWN TO ZERO VIA A GENERAL PURPOSE PROCESS,WE ARE GOOD TO GO . 2.121630476687219 . def get_grads(weights,bias,x,y,loss_func=&#39;squared_loss&#39;): if loss_func == &#39;squared_loss&#39;: weights_grad = 2*np.mean((x@weights.T + bias - y)*weights) bias_grad = 2*np.mean((x@weights.T + bias - y)) else: print(&quot;Sorry I&#39;m not yet scalable enough for arbitrary loss functions&quot;) return weights_grad,bias_grad grad_init_weights,grad_bias = get_grads(init_weights,init_bias,x,y,loss_func = &#39;squared_loss&#39;) def learn(x,y,init_weights,init_bias,loss_func,lr=0.001,epochs=5000): out = give_expected_output(x,init_weights,init_bias) error = loss_func(out,y) #print(f&#39;initial error, epoch 0: {error}&#39;) errors = [error] pres_lr = lr for i in range(epochs): weight_grad,bias_grad = get_grads(init_weights,init_bias,x,y) init_weights -= weight_grad*pres_lr init_bias -= bias_grad*pres_lr out = give_expected_output(x,init_weights,init_bias) error = loss_func(out,y) if np.mean(weight_grad)&lt;0.0001: pres_lr = pres_lr*2 errors.append(error) return errors,init_weights,init_bias . errors,final_weights,final_bias = learn(x,y,init_weights,init_bias,get_error) %matplotlib inline import matplotlib.pyplot as plt plt.plot(errors) . [&lt;matplotlib.lines.Line2D at 0x7f2be3ea9df0&gt;] . final_weights,final_bias . (array([[-0.82604061, -1.82604061]]), array([0.97582166])) . Deep RL Methods . Motivation . Deep Learning allows for learning generalizable mappings between input and output. In supervised setting we are given a fixed dataset D = {$(x_{i},y_{i})$},and are tasked to predict $y_{i}$ using $x_{i}$. Thus, we know groud truth for all the input data. This let&#39;s us formulate a loss that reflects our dissatisfaction with the predicted outputs and optimize over it. But, consider how we learn to perform any new task ? The dataset and the learning signal comes sequentially and is also dependent on our actions. We don&#39;t have prepared datasets in real life. We learn from experience. RL deals with learning under this natural setting. The key difference to note is that the dataset is not constant, It changes everytime, contingent on your actions and the stochasticity in the environment. To make this clear, consider learning to play a video game. The pixels (call it state) and the score you receive ( call it reward) cannot be predetermined until you actually go through the experience. Here the dataset D = {$(state_{i},reward_{i})$} is not same everytime you play the game. RL provides a formalism for learning optimal decision making. This will become clear when we see some concrete examples and code. But combing these techniques with Neural networks has given us general algorithms that learn to play atari games, beat world champions at Go and train robots to learn simple tasks. . Problem . Consider the Following optimization problem. Find the shortest path from state $s_0$ to goal $g$,where the edges indicate the cost/distance ? . .",
            "url": "https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html",
            "relUrl": "/research/2021/02/21/Deep-Learning-Useful-Ideas.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Estimating Distributions",
            "content": "",
            "url": "https://vinayvarma.work/reinforcement%20learning/2020/11/10/Estimating-Distributions.html",
            "relUrl": "/reinforcement%20learning/2020/11/10/Estimating-Distributions.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Accept all your drives",
            "content": "Acknowledge and accept all human drives. The tension between ‘Culturally-accepted’ and ‘evolutionarily-primed’ behaviors create unnecessary fatigue and misery. Evolutionary forces create strong drives that are to be suppressed or accepted based on the imaginative reality of the culture. Breaching those norms leaves us feeling ashamed and guilty. We seek to adjust our thoughts and behaviors to fit into the culture.This often happens in very subtle ways. Mentally note or record in a journal the instances you felt the slightest tinge of guilt or indulged in behaviors that you are not happy. Now, accept that this is by definition is part of human nature. Sometimes mere social baggage creates unnecessary friction against evolutionary drives. Maybe the advantage of sticking to cultural imagination is not worth it. Even, noticing vast differences in the cultural norms across the globe can give a different perspective.Guilt is a bad feeling and it cannot be endured for long. So, our brains invent reasons to escape this feeling. . All behaviours that are against our wellbeing are sustained either by this escape-mechanism or through our complete unawareness of their effects. Only an inquisitive and open mind has the luxury of ever discovering the behaviours in the second category. The creation of the ego is an apt example. Raising our awareness is the only way to open the possibility of improving our well-being. .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/09/08/Accept-your-drives.html",
            "relUrl": "/think%20on%20these/2020/09/08/Accept-your-drives.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A different dimension",
            "content": "Heightened experience of life doesn’t come from grabbing better beliefs and ideologies. Beliefs offer certainty. Our brains are ill-equipped to deal with probabilities. So, a well-formed belief or opinion might serve as a good heuristic. But, Beliefs concerning oneself create an identity.It only creates angst and unnecessary dissatisfaction. It also has the side-effect of pushing life in the background. It feeds on the stability of its beliefs. It works only in the realm of thoughts – It strives to engross and fit everything in its tiny bubble. Any amount of brooding or theorization over this only can burst one to creates newer and more subtle ones. Non-judgemental awareness of this process loosens the hold. At this heightened level of perception, life happens on a different dimension .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/09/05/different-dimension.html",
            "relUrl": "/think%20on%20these/2020/09/05/different-dimension.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Continual Learning",
            "content": "Introduction . The objective of this work is to provide a coherent picture of Continual Learning in the context of Neural Networks. I hope that this would serve as a launch pad for further investigations of the reader. . Continual Learning . Modern Deep Learning systems,in general, deal exclusively with i.i.d. data. If our grand objective is to build General Intelligence,then it is mandatory for a single system to learn multiple tasks. Here , We will start with the general setting of continual learning and then begin to investigate how various features of the problem are solved by current methods. . Note: None of the known methods satisfactorily solve Continual Learning problem in it&#8217;s General Setting. . Continual Learning : The General Setting Optimize a defined loss function under Infinite Stream of Data,without any explicit distinction between tasks. Note : Here &#39;task&#39; is artificially introduced to represent variation in data distribution and even loss function. . The following are some of the desirable features of the learning scheme 1: . Fixed Memory : Memory requirements cannot grow with each additional task. . | No Task Boundary : Learn from the input data without explicitly defining task boundary increases the flexibility of the method and also reflects real-world setting. . | Online Learning : No offline storing of data.(i.e training with large batches for multiple epochs) . | Forward Transfer : Previous tasks should ease the learning on new tasks. . | Backward Transfer : Upon Learning future tasks, performance on correspondingly related past tasks should improve. . | Problem Agnostic : Should be flexible enough to work with different loss functions. . | . Important: Humans do not exactly operate in the General setting of &#8217;Continual Learning&#8217;. We often have local control over the stream of tasks. We decide,apriori the sequence of tasks to perform in order to reach a desired goal. We also assign ourselves tasks(thus access to task labels) in order to optimize our goals. . Implicit in the above definition,we expect the learning procedure to learn flexible representations. Traditional machine learning paradigm aims to learn a specifit task - machine translation,Image recognition,Speech synthesis - end to end,often from scratch. This is facilitated by training on large amounts of data,specific to the task.Consider Classical MNIST and CIFAR-10 Datesets,that have propelled progress in Computer vision. Each of these have 60,000 Images split equally between ten classes. This is in sharp contrast to how a human experiences learning. Our learning involves learning large number of classes,with few examples from each class.(say 6000 classes with 10 examples for each). Thus,our learning procedure is optimized for this setting and we are forced to learn concepts and abstractions that let us learn quickly and efficiently with less data. Later in this section,we will see techniques that tries to alleviate this problem in modern neural networks. . Let&#39;s consider three scenarios,which will better help us to better parse the existing literature in the field. . $e^{i pi} + 1 = 0$ . $$ sum_{i=0}^n i^2 = frac{(n^2+n)(2n+1)}{6}$$ . Approaches . . The approaches taken can be broadly categorized into three groups 2: . Modify the Update Rule . | Replay methods . | Use Semi-distributed representations . | Meta-Approaches . | Here we will cover some representative works from each of the above categories. . 1. Modify the Update Rule . Instead of updating all the weights on a new task, the most important weights are retained. . 4. Meta-Approaches . First, We will try to understand the various incarnations of these approaches and finally conclude with some recent works that try to leverage Meta-Approaches for Continual Learning. . Meta-Learning is a general paradigm that aims to condition the learning rule itself based on pre-defined properties of the learned representations. This will become clear when we see concrete examples. But,for our task of continual learning, Meta-Approaches offer a way away from hand-engineered approaches to learning-based solutions. Let us explore few foundational works in this field,before we see a concrete example for Continual Learning. . . Note: I will not cover the complete spectrum of Meta-Learning approaches,just enough to appreciate how this paradigm allows for a more flexible approach towards solving Continual Learning. But,for a good strating point check Lil&#8217;Log&#8217;s Blog . MODEL-AGNOSTIC META- LEARNING (MAML) 3 . Notation : . T : The task, corresponds to the objective or loss function,domain,environment. . p(T): The distribution of tasks from which a new task is sampled. . f&theta; : A model(neural net) parametrized by $ theta$ . {Ti} ~ p(T) : Tasks used for meta training . {Tj} ~ p(T) : Tasks used for meta testing . {D$T_i$} : Meta-training set . {D$T_j$} : Meta-testing set . {DTtr} : Traning data for task T . {DTtest} : Test data for task T . LTi&lt;/sub&gt; : Loss or feedback from the sampled task Ti&lt;/p&gt; Given that we want our model to quickly adapt to a wide range of tasks, can we incorporate this inductive bias explicitly into our training ? . PROBLEM STATEMENT : . Learn the weights of a neural net that can quickly learn to perform well on a new task. . Method : . The intuition is to learn the right set of initial parameters that which upon few gradient steps on a new task,would result in good performance. . Consider a task Ti drawn out of task distribution p(T). Now, from this Ti we can sample training dataset Ditr,the loss/feedback on which is given by LTi&lt;/sub&gt;. This loss is parameterized by f&theta;. Suppose we take few gradient steps on this $ theta$ to give $ phi$. The error on test data set ,Ditest sampled from Ti,acts as training signal to update orginal f&theta;.&lt;/p&gt; What makes this process different from traditional training of neural networks is that the gradient is taken over meta-parameters $ phi$ through $ theta$. Consider the case of taking one gradient over LTi&lt;/sub&gt; to arrive at $ phi$.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; $$ phi_i = theta - alpha nabla_ theta mathscr{L}( theta,D_{T_i}^{tr})$$ . Now,the loss on $ phi$i (taken on $ {D_{T_i}^{test}} $ ) is backpropagated through $ theta$. . $$ min_{ theta} Sigma_{{T_i} thicksim p(T)} L( phi_i,D_{T_i}^{test}) = min_{ theta} Sigma_{{T_i} thicksim p(T)} mathscr{L}( theta - alpha nabla_ theta mathscr{L}( theta,D_{T_i}^{tr}),D_{T_i}^{test}) $$ . $ alpha : Learning rate(hyper-parameter)$ . Note that the meta-optimization is performed over the model parameters $ theta$, whereas the objective is computed using the updated model parameters $ phi$. In effect, the method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task. . . Now, We will see how this approach can be applied to Continual learning by just tweaking the problem statement. . $ mathscr{L}$ . For example, here is a footnote 1. And another 2 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ . h&theta;(x) = &theta;o x + &theta;1x . h&theta;(x) = &theta;o x + &theta;1x . 1. arXiv:1909.08383!↩ . 2. arXiv:1905.12588!↩ . 3. arXiv:1703.03400!↩ . 4. arXiv:1905.12588!↩ . &lt;/div&gt; .",
            "url": "https://vinayvarma.work/research/artificial%20general%20intelligence/2020/08/08/Continual-learning.html",
            "relUrl": "/research/artificial%20general%20intelligence/2020/08/08/Continual-learning.html",
            "date": " • Aug 8, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Basics of Deep Learning",
            "content": "About . This notebook is a walkthrough of the amazing book by Jeremy howard and Slyvian Gugger. To buy the orginal book : Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD. I am using their free git-hub version . Download files and unzip .",
            "url": "https://vinayvarma.work/jupyter/2020/07/02/fastai-book-walkthrough.html",
            "relUrl": "/jupyter/2020/07/02/fastai-book-walkthrough.html",
            "date": " • Jul 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Q-learning Algorithms",
            "content": "Introduction . Let&#39;s start with humans learning from experiance.Say we are trying to learn to bicycle. We are driven by a goal to stay balanced and pedal.Along the way we fall.Now we need to start again.Somewhere in the gap between each iteration,we are learning and improving.Let&#39;s hash out the features of this process,if we are to model this : . Past actions influence future output: There is no immediate feedback.Each micro-action(exerting more pressure on the pedal,..) along the way either leads to falling-off balance or keep moving. | Computational Problem : How to assign credit to actions when they are not temporally connected ? . Outcomes might not be deterministic: There are features of the environment(road,weather etc) that we do not fully understand that can effect outcome of the action. | Computational Problem: How to make inference about the properties of a system under uncertainity ? . Since out of the above two the latter seems to be simpler,let&#39;s start by building our intuition about Non-deterministic systems : Let&#39;s say we are given a Non-deterministic system whose properties we are unaware of but can only query it.How can we build our knowledge about the properties of this system ? Can we come up with a systematic way(algorithm) to estimate it&#39;s properties ? . query(),query(),query() #Fires different measurements each time. . (1.830642819404602, -18.865480422973633, 0.9646076709032059) . Now to understand about any of the properties of this &#39;query&#39;,we need to start with making some assumptions - say the numbers are coming from some unknown distribution with some stable(constant) mean. Let&#39;s start estimating this. . . Note: 1. How will your estimate of this mean change with each query ? . How does your confidence on this mean change with increasing number of queries $n$ ? | def estimate_mean(n): &quot;&quot;&quot;Returns list of estimated means after each query repeated for n times.&quot;&quot;&quot; list_n = [] # keep track of all the outputs from our slot machine list_mean = [] # collect the means after every sample. for i in range(n): out = query() list_n.append(out) list_mean.append(sum(list_n)/len(list_n)) return list_mean . Now let&#39;s say after each query we update our estimate of running mean. . estimate_mean(10) # The list of estimates for n = 1...10 . [-3.2570073008537292, 2.4307329952716827, -3.6120274662971497, -6.871547028422356, -6.159279823303223, -2.986470659573873, -2.4990574217268398, -1.9037501001730561, -2.7559810421533055, -1.0134802050888538] . #let&#39;s plot these means list_means = estimate_mean(100) plt.hist(list_means) . (array([ 1., 0., 1., 32., 59., 4., 2., 0., 0., 1.]), array([-4.82607206, -3.65700102, -2.48792998, -1.31885894, -0.1497879 , 1.01928314, 2.18835417, 3.35742521, 4.52649625, 5.69556729, 6.86463833]), &lt;a list of 10 Patch objects&gt;) . We can see that the calculated mean vary a lot but seem to be closer to $0$ most often..But how does our estimate itself depend on $n$(number of queries) ? . #let&#39;s measure the stability of our estimates by taking the difference of each successive estimates. diff_means = [list_means[i]-list_means[i-1] for i in range(1,len(list_means))] plt.plot(diff_means) . [&lt;matplotlib.lines.Line2D at 0x7f96ba5e1350&gt;] . Our estimate of the mean don&#39;t seem to change much after a while.This is interesting.This aligns with our intuition - with more samples, we can be more confident.We can even go about proclaiming that whatever the dynamics of the system it&#39;s mean might be constant ? . . Important: Remember this insight..for any system with stable mean we can be sure that after a while our estimate itself becomes stable. . Neural Nets with Q-LEARNING . Reference : Playing Atari with Deep Reinforcement Learning . Key Questions : . 1.Can we use neural nets in RL setting where the observations are correlated and Non-stationary(non-iid) ? . solution : Experiance Replay . Context . The key algorithm introduced in the paper is summarized below but we need necessary context before naively implementing it. Let us start with our knowledge of using Deep Nets in Supervised setting, and try to formulate the problem in similar context : . From each state the agent encounters we want the agent to output a probability distribution over all possible actions, which when sampled from results in optimal return. So, if we can have access to the labels(optimal actions) from each state we can easily create a loss function and proceed with SGD over all the states. . Here&#39;s a simple approach for bootstrapping the labels: . Play the episode multiple times. . | Record the returns . | Choose the episodes with highest return. These might be the ones that the agent chanced upon good actions,so train on them. . | Cross-Entropy method . Here&#39;s how the environment looks like: . import gym import torch import numpy as np env_name = &quot;CartPole-v0&quot; env = gym.make(env_name) def moving_average(x, w): return np.convolve(x, np.ones(w), &#39;valid&#39;) / w . Here we will use gym&#39;s cartpole environment. Let&#39;s quickly check the average return under random actions. . #collapse-show def play_episode(env,obs_acts,sampler): ob = env.reset() done = False rewards = 0 while not done: acts = obs_acts(ob) actn = sampler(acts) next_obs, reward,done, _ = env.step(actn) rewards += reward ob = next_obs return rewards . . Average return for 1000 episodes . np.mean([play_episode(env,lambda x : env.action_space.n,np.random.choice) for i in range(1000)]) . 22.295 . Now that&#39;s the average return of a policy taking random actions at each state. In previous section, we have seen how properties of any distributin can be estimated in a step-wise manner,with each interaction resulting in more accurate estimate. But, here the reward from the environment depends on our actions(In this case moving left and right) at each state,with each action resulting in a different mean reward. Our task is to learn this mean reward corresponding to each action at a given state. . #collapse-show import torch import torch.nn as nn obs_size = env.observation_space.shape[0] hidden_size = 128 n_actions = env.action_space.n net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) . . # Bootsrapping with the cross-entropy loss of best episodes. from collections import namedtuple import torch.optim as optim obs_size = env.observation_space.shape[0] hidden_size = 128 n_actions = env.action_space.n sft_max = nn.Softmax(dim=1) net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) def train(net,data,loss,optmizer): optimizer.zero_grad() keys,labels = data pred_vals = net(keys) loss_v = loss(pred_vals,labels) print(f&#39;loss before update {loss_v.item()}&#39;) loss_v.backward() optimizer.step() print(f&#39;loss after update {loss(net(keys),labels).item()}&#39;) def categorical_sampler(): def sampler(probs): return np.random.choice(len(probs),p=probs) return sampler def play_episode(env,net,sampler): episode_step = namedtuple(&#39;episode_step&#39;,field_names=[&#39;obs&#39;,&#39;actn&#39;]) lis_steps = [] episode = namedtuple(&#39;episode&#39;,field_names=[&#39;tot_return&#39;,&#39;lis_steps&#39;]) ob = env.reset() done = False rewards = 0 while not done: acts = sft_max(net(torch.FloatTensor([ob]))) #import pdb;pdb.set_trace() actn = sampler(acts.data.numpy()[0]) lis_steps.append(episode_step(ob,actn)) next_obs, reward,done, _ = env.step(actn) rewards += reward ob = next_obs return episode(rewards,lis_steps) . batch_size = 20 percentile = 75 sampler = categorical_sampler() def get_batch(env,net,batch_size,sampler): batch = [] while len(batch) &lt; batch_size: episode = play_episode(env,net,sampler) batch.append(episode) return batch . batch = get_batch(env,net,10,sampler) lis_returns = list(map(lambda x: x.tot_return,batch)) np.mean(lis_returns) . 27.0 . Now,let&#39;s train . #collapse-show mean_returns = [] loss = nn.CrossEntropyLoss() optimizer = optim.Adam(params=net.parameters(),lr=0.01) while True: batch = get_batch(env,net,10,sampler) lis_returns = list(map(lambda x: x.tot_return,batch)) mean_return = np.mean(lis_returns) mean_returns.append(mean_return) ret_threshold = np.percentile(lis_returns,percentile) obs = [] actns = [] filtered_episodes = filter(lambda x : x.tot_return &gt;= ret_threshold,batch) if mean_return &gt; 199: print(&#39;training complete&#39;) break else: for episode in filtered_episodes: obs.extend(list(map(lambda x: x.obs,episode.lis_steps))) actns.extend(list(map(lambda x: x.actn,episode.lis_steps))) print(f&#39;{len(mean_returns)} mean return = {mean_return},threshold = {ret_threshold}&#39;) train(net,(torch.FloatTensor(obs),torch.LongTensor(actns)),loss,optimizer) import matplotlib.pyplot as plt plt.plot(moving_average(mean_returns,10)) . . [&lt;matplotlib.lines.Line2D at 0x12107b950&gt;] . It&#39;s cool that a simple approach derived out of our intuition actually worked. Now let&#39;s check the performance of the same algorithm on FrozenLake environment. . env_fl = gym.make(&#39;FrozenLake-v0&#39;) . Home Work 1 : . Solve the above environment with the above method. For details of the environment refer FrozenLake . CrossEntropy Method : Why it Works ? . Due to the frequent use of CrossEntropy method in Supervised learning setting we have not spent time dealing with it&#39;s formulation. Here&#39;s the necessary deep dive : . Deep-Q Learning . Prerequisites . Psuedo-code . Code . #collapse-show from collections import deque batch_size = 100 batch = deque([],batch_size) min_batch_size = 30 env_name = &quot;CartPole-v0&quot; env = gym.make(env_name) obs_size = env.observation_space.shape[0] hidden_size = 128 n_actions = env.action_space.n epsilon = 0.2 net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) def e_greedy_sampler(epsilon): def sampler(qf): nped_qf = qf.detach().numpy().squeeze() if np.random.uniform(0,1) &lt;= epsilon: return np.random.choice(len(nped_qf)) else: return np.argmax(nped_qf) return sampler def get_pred_q(episode,gamma=0.99): rewards = [step.r for step in episode] pred_q = [step.qa for step in episode] #appending &#39;0&#39; for terminal state. pred_q.append(np.array(0)) labels = [] for i in range(len(rewards)): labels.append(rewards[i]+gamma*pred_q[i+1].max().item()) return labels class QAgent(): def __init__(self,net): self.net = net def act(self,ob,fn = lambda x: x): return fn(self.net(ob)) def train(self,data,optimizer,loss_type =&#39;squared&#39;): q_actions = [] actions = [] labels = [] for step,label in data: labels.append(label) q_actions.append(step.qa) actions.append([step.a]) q_tensor = torch.cat(q_actions,0) a_tensor = torch.tensor(actions) labels_tensor = torch.tensor(labels) q_actions_taken = torch.gather(q_tensor,1,a_tensor) if loss_type == &#39;squared&#39;: loss = nn.MSELoss() loss = loss(q_actions_taken.squeeze(-1),labels_tensor) #print(f&#39;loss:{loss}&#39;) loss.backward(retain_graph=True) optimizer.step() agent = QAgent(net) # The same network we used in cross-entropy method. #change our previous play_episode to store (ob,a,q,r,next_ob) def play_episode(agent,env,sampler,n_steps=1): epi_step = namedtuple(&#39;epi_step&#39;,field_names=[&#39;ob&#39;,&#39;a&#39;,&#39;qa&#39;,&#39;r&#39;]) episode = [] pred_q = [] tot_return = 0 ob = env.reset() done = False while not done: qf = agent.act(torch.FloatTensor([ob])) action = sampler(qf) #import pdb;pdb.set_trace() next_ob,r,done,_ = env.step(action) tot_return += r episode.append(epi_step(ob,action,qf,r)) ob = next_ob pred_q = get_pred_q(episode) return tot_return,[(episode_step,pred_q[i]) for i,episode_step in enumerate(episode)] sampler = e_greedy_sampler(epsilon) optimizer = optim.Adam(params=net.parameters(),lr=0.01) threshold = 199 def train(sampler,optimizer,threshold,agent,env): return_vals = [] counter = 0 while True: episodic_ret,lis_step_label = play_episode(agent,env,sampler,n_steps=1) counter += 1 return_vals.append(episodic_ret) #print(f&#39;{counter} return == {episodic_ret}&#39;) if episodic_ret &gt; threshold: #print(&#39;solved.&#39;) break batch.extend(lis_step_label) if len(batch) &gt; min_batch_size: idx = np.random.choice(len(batch),min_batch_size) optimizer.zero_grad() loss = agent.train([batch[id] for id in idx],optimizer,loss_type=&#39;squared&#39;) return return_vals return_vals = train(sampler,optimizer,threshold,agent,env) . . plt.plot(return_vals) . [&lt;matplotlib.lines.Line2D at 0x1a28cc8f50&gt;] . That took about 700 episodes and there&#39;s almost no general trend.Let&#39;s see if could replicate : . # Run this when you got lot of free time. lis_return_vals = [] for i in range(20): net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) agent = QAgent(net) lis_return_vals.append(train(sampler,optimizer,threshold,agent,env)) . #but here&#39;s a sample plot : plt.plot(lis_return_vals[0]) . [&lt;matplotlib.lines.Line2D at 0x1a2b106910&gt;] . Home Work 2 : . Though we seem to have solved the environment,the training(episodic rewards) process seems too unstable. . Can you make it stable ? | List some of the possible reasons for instability ? | Bag of Tricks : . . Tip: SOME OBVIOUS TECHNIQUES . In our implementation of Deep-Q learning agent we have kept epsilon constant throughout the training. We can anneal it towards a lower value as the training proceeds. | Our update was Q(i) = r(i) + gammaQ(i+1) . So after every batch of episodes(after the update) we would expect our Q(i)&#39;s closer to the labels (r(i) + gammaQ(i+1)) but due to the proximity of Q(i) and Q(i+1) updating Q(i) also changes Q(i+1). Unlike supervised setting, our labels here are a not constant,thus introducing instability in training.We can keep our labels constant for a couple of batches ( by storing the weights and using them to produce labels). | Well, DeepMind pushed this a whole lot further by adding several other techniques in this paper : . Rainbow: Combining Improvements in Deep Reinforcement Learning .",
            "url": "https://vinayvarma.work/reinforcement%20learning/2020/05/06/Q-Learning.html",
            "relUrl": "/reinforcement%20learning/2020/05/06/Q-Learning.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Policy Gradient Algorithms",
            "content": "",
            "url": "https://vinayvarma.work/reinforcement%20learning/2020/05/06/Policy-Gradients.html",
            "relUrl": "/reinforcement%20learning/2020/05/06/Policy-Gradients.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Collected Thoughts",
            "content": "Intention vs Expectation . There is a subtle difference between intention and expectation. We can have intention without any expectation. Intention comes without any bondage. Expectation pulls us into unnecessary drama – our life will depend on the currents of situations, drawing out life energies. Through intent and purpose, we express ourselves. Life is nothing but an expression of energy. . Well-being . Seeking acknowledgment and validation from others creates psychological dependency. Longing to be around people arises out of this compulsion to validate. We seek to be cared, respected, included. These primordial drives have kept us alive. All compulsions seem to arise out of a center and work to keep it alive. This is a live process that is happening beneath our awareness. Raising awareness towards this process brings well-being. . Beyond chatter . We have incredibly complex minds. Capturing it in a Self-image or ego is futile. Ever refreshing attention to the present is all that’s needed to capture the enormity of life. Ego constricts.Beyond chatter, there is a profound experience. . Inner Exploration . We are our minds.All that we call life arises out of it. Most of the time, our conscious brain runs on auto-pilot – compulsively rolling from thought to thought. Paradoxically, without any force but by just observing these thoughts and sensations as they arise in consciousness seems to have an immediate calming effect. The experience of paying attention to the contents of consciousness brings about fundamental changes in the very perception of life. It would be a disaster to go through life without ever exploring the depths of it. . Experience . Seek experiences. Personal transformation arises under experiences bereft of any thought or rationalization. These leave a deep impression on us that makes change inevitable. If you fantasize a life of leisure and travel – take a small vacation, Curious about spirituality – take a day off and meditate. We are capable of following irrational group norms based on beliefs and ideals alone. I would rather explore life grounded in reality than led by the figments of cultural imagination. Poke around your boundaries, seek experiences. . On Impermanence . Thoughts, feelings, emotions, beliefs change continuously. An appreciation of this fact is needed for a balanced life. Almost nothing of me remained from a decade ago…the contents of my consciousness and body have changed completely. But, there is an element in me that stitches a sense of continuity. The grip of the ephemeral loosens when one becomes aware. Not believe or theorize. . On happiness . It all starts with doubting human condition. Why is it that happiness comes after fulfilling a desire and lasts only for a while? So we are bound to hop from a desire to desire, accumulating whatever angst along the way. Can there be joy untethered to desire ? If not, then a sensible solution would be to have easily achievable goals. This makes the whole hopping thing seem more continuous stream of happiness. But, do we have control over our desires ? . On Love . We all seem to share a longing to expand. To become more. This drive manifests as a desire for wealth, status, recognition or freedom from all these. Our minds are not satisfied with the boundaries of our physical bodies – there is an element in us that is seeking to be boundless. What is stopping us from loving unconditionally ? . On Education . Education can be better. A complete human being unsmitten by the relentless need to compare, capable of love is a rarity. When a child passes through a decade or so of education – his capacity towards life should be enhanced. Awareness brings about inner harmony that resolves the need for religion, government, authority, morality. Raising the awareness of our species is the best engineering solution against war, crime, pollution, and self-induced suffering. . Stretch your limits . Thought is limited to the past. It can’t operate without memory. And memory is limited and fallible. Belief is restricted thought. Collection of beliefs about oneself creates Ego. When a free thought disturbs the certainty of a belief – Ego shatters. Ego feeds on the stability of identity. Seeking to stretch your limits eventually renders Ego useless. . Joyful life . We are the creatures of limited time and energy. What we choose to make out of these is entirely our choice. Awareness of these basic constants leads to a responsible life. Complete responsibility equals absolute freedom. When we realize our ability to respond to life, events, thoughts will live a joyful life. . The Vicious cycle . We Sense the world. And then we associate. We then segregate them into good and bad. Any further perception is limited by these associations. . Discipline . It takes some effort to stick to a regimen. But it’s the only way to transform oneself. A sense of personal growth seems to be very fundamental for a good life. We all have this longing to expand and grow beyond ourselves. Whatever the means one may choose, discipline is the tool. . Keep it simple . Life is simple. Catch yourself, whenever you are trying to make it otherwise. Do not escape reality. Acknowledge your feelings and desires without judgment. Do not hurry. There is no compulsion to reach conclusions. Appreciate uncertainty. We can take better actions under this framework. Forcing reality is a loser’s game. Keep it simple. . Pay the dues . Pay attention to your desires and fears. We are complex creatures. Often we are unsure of what we want. It takes deliberate struggle to evade dissatisfaction and boredom. So let it be. Pay the dues. . Take Charge . Imagine. Consciously, not impulsively imagine what your ideal life should be like. Imagine freely, the kind of family, the nature of work and the world you would like to live in.Feel it to the bone. Now, do What you should be doing today to move in that direction. You can either live intentionally or succumb to the compulsive desires sold to you. There is always somebody dictating you the norms, selling you their fears and insecurities. Taking charge involves paying enough attention to these superficial desires, so they lose their grip. To connect with your deepest longings, you need to imagine freely and act. The feedback can only come through action. So, it’s better to imagine yourself, take charge and act. . Anger . It’s absorbing. But, it damages both the ends. The physical sensations are not pleasurable,often the actor feels more pain than the recipient. A moment’s awareness settles everything down. Somewhere along the line,control is being dropped. It is a fun exercise to actively take note of these situations , and seek to involve in them. To non-judgmentally observe your response. It becomes impossible to be both conscious and angry. .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/05/05/collection.html",
            "relUrl": "/think%20on%20these/2020/05/05/collection.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Thoughts on Building Things",
            "content": "Start early, start now . Start early. Wherever you are, now is the time. Those who made it might not tell you but luck is a big factor. The earlier you start, the more can you fail. It’s an odd’s game. Don’t be fooled by the myth of ‘Genius’ nor the ‘Hard work’. Cowards subscribe for ‘the genius’ while the winners and aspiring ones go for ‘the ‘Hard work’.Both are overly result-oriented. Planning for the worst and giving your all is the name of the game. And having time on your side lets you play it for a tad longer. Start now. . Intensity, at will . Spend some time with yourself. Remove all the distractions – thoughts, compulsions. Cultivating a craft where you can be completely absorbed, will open up a new dimension. When you remove distractions, life intensifies. This percolates into all areas of life. You will become capable of intensity, at will. . Nobody to please . Learn to disagree. Compulsively worrying to satisfy and please others at the stake of personal wellbeing is a pathological condition. Being spiteful and nasty is not an option. But, not negotiating for your well-being is pathetic. Over time, the effects percolate. Personal beliefs are altered to fit the reality. True capacities are shunned and never realized. Agreeable people are paid less. Their position in social hierarchy remains mediocre. There is no moral obligation to please anybody. . Create a routine . Create a routine. Else you will be put in one. Exercise your freedom to choose what you want to do. Repeating the same steps every day is the only way to gain mastery. It is by building this reservoir of work you can sustainably stick with it. This is your ticket to freedom. Uncommitted become the slaves of norms. They will always be struggling to escape, always at odds with reality. start with your body. Have an exercise routine that can be maintained. Next, Work. Choose what you want more of and create a routine around it. Do not be tempted to fit too much here. Slowly you will develop expertise and confidence that will let you do more of it. . The most important thing . Cultivate the ability to break-down complex problems into simpler ones. This is the meta-skill that translates into greatness in every field. Accomplishing ridiculously difficult tasks demands the ability to visualize the composing sub-tasks and formulate daily routines around them. On any day, there should always be one thing that takes your complete attention, irrespective of the breadth of the project. As an entrepreneur, your sole responsibility is to answer what is the most important thing, now ? . On work . Selling time for money is addictive. It nullifies the odds of accumulating wealth. It makes one blind to possibilities, incapable of imaging the power of leverage. Aspiring entrepreneurs should never work for money. They should actively avoid fixed pay contracts. Only under the circumstance of receiving rare skill, knowledge can they transact their time. Actively seek mild discomfort. Once succumbed to the safety of a job, anything creative sounds dangerous and implausible. The closer you can be to the creative process, the better. For employment, Startups are the best bet. Manipulating your environment is the most direct way of influencing outcomes. Under no circumstance, your time should be spent away from the creative process. There are always better options. . Luck . There is a controllable element to what people refer to as Luck – being in the right place at the right time. Depending upon what you want to do, there are great places(people) around the world. Anything that can structurally avoid complacency, is a great choice. We should look for as much discomfort as possible and the most sensible way is to place yourself in a hard place with hard people. . Consistency . Don’t directly rush into doing something. Take some time and put some labor to find out why you want to create something. Only when you are clear on Why proceed further. The next important question is what you can do right now to get a bit closer? Daily commitments should be so simple that they sound laughably easy.This ensures momentum, as we achieve them daily. We are not naturally good at forecasting the cumulative effects of simple habits. The key to accomplishing ridiculously difficult tasks is consistency, not the quantum of work done on a day. . Progress . Clarity of what you want in the long term is absolutely crucial. Define it. Take some time and write it down. Yes, it might evolve. But you should have a clear understanding of the essence. Ask specific questions like How do you want your day to be? How does your ideal work environment look like? How does your relationships, health, financials look like? Create a practice to develop deeper understanding of your longings? Only with clarity of vision can there be a possibility of progress. . Hard Work . Working more hours is not hard work. Putting your heart to create something is not hard work. Constantly stretching your capabilities is not hard work. Taking responsibility is not hard work. Working for the paycheck, bearing and accepting the inanity of your work, waiting for the weekend, passing through life without intensity is Hard Work. . Every Day . Focus on what you are doing every day. Start with the smallest change that can be incorporated into your routine. Now stick to it every day. This is the secret of accomplishing impossibly difficult tasks. Focus on the routines and habits. Nothing more is needed. . Fear is good . When you are starting something new, the fear of failure is often useful. This forces you to focus on risk. Being able to use this fear to ask the right questions, to strategies and come up with novel ideas is the defining skill of a successful entrepreneur. In the right proportions, it illuminates threats, directs the energies. Fear is good. . Copying Ideas . Don’t get lost in the “next big idea” fallacy. There are two practical ways to come up with successful business ideas. Look for the problems you are facing or Copy from the solutions other entrepreneurs successfully implemented. The first category involves listing down all the activities in your day while thinking how each one of them can be made better. Now, once you choose a few, write down how much you are willing to pay and the kind of service or the product you want. If an idea filters through this process successfully, it can be considered further. Don’t fool yourself here,if you aren’t willing to pay chances are nobody will. The second category is much simpler. Choose an industry. (better you already got some idea about it.).Now find out what’s happening within it in other countries. Read their magazines, news sites, etc. Now the question boils down to, What successful business products and models can you take inspiration from? Yes, that’s called copying ideas. Hopefully, you are doing all this to create some value, not justify your genius. . Little by little . We lack the ability to foresee the effects of small changes accumulated over time. Habits are remarkable ways to leverage the power of compounding. Introspect how your daily time is spent. Add or substitute a habit at a time. There is no hurry. This is a long term game. Keep the changes to absolute minimum.In the initial stages, momentum is the key. Whatever your ideal time or effort, reduce it by half – this should be your initial commitment. When creating habits, setting ridiculously easy goals so that it’s impossible to fail is the key. Consider any extra effort put as the bonus and feel great about it. Little by little your life will be transformed. . Attention . Attention is limited. Any practice or technique that allows you to direct your energies will have a compounding effect on your life. Measure your day by how much of it is intentional. There are always people and things constantly attempting to grab your attention – the News, Internet, fancy toys. It’s a skill that can be trained. .",
            "url": "https://vinayvarma.work/entrepreneurship/self-empowerment/2020/05/05/On-Buliding-things.html",
            "relUrl": "/entrepreneurship/self-empowerment/2020/05/05/On-Buliding-things.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Dissatisfaction",
            "content": "Dissatisfaction. This is the most subtle and malignant force within us. The majority of life’s efforts are to escape this feeling. Our desires come out of deep dissatisfaction with the present and hope to escape from it in the future. But this is a ceaseless process, always keeps us tethered to the future. It’s funny that life always happens now. .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/05/05/Dissatisfaction.html",
            "relUrl": "/think%20on%20these/2020/05/05/Dissatisfaction.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Create a routine",
            "content": "Create a routine. Else you will be put in one. Exercise your freedom to choose what you want to do. Repeating the same steps every day is the only way to gain mastery. It is by building this reservoir of work you can sustainably stick with it. This is your ticket to freedom. Uncommitted become the slaves of norms. They will always be struggling to escape, always at odds with reality. start with your body. Have an exercise routine that can be maintained. Next, Work. Choose what you want more of and create a routine around it. Do not be tempted to fit too much here. Slowly you will develop expertise and confidence that will let you do more of it. .",
            "url": "https://vinayvarma.work/productivity/2020/05/05/Create-routine.html",
            "relUrl": "/productivity/2020/05/05/Create-routine.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://vinayvarma.work/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "On Belief",
            "content": "Belief is the easy thing. It demands no cognitive load. Decisions are made almost unconsciously. Doubt takes deliberation. Belief is common. It brings together people and offers safety. Groups welcome or ostracize individuals based on common beliefs.Evolution further selects these individuals. The desire to be identified with a group is an ever operating natural force. This raises the odds of our survival in the wild. While our well-being lurks on the fine balance of Doubt and Belief. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/25/On-Belief.html",
            "relUrl": "/think%20on%20these/2019/09/25/On-Belief.html",
            "date": " • Sep 25, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Love",
            "content": "A heart brimming with love and a radiant body are necessary to experience life at its fullest. Love doesn’t require anything from outside one’s agency. It doesn’t demand people to be less mean nor for us to constantly look for justification in their actions. It doesn’t arise out of any expectation from oneself or others. Whoever is around you, the habit of having good intentions for their life creates immediate wellbeing. Giving your best to the task, say in playing football for your team, might involve strategizing against your opponent. But, nowhere does it require you to change your intentions. It only becomes a zero-sum game when you rest your source of joy outside yourself – like in winning a game or getting a promotion. Then, inevitably subtle forms of hate arise helping us maneuver through dissatisfaction. But, this leaves us incapable of Joy. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/25/Love.html",
            "relUrl": "/think%20on%20these/2019/09/25/Love.html",
            "date": " • Sep 25, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Inefficiency",
            "content": "Life is mostly inefficient. We end up doing things that in retrospect offer no value. We regret. Yet, this trial and error process is the only way to discover. All our expectations, thoughts, longings are based on our present level of awareness. We cannot act like our future selves.Realizing this is self-empowering. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/23/Inefficiency.html",
            "relUrl": "/think%20on%20these/2019/09/23/Inefficiency.html",
            "date": " • Sep 23, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "Now",
            "content": "It’s a strange realization. There is only now. All our dreams, if realized will have to dawn on some now. There is no escape from now. All the sensations that we derive from indulging in work, thoughts will be experienced now. So the scope of our life depends on our capacity to experience now. It’s a strange realization . Life is happening now. Whether I plan for future, muse on my past or feel the burning pain of my wound , It all happens right now and that’s all I got. A moment arises within the cessation of the previous one. There is no way to latch. In thinking about the future – all my sensations whether the excitement or the dread happens in the present. So it follows whether can I keep myself joyful right now – without the necessity of a person, thing, belief or ideology. That would be cool. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/17/Now.html",
            "relUrl": "/think%20on%20these/2019/09/17/Now.html",
            "date": " • Sep 17, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "Move",
            "content": "Life is in motion. Stagnation is death. Take action. That’s the only way to interact and influence. Time is ticking away. There is nothing to be preserved. So move into every moment - without baggage and drama. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/03/move.html",
            "relUrl": "/think%20on%20these/2019/09/03/move.html",
            "date": " • Sep 3, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "Mental states",
            "content": "The joy of achievement, the pain of losing a loved one are fundamentally different mental states, invoked under certain conditions. The desired mental states we all seek are similar. In life, we try to line it up with certain mental states while avoiding others. This is the origin of all our desires. So it seems pertinent to find out the controller of these states? From where do they arise ? Does pre-conditions matter to get into a certain state ? .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/08/28/Mental-states.html",
            "relUrl": "/think%20on%20these/2019/08/28/Mental-states.html",
            "date": " • Aug 28, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Action",
            "content": "Only tools we are in complete charge are our body, mind. All life happens through them. It pays to think in terms of this simple model. Much of the unnecessary suffering arises out of confusion with other minds. We have no access to the consciousness of other humans. So we cannot control their likes, dislikes, affection towards you. But, we seem to be in constant conflict with this reality. Our relation to the world occurs in pairs of action and expectation. But, can there be action without a desire or exception? .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/08/19/Action.html",
            "relUrl": "/think%20on%20these/2019/08/19/Action.html",
            "date": " • Aug 19, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://vinayvarma.work/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vinayvarma.work/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}