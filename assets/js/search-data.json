{
  
    
        "post0": {
            "title": "Inside Spotify's Recommender system",
            "content": "For demonstration purposes, I&#39;ll take the liberty of citing a couple of my recent papers, namely the first SignalTrain paper . References . .",
            "url": "https://vinayvarma.work/reinforcement%20learning/2022/05/23/Recommender-systems.html",
            "relUrl": "/reinforcement%20learning/2022/05/23/Recommender-systems.html",
            "date": " • May 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Linear Algebra -  (Chapter 0.1)",
            "content": ". Note: These are not meant to be used for self-teaching the subject. My objective is to collect the summary of all the ideas that a working Machine Learning Researcher should be aware of. Use this as a reference to check or deepen your understanding. At the start of each post I will provide an opinionated learning strategy to teach yourself the subject from scratch in the shortest possible time. In most cases the post is based off those resources, thus the credit is due to the orginal authors. . Pointers to learn from scratch (Go through these resources in the given order.) . Read and workthrough the first 4 chapters of the book mml. . Watch through this series of videos by 3Blue1Brown - Essence of linear algebra along the way. . For all practical purposes this should suffice. But, I would also suggest to read through Linear Algebra Done Right . . Note: I tried to build the subject from ground up. This will inevitably involve some proofs. You are not expected to be able to prove these but it&#8217;s good to see them atleast once. Conveniently, I have done these proofs via handwritten notes making annotation and commentry easier.I tried include the minimal set of proofs to allow the reader to deeply appreciate the topic and it&#8217;s applications. . Central Objects : Vectors and linear maps . Here we will define the lead actors - vectors(objects of study) and linear maps(mappings between these objects) . Vector Space : Set of all objects called vectors that satisfy some intuitive constraints. Consider the following vector spaces. . . Note the commonality between the following objects. For illustration let&#39;s use geometric vectors, that can be seen as pointed arrows within a coordinate axis, represented as a list of numbers. . . All these satisfy some intuitive properties once we define addition and scalar multiplication. Let&#39;s call all such objects Vectors . . Linear Algebra studies the emergent properties of objects once we impose simple properties like commutitivity,associativity etc. Note that we are just formalizing the familiar notions. This helps us discover some very interesting properties as we shall see. . In short, we introduced the notion of vector space which consists of . A set X of vectors | A set C of scalars . such that vectors are closed that are closed under addition,subtraction and scalar multiplication and satisfies natural associative and distributive laws. . | . Linear Maps . Linear Maps are the functions that map between the elements of the vector space. But not all mappings, only those that satisfy linearity conditions as outlined below: . . Now,If you have studied Linear Algebra, you might have jumped directly at geometric vectors and matrices. Below we will take a slightly more abstract approach that will pay dividends when we study arguably the most used decorated area the subject Matrix Decompositions - SVD and Eigen Value decomposition. . Idea 1: Spanning Vectors . Big Idea 1: All the elements of a vector space can be represented succintly as a linear combination of few a vectors. In other words,Irrespective of the vector space (geometric vectors,polynomials, functions etc), there exists a set of few vectors that can be used to generate all the elements of that set. This section builds up the material to prove this fact. . Subspace . There exist a subset of the vector space that also satisfy the properties of the vector space. This will be more clear via examples below. . . . Tip: To check if a set forms a valid Subspace check if the elements of the set are closed under a.addition and b.zero element belongs to the set c. closed under scalar multiplication. Rest of the properties(distributivity,associativity etc) are anyways satisfied by definition(as these are subset of the vector space). . . Now it is sensible to ask whether by combining two subspaces(UNION,INTERSECTION) can we get another valid subspace. Convince yourself that the INTERSECTION of two subspaces is always a subspace which is not the case for UNION. If we define the notion sum of two subspaces as the collection all elements that can be written as sum of an element from each subspace, we note that this set is also a vector space thus a subspace of the original. Also this is the smallest subspace containing both the subspaces (just like how union of two sets in set theory is the smallest set containing both the sets). . . . . Important: Understand the above claims before proceeding. . Span,basis and linear independence . Note that give two vectors $v_1$ and $v_2$ the set $V = { lambda_1 hat v_1 + lambda_2 hat v_2: lambda_1, lambda_2 in C }$ forms a subspace. Also note that if we have another vector $ hat{v_3} = 2 hat{v_1}$ and define the new set $V&#39;$ as $V&#39; = { lambda_1 hat v_1 + lambda_2 hat v_2 + lambda_3 hat v_3: lambda_1, lambda_2, lambda_3 in C }$, we will still have $V = V&#39;$. Using $ hat {v_3}$ doesn&#39;t give us any new vectors that is not already in $V$. Let&#39;s add two new words to our vocabulary Span and Linear Independence. . . Below we see how span and linear independence are related. . . Now, we are certain that given a subspace spanned by $n$ vectors, the subspace can also be spanned by a set of $k$ linearly independent vectors where $k &lt;= n$. What&#39;s the size of the smallest $k$ that can still span the space ? . Basis: Set of the minimal number of vectors needed to generate all the elements of the subspace. It is easy to see that all such vectors should be linearly independent. (else it wont be the minimal set). Below examples will make the concept more concrete. . . It can be trivially shown that the cardinality of the basis set(#number of vectors in the basis set) is constant, no matter which set of independent vectors are chosen. For example, in case of geometric vectors in $R^{n}$, we could have chosen a differnt basis but it still has to have same count of vectors to span the same space. Thus we can call this cardinality the dimension of the vector space. . Now given a spanning set, how to get the basis ? . Just remove any vector that can be expressed as a linear combination of others until no more. At th end we are left with the basis set. | . Simalarly, We can build the basis set from a single vector(or a subset of the basis set) by adding vectors that cannot be expressed as a linear combination of the given set but belongs to the subspace. . To drive all these points home, let&#39;s solve some problems dealing with span,dimension and linear dependence. Along the way we will also learn how to solve system of linear equations. . Skill 1 : Basis and solution to system of linear equations . 1.Finding Solutions to system of linear equations. . This boils down to answering whether the right handside of the equation exist within the span of vectors defined by the equations as shown below. . . Now we know that any vector in $R^{4}$ can be spanned by a set of 4 linearly independent vectors. Thus, if the above vectors are linearly independent we will have a unique solution,else none.(note that the uniqueness comes from the definition of basis - minimal set of linearly independent vectors.). We haven&#39;t yet discussed how to find the coefficients - $x_1$,$x_2$,$x_3$,$x_4$. The techinque is called Gaussian Elimination in case the reader is not familiar check out this wiki entry.. Below is the solution. . . Solution Concept . Subtracting/adding one equation to the other does&#39;nt change the solution of a system of equations. Gaussian Elimination Provides a systematic algorithm leveraging this fact. In the above question the last row reads as 0=1,thus we have no solution. | . Now Let&#39;s see a case where there can be infinite solutions.Conceptually it means the set of vectors defined by the linear equations has redundant vectors.(more vectors than the dimensionality of the space). This will give us more than one way of reaching any point. . . NOTE: We have 5 vectors for a 4 dimensional space. . . Solution Concept . The idea is to reduce the vectors such that there is a 1 at a different slot for each of the vector,making reading off linearly independent vectors easy.(as any linear combination of the previous vectors cannot produce a value in the new slot.) In the above solution we note that $ hat v_1$,$ hat v_3$,$ hat v_4$ can produce 1&#39;s at 1st,2nd and 3rd index repectively thus are linearly independent. This makes $ hat v_2$ and $ hat v_5$ redundant. Also observe that we can write $ hat v_2$ as linear combination of $ hat v_1$,$ hat v_3$,$ hat v_4$ implying we can have a zero. Similarly for $ hat v_5$. Below note makes this clear. | . . . Important: If you have trouble solving above two questions, refer to the text mentioned at the beginning of the chapter. Here I only intend to show how the notion of span and independence translate to finding solutions to system of linear equations. . Before proceeding let&#39;s consider one last problem. . . let&#39;s start by find the basis for $U_1$ and $U_2$. This will let us infer the space spanned by both the sets. Note here how finding basis set is helpful as it&#39;s the simplest representation of the underlying vector space. . . Summary . We understood and defined the notions of vector space, linear maps , span and linear independence. Using only abstract definitions ( with out the need for geometric vectors in $R^{n}$ ), We realized that every vector space is characterized by a set of basis vectors. It turns out that these basis vectors suffice to succintly represent the entire vector space, due to the following property. . Uniqueness: Let $V$ be a vector space and B be a set of basis vectors. Then every vector in $V$ can be uniquely represented as a linear combination of vectors in B. Let us call the coefficients/scalar multiples Coordinates. | . Proof sketch: Write down the two different representations of a vector in $V$. Subtract the two and see that since the basis set is independent, we will have that each of the scalar corresponding to each vector equal to zero. (something like (a1-c1)$ hat b_1$ + (a2-c2)$ hat b_2$ $ dots$ = 0, implying a1=c1,a2=c2 etc as $ hat b_1$,$ hat b_2$ etc are linearly independent.) . Fixed Count: For any vector space eventhough the specific set of basis vectors might vary, the number of vectors in the set remain same. Let&#39;s call it the dimension. | . Proof sketch: This statement can be seen more clearly if we try to convince us of following two facts: . Consider a basis set B with size n of the vector space $V$. Any set of vectors smaller than n can&#39;t span the space $V$. . | Similarly and set of vectors in $V$, of size greater than n can&#39;t be linearly independent. . | . Above statements can be seen clearly by just attempting to write down what they imply. See how it boils down to characterizing solutions to system of linear equations. Below I show for one case: . . Above properties gives us an equivalence between vectors in $R^{n}$ and generic vectors. As any vector $ hat v$ belonging to a vector space can be uniqely represented by a list of co-ordinates [c1,c2,..cn] $ in$ $R^{n}$. . Example: . Find the coordinate vector of $p(x) = 4 - x + 3x^2$with respect to the basis $ {x^2, x, 1 }$ ? . It&#39;s just $(3,-1,4) in R^{3}$ . Idea 2 : Linear Maps can be represented as a Matrix. . We learned that vector spaces can be succintly represented by the corresponding basis set. Now linear Maps(defined in previous section) between vector spaces can be represented by a matrix. Once we are given the basis for a vector-space any vector in the space is defined by the corresponding scalar multiples. Take any vector in $R^2$, $ hat a = (x,y)$. It can be written as $ hat a = x hat e_1 + y hat e_2$, where $ hat e_1 = (1,0), hat e_2 = (0,1)$ are the vectors corresponding to co-ordinate axis(basis set). Also note how changing basis set changes the representation. Below I show how a matrix is just a convenient way of representing a linear mapping. . . . . Important: Note how the structure of the linear map(linearity) and the vector-space are crucial for this succint representation to be possible. . This immediately directs us how best to define operations with matrix. Given a vector and a matrix corresponding to the linear map, their multiplication should give the resultant vector after applying the map. Similarly,matrix multiplication with another. . . Important: Take time to internalize this. Typical definition of matrix multiplication might have seemed arbitrary but realizing that it is equivalent to the above might finally make it click. Also Matrix representation doesn&#8217;t make sense without specifying the corresponding basis-set. When not given we assume standard basis.($ hat e_1, hat e_2$ etc) . . Important: Before Proceeding Solve this toy problem (solution at the end) . . Better Representations : Change of basis, PCA, SVD. . Given some points from an n dimensional space, their representation will vary based on the chosen basis. Then, what is the right basis ? . Consider a linear map. We know that, It&#39;s representation varies based on the chosen basis. First let&#39;s learn how this representation changes when we change the basis. . Idea 3 : Change of Basis . Below I show how to get the representation(matrix) corresponding to a linear map when we know it for one basis and asked to get it in another basis. Here we used the symbol $P^{-1}$ to represent the inverse of a mapping. We will later discuss how to derive this from first principles,but the typical highschool algorithm involving determinants or gaussian elimination should suffice at the moment. . We already understand that a matrix vector multiplication is nothing but representing the given vector in different bases. To ensure we are completely clear on this, let&#39;s do a sample problem. . . Now let&#39;s see the effect of a linear map under the new basis. . . . . Let&#39;s walkthrough what we have done above again. . We realized that a matrix represents a linear map and vice-versa. | Give points in the basis $v_1$,$v_2$ we can find a linear map that finds new coordinates of those points with the basis $e_1,e_2$. We know how to get this matrix by looking at the effect of the linear map $P$ on the basis set $v_1$,$v_2$.(check the note on matrix multiplication,if this is not clear). | Now the trick is to get $B$, we can change the basis to $e_1,e_2$ and use the representation of the map under this basis - $A$ and then convert back to new basis,(round tour) to get $B$. | . Befor we proceed, let&#39;s understand linear maps,this would make our study of matrix decompositions more natural. . Linear Maps - Properties . We already know the definition of a linear map and the fact that it can be represented by a matrix. And we have also seen how the change of basis effects a given linear map. But why change basis ? How does these ideas help solve problems ? Let&#39;s gain insight by solving some problems. . Q1. What is the standard matrix for the transpose map of a 2*2 matrix w.r.t standard basis ? . Q2. Find the standard matrix of the differentiation map D : P 3 → P3 with respect to the standard basis {1, x, x2 , x3} ⊂ P3 . Q3. compute the fourth derivative of $x^2e^x + 2xe^x$. . Below We will see how changing basis is useful sometimes. The key is to see that matrix multiplication is easy for diagonal matrices. Thus if a give linear transformation be converted be done in a basis such that the corresponding matrix is diagnol - we can recursively apply that transformation easily. Will be clear with below examples. . Q4. Find 10th fibonacci number ? . Q5. What is the standard matrix for the transpose map of a 2*2 matrix w.r.t Pauli&#39;s basis ? . Q6. Compute $ int x^2e^{3x} dx$ using matrices . Extras . To further understand the structure of the vector space, below I share some other fun ideas. Some of these are a bit hard to understand and not strictly necessary. You can safely skip. . Solutions . .",
            "url": "https://vinayvarma.work/research/2022/01/11/Linear-Algebra.html",
            "relUrl": "/research/2022/01/11/Linear-Algebra.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Research notebook",
            "content": "Note : The following is my working notes,only to aid my thiniking. Likely not useful for others. . NeuroSymbolic AI . explicit variable manipulation(local/discrete) vs distributed representations. It is now accepted that learning takes place on a continuous search spaceof (sub)differentiable functions; reasoning takes place in general on a discretespace as in the case of goal-directed theorem proving . Applications: . Planning(RL) . A common thread across the above examples and applications is the needfor modellingcause and effectwith the use of implicit information . Once a symbolic description of the formif A thenBhas been associated with a neural network, surely the idea ofintervention[9] and counterfactual reasoning become possible .",
            "url": "https://vinayvarma.work/research/2021/11/01/WorkBook.html",
            "relUrl": "/research/2021/11/01/WorkBook.html",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Lesson learnt while building a Deep Reinforcement learning based automated stock trading service.",
            "content": "As of today, I have been working at Niveshi for nearly two-and-half years. We Started out in late 2018, with nothing but a rough idea at best to automate strategy generation using DeepRL. I will spare other details but I believe we have succeeded in building an automated pipeline from getting raw data to ready-to-deploy startegies. Here I&#39;m logging some lossons learned along the way. These will be useful for folks trying to make research ideas in Artificial Intelligence work in the real world, especially Deep RL. I will take stock trading as a running example,to illustrate some general principles. Some familiarity with RL is assumed. . Objective : What are we trying to solve ? . Find statistical regularities in the historical data and identify profitable trading rules. For simplicity, let&#39;s assume the data includes only historical price and volume. From RL perspective, given a set of time-series features learn to trade profitably. This amounts to outputting appropriate actions at every time-stamp, in order to maximize profits. . Usually techniques are sampled from traditional machine learning,signal processing and statistics. The recipe is as follows : . Hire a bunch of smart folks. | Give them the data and ask them to make money for the fund. | Incentivize them appropriately. | . Apart from the laundry list of problems that come with managing people, this approach is quite costly, and doesn&#39;t scale well. Markets quickly absorb any obvious edges. Consequently,any successful pattern,however sophisticated, will have a short shelf-life. This keeps the team busy continuosly searching for novel ways to leverage the data and extract profitble stratagies. DeepRL, atleast in theory, promises to automate all this. . Notes to my-self . How to gaurentee success with Neural Networks ? . Collect Large , Clean and Varied ( interesting samples,edge cases) data. | Train a large enough network neural network on it. | . For whatever reason if your problem can&#39;t meet the above conditions, get ready for a painful ride. . | How to deal with less data ? . signal = (amout of data) * (signal from each data point). . | Roughly,the amount of signal provided per data-point follow Self-supervision &gt; Supervised &gt; Reinforcement Learning. . Consider learning the structure of the data before attempting to formulate as the RL problem. . | Add auxillary losses. These might either fasten learning or can be used as standalone signals to improve the robustness of the base signal. . | When you are limited by the scale of data, ensemble of simple architectures can beat the effort over novel architectures. . | . Don&#39;t Fiddle with network-architectures yet. | When You begin workin on a new problem, Start with the simplest yet bespoke architucture you can find. At the outset,you have too many variables to deal with and the network-architecture is the least important of all. . Note: Best architecture is not only function of the domain but also the scale of data. The inductive bias of CNN&#8217;s and RNN&#8217;s might not be necessary or even optimal under the limit of infinite data. This also explains the recent surge in Multi-head Attention based architectures, which are closer to Multi-Layer Perceptrons than CNN&#8217;s. These architectures work better with lots of data. Otherwise, you better stick with your CNN&#8217;s. . Don&#39;t do AutoML. . We have never done blind hyper-parameter search. Imagine working with a peculiar dataset where almost there is no useful published work to start with. And you just managed to train the model. How would you go from here to betting your capital on its outputs ? . | Ablation studies and Hypothesis-testing is the way to go. . Based on our understanding of the algorithm and the domain we made hypotheses and tested them on at a time. Note that at this point we only managed to fit on training data. Performance on out of sample is out of question. To give an example, We have spent inordinate amounts of time thinking and empirically testing out the effects of say changing any particular hyperparameter like gamma or entropy factor would effect some aspect of the resulting strategies like Average holding time. This would not only develop your understanding of the problem but gives you levers to control the specific atttributes of your strategies at the later stage. . Many useful things are not useful in practice. . In theory any marginal improvement is useful. In practice, marginal improvement cannot be justified at the cost of adding algorithmic/implementation complexity to the system. This throws majority of reasearch papers out-of-the window. . | Don&#39;t Learn from pixels. Please consider adding any auxillary information about the dynamics.State Representation is really important. . | Reduce the dimensionality of Action and State space. . Focus on explicitly reducing the noise from the state. If you don&#39;t have a good set of minimal features required for the task, which is often the case, focus on building a pipeline to choose relevant features from you global set. Classical techniques may be of little use here, It is worthwhile to see if we can use the orginal task and the model to glean feature importance. . Here state simply refers to the features fed as input to the network. Theoritically,it seems network should be able to do feature selection and engineering implicitly. But, We have found drastic improvements by reducing noise from the input features before feeding to the network. In stock markets, many patterns found through training data don&#39;t generalize. It is intuitive to rely on Occam&#39;s Razor- Among two similarly performing strategies, prefer the one that used less features. Many RL algorithms relies on having access to a hashtable storing visitation frequencies and other metrics of the state-space. A neural network acts as a drop-in replacement in modern DeepRL methods with potentially infinite state-spaces. This might also contribute to why the performance is improved by drastically reducing the dimensionality of the state. . And reducing Action space has more to do with decreasing the complexity of the modeling problem. . | Engineering Environment | Reward Engineering | Develop checks for checking robustness. . For other domains it translates to developing techniques of accessing overfitting apart from checking run-on-test. . | You can use machine learning to understand ML models. . | No single metric to optimize . | Carefully Engineer the environment. Reward engineering and desigining environment are intimately tied. Eg: Agent got stuck in high portfolio states. For instance if the global trend is up..it learned to get some high portfolio and stay there. Correlation vs causation. . | Technical Notes . | Focus on gathering Large amounts of Clean Data. This should be the first priority. . | Focus on solving the most general instantiation of the problem. This system can later be used as a backbone to specialize. | # Some Philosophical musings . Why not to join in a startup ? . Here I&#39;m assuming you are an employee, not the founder. . Lack of proper mentorship. Startups hire just enough people to get things done. And often times you are the only one working on an area. During my time at Niveshi, I was the only machine learning engineer at the company. To grow, you need fast feedback loops. Nothing replaces learning from an experienced mentor. . | You won&#39;t get rich by working at a startup.You will get rich by deploying large sums of capital over multiple startups. Capital is cheaper than time. You can only work in so many startups over your career.Given abysmal success rate, it is stupid to diversify your time this way. Moreover,as an employee you will own 10 to 30 times lower equity than the founders. This is true even if you are the founding engineer of an early stage startup. Considering that you are just starting out your career,your market value is low. You will be paid minimal equity and compensation possible, even if they are generous. A better alternative is to work towards increasing your reputation early in your career. Earn a safety net of Career capital and money by working in big companies. Then,later start your own company or even work at a startup when you are potentially overvalued. Diversifying your bets among several low-risk and high-risk has better return profile than medium-risk ones.[https://en.wikipedia.org/wiki/Barbell_strategy] . | You will likely learn some bad engineering practices. Believe it or not we managed to get away without having any kind of unittesting. Our code get&#39;s pushed only by means of peer review. . | There are no established practices to deal with common painpoints. Resolving conflict. . | .",
            "url": "https://vinayvarma.work/research/2021/02/21/What-I-Learned-at-Niveshi.html",
            "relUrl": "/research/2021/02/21/What-I-Learned-at-Niveshi.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Deep Reinforcement Learning - Theory (Chapter 1)",
            "content": "I plan on to write series of posts explaining key ideas in Modern Reinforcement Learning(RL). Current post covers basics of deep learning,followed by an introduction to RL Theory.(Thourougly proves two key algorithms for learning in RL setting - Value Iteration and Policy Iteration.). If you prefer a more practical introduction refer this. . Note : This is not intended to be the first introduction to deep learning. Here I just provide a self-contained summary. Only hard prerequisite is to have good intuitions for matrix multiplication and the notion of taking a derivative. Otherwise refer to Essence of linear algebra and Calculus . Basics . Many deep learning models follow a simple recipe: . 1. Gather the data. 2. Define learnable parameters. And specify how they will interact with the data.(architecture) 3. Define a loss function to minimize. 4. Adjust the parameters until satisfied. . Train a linear regression model using gradient-descent. . Here we will see how we can perform all the above steps starting with the most barebones implementation. Note that the procedure outlined here is general purpose - meaning the way we adjust parameters is going to remain same irrecpective of the modality of the data, details of the loss function or the architecture. . Step 1. Gather the data . Let&#39;s generate some fake data.Let&#39;s assume that the data is coming from $y = 2*x1 - 4.2*x2 + 1 + noise(measurement error)$. This can be more succintly represented in vector notation : $$y = begin{bmatrix} x1 x2 end{bmatrix} . begin{bmatrix} 2 -4.2 end{bmatrix} + 1$$ . import numpy as np def get_data(*params,const=None,rows=1000): #number of features in the input dim = len(params) x = np.random.normal(0,0.3,(rows,dim)) y = x@np.array([params]).T if const: y += np.array([const]) return x,y x,y = get_data(2,-4.2,const=1) x.shape,y.shape . ((1000, 2), (1000, 1)) . Step 2. Define learnable parameters. And specify how they will interact with the data.(architecture) . Now we aim to learn the right coefficients to approximate the data generation process. First let&#39;s look at some code. . # Start with a random guess that respects the sanctity of the data.i.e our inputs are of dimension 1000*2 # outputs are 1000*1. Multiplying inputs by a 2*1 matrix(weights) and adding a constant(bias) is the simplest way # to ensure an output of 1000*1. # initial guess init_weights = np.array([[0.,-1.]]) #shape -&gt; 1*2 init_bias = np.array([0.]) #expected output def give_expected_output(inpt,weights,bias): return ((inpt@weights.T) + bias) out = give_expected_output(x,init_weights,init_bias)#shape -&gt; 1000*1 def get_error(out,expected_out): return np.mean((expected_out - out)**2) get_error(out,y) # IF WE CAN DRIVE THIS NUMBER DOWN TO ZERO VIA A GENERAL PURPOSE PROCESS,WE ARE GOOD TO GO . 2.121630476687219 . def get_grads(weights,bias,x,y,loss_func=&#39;squared_loss&#39;): # Just taking the mathematical gradient as defined by the model. if loss_func == &#39;squared_loss&#39;: weights_grad = 2*np.mean((x@weights.T + bias - y)*weights) bias_grad = 2*np.mean((x@weights.T + bias - y)) else: print(&quot;Sorry I&#39;m not yet scalable enough for arbitrary loss functions&quot;) return weights_grad,bias_grad grad_init_weights,grad_bias = get_grads(init_weights,init_bias,x,y,loss_func = &#39;squared_loss&#39;) def learn(x,y,init_weights,init_bias,loss_func,lr=0.001,epochs=5000): out = give_expected_output(x,init_weights,init_bias) error = loss_func(out,y) #print(f&#39;initial error, epoch 0: {error}&#39;) errors = [error] pres_lr = lr for i in range(epochs): weight_grad,bias_grad = get_grads(init_weights,init_bias,x,y) init_weights -= weight_grad*pres_lr init_bias -= bias_grad*pres_lr out = give_expected_output(x,init_weights,init_bias) error = loss_func(out,y) if np.mean(weight_grad)&lt;0.0001: pres_lr = pres_lr*2 errors.append(error) return errors,init_weights,init_bias . errors,final_weights,final_bias = learn(x,y,init_weights,init_bias,get_error) %matplotlib inline import matplotlib.pyplot as plt plt.plot(errors) . [&lt;matplotlib.lines.Line2D at 0x7f2be3ea9df0&gt;] . final_weights,final_bias . (array([[-0.82604061, -1.82604061]]), array([0.97582166])) . In the above code what happens if &#39;lr&#39; is not dynamically adjusted ? Now, imagine an arbitrary architecture (interaction between parameters and data). Can we have a general purpose get_grads function that efficiently evaluates the gradient for this network ?. For many such practical concerns, I suggest going through this free course from fast.ai. Modern libraries like Pytorch provide simple abstractions to ignore all such details. Now some general intuitions towards coming up with architectures for learning more complex functions. . Adding Inductive biases - convolutions and recurrent networks. . In the below feedforward network each neuron is connected to all the neurons in the previous layer. No inductive biases in the connectivity. . . Locality : For almost all natural signals it is easier to predict the future using recent past compared to any earlier versions. Locality allows for the sparsity of weights. We can put faraway weights to zero. In the below figure the 15 weights of the first layer is reduced to 9. It&#39;s also important to be aware of the concept of receptive field(RF). RF of layer a w.r.t b is simply the number of neurons in a that influence the outputs of layer b. . . Stationarity : Same patterns are repeated again and again. We don&#39;t need connections from the inputs far down. Stationarity implies weight sharing. . . Compositionality: There are hierarchies. Letters make up words,words make up sentences and so on. Compisitionality implies deeper networks. . Now We will see how popular building blocks like CNN&#39;S and RNN&#39;S leverage these properties. CNN&#39;s are a linear layer with lots of weight sharing and sparsity. RNN&#39;s just use weight sharing but BPTT takes the locality into account. . CNN = linear layer + weight sharing + sparsity. . Consider convolving over a 4 4 inputs with a 3 3 kernel with a unit stride as presented below. . If I stack the 2-d input into a 1-d vector by unrolling left to right and top to bottom, the convolution can be represented as a matrix multiplication. . The zeros along the columns encode locality while the replication of same weights along the rows account for stationarity. If the above properties doesn&#39;t make sense for your input , then CNN&#39;s aren&#39;t the right choice. . RNN = linear layer + weight sharing + BPTT(Back-prop through time) . . RNN&#39;s are used for sequence data. In the above figure the arrow indicates matrix multiplication. The hidden state h(t) at time t is equal to Affine_transform(x(t)) + Affine_transform(h(t-1)).(Note: Affine_transform refers to matrix multiplication). The following code will makes it all clear. . Consider the following sequence : . &#39;Hey Jude, don&#39;t make it bad. . Take a sad song and make it better. . Remember to let her into your heart, . Then you can start to make it better. &#39; . Now we would want to classify it into either positive or negative sentiment. Ideally we would have a collection of such sequences with their corresponding labels. Here note that the number of words in each sequence need not be same. A naive approach would be to string all the words in a sequence to a single column,and run it through a fullyconnected network.(variable sequence length still poses a problem.) Let&#39;s see how an RNN can accomplish this with much less parameters. . import torch import torch.nn as nn import torch.nn.functional as F class RNN(nn.Module): def __init__(self,i_sz,h_sz,out_sz): super().__init__() self.i_sz = i_sz self.h_sz = h_sz self.out_sz = out_sz self.input_hidden = nn.Linear(self.i_sz,self.h_sz) self.hidden_hidden = nn.Linear(self.h_sz,self.h_sz) self.hidden_output = nn.Linear(self.h_sz,self.out_sz) def forward(self,x): h = 0 bs,seq_len,emb_sz = x.shape for i in range(seq_len): # This just adds the hidden representation which is a function of all the words fed until t-1 to the #word at t h = h + self.input_hidden(x[:,i,:]) # Stores the hidden representation for next word in seq. h = F.relu(self.hidden_hidden(h)) return self.hidden_output(h) . bs = 101 seq_len = 10 vector_len = 100 seq_1 = torch.rand(bs,seq_len,vector_len) rnn = RNN(i_sz = vector_len,h_sz=20,out_sz=1) out = rnn(seq_1) out.shape . torch.Size([101, 1]) . This can be seen as a two layer network.The last layer converts h_sz to the output dimension, while the first layer maps i_sz to h_sz. But, the first layer only uses weight matrices of size 100 20,20 20. Totalling of around 2400 parameters. Our naive version would have seq_len i_sz h parameters.(around 20000). Moreover, In RNN number of paramenters is independent of sequence length. . The above model is just an instantiation of weight sharing for sequential data. We haven&#39;t still leveraged the locality aspect (sparsity). . Moreover,eventhough we only have 3 different weight matrices,the actual number of layers is proportional to the size of the for loop. Aside from being very slow and memory intensive, gradients of loss w.r.t initial operations( i = 0) very unlikely to be stable.(According to the chain rule of derivatives the gradient of loss w.r.t first matrix multiplication would involve multiplying atleast seq_len of partial derivatives. The resultant can easily explode or vanish.) What if we only take gradients for the last n operations. This is also called Truncated BPTT. Here&#39;s the modified forward function. . def forward(self,x): h = 0 bs,seq_len,emb_sz = x.shape for i in range(seq_len): # This just adds the hidden representation which is a function of all the words fed until t-1 to the #word at t h = h + self.input_hidden(x[:,i,:]) # Stores the hidden representation for next word in seq. h = F.relu(self.hidden_hidden(h)) if i%3 == 0: h = h.detach() return self.hidden_output(h) . This just flushes the memory for the backward pass after every 3 steps. Aside from solving obvious practical problems,this also has regularizing effects. We are implicitly encoding our bias - you need not look past the last 3 points in the sequence - a.k.a locality. . Self-Attention . Consider a sequence of vectors ($x_1$,$x_2$..$x_n$). It helps to imagine them as vectors corresponding to sequence of words. If you can bear with me,the following operation converts them into another sequence of vectors ($y_1$,$y_2$..$y_n$) of same dimension. . # Two column vectors(each with dimension of 3*1) import torch import torch.tensor as tensor X = torch.cat([tensor([[1.],[2.],[3.]]),tensor([[4.],[5.],[6.]]),tensor([[7.],[8.],[9.]]),tensor([[10.],[11.],[12.]])],dim=-1) matrix = torch.rand(4,4) # a random matrix Y = X@matrix X.shape,matrix.shape,Y.shape . (torch.Size([3, 4]), torch.Size([4, 4]), torch.Size([3, 4])) . Now,let&#39;s generate the same V with a fancier set of operations. . temp = X.T@X Y_new = X@temp.T Y_new.shape . torch.Size([3, 4]) . Here temp.T is acting as matrix. This also removes the need for additional initialization. This operation also lends to the following intution: . temp is the dot product of each column vector in U with all the vector within it. The captures the measure of similarity between the vectors. | Y_new is just a linear combination of U with the corresponding weights from the temp. . | In other words, $y_i = sum_{j} w_{ij}.x_{j}$ where w&#39;s are taken frow the rows of temp. . | . In the above figure each vector $x_{i}$ is used three times. Let&#39;s take $x_{2}$ for illustration: . To get $w_{22}$ | Similarly to get weight&#39;s required for all the other outputs $y_1$,$y_3$ and $y_4$ | $x_2$ is also used in linear weighting with w&#39;s to get $y_2$ | . But, In the whole compution we are not learning any weights. Everything is being generated from the input. We can introduce three different set of x&#39;s for each of the above operations. Let&#39;s initialize three square matrices $W_k$,$W_q$,$W_v$, each of size (4,4). . w_k,w_q,w_v = torch.rand(4,4),torch.rand(4,4),torch.rand(4,4) # learnable parameters keys,queries,values = X@w_k,X@w_q,X@w_v # This is the naming convention used in the literature. temp = queries.T@keys Y_new =values@temp.T Y_new.shape . torch.Size([3, 4]) . That&#39;s it. Self-attention refers to performing above operations. We additionally normalize the weights in the temp with softmax. Further, We can also use sets of matrices ($W_k$,$W_q$,$W_v$),essentially replicationg self-attention with different weight matrices. The resulting outputs can be concatenated and be passed through a linear layer to get back the orginal dimension.(This is called multi-head attention) . import torch.nn as nn import math import torch.nn.functional as F class SelfAttention(nn.Module): def __init__(self,feat_sz,n_heads=1): super().__init__() # for n_heads we need the corresponding number of weight matrices of size feat_sz*feat_sz to get new #set of (keys,queries,values),Computationally this can be fused inside a single linear operation. self.heads = n_heads self.get_keys = nn.Linear(feat_sz,feat_sz*n_heads,bias=False) self.get_queries = nn.Linear(feat_sz,feat_sz*n_heads,bias=False) self.get_values = nn.Linear(feat_sz,feat_sz*n_heads,bias=False) self.comb_heads = nn.Linear(n_heads*feat_sz,feat_sz) def forward(self,x): # typically data is fed with features along the `columns`. bs,n_seq,feat_sz = x.size() keys = self.get_keys(x).view(bs,n_seq,self.heads,feat_sz) queries = self.get_queries(x).view(bs,n_seq,self.heads,feat_sz) values = self.get_values(x).view(bs,n_seq,self.heads,feat_sz) # `torch.bmm` performs matrix multiplication for a given batch.It is efficient to squeeze n_heads along # with batches and perform the calculation at once. keys = keys.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz) queries = queries.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz) values = values.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz) dot = torch.bmm(queries,keys.transpose(1,2)) #Rescaling the elements to control the scale dot = dot/math.sqrt(feat_sz) dot = F.softmax(dot,dim=2) out = torch.bmm(dot,values).view(bs,self.heads,n_seq,feat_sz) #reshaping out = out.transpose(1,2).contiguous().view(bs,n_seq,self.heads*feat_sz) # passing through a linear layer to combine all the heads out = self.comb_heads(out)# gives bs,n_seq,n_heads return out . input_ = X.unsqueeze(0).transpose(1,2) # input with features arranged in columns(shape = [1, 4, 3]) sa = SelfAttention(feat_sz=3) Y = sa(input_)# simple attention mha = SelfAttention(feat_sz=3,n_heads=6) Y_6 = sa(input_) Y.shape,Y_6.shape . (torch.Size([1, 4, 3]), torch.Size([1, 4, 3])) . . Note: Attention is first introduced to deal with sequences in the context of natural language. But, nothing in the above implementation handles order. It just maps a set of vectors to another. To enforce order, we can simply add a position vector to our inputs. . Above ideas mark the end of the theoretical minimum. These will be needed only when you are trying to solve any of the interesting applications discussed below with RL. . Deep RL - Theory . Motivation . Deep Learning allows for learning generalizable mappings between input and output. In supervised setting we are given a fixed dataset D = {$(x_{i},y_{i})$},and are tasked to predict $y_{i}$ using $x_{i}$. Thus, we know the groud truth for all input data. This let&#39;s us formulate a loss that reflects our dissatisfaction with the predicted outputs and optimize over it. But, consider how we humans learn to perform any new task ? The dataset and the learning signal comes sequentially and is also dependent on our actions. We don&#39;t have prepared datasets in real life. We learn from experience. RL deals with learning under this natural setting. The key difference here is that the dataset is not constant, It changes everytime, contingent on your actions and the stochasticity in the environment. To make this clear, consider learning to play a video game. The pixels (call it state) and the score you receive ( call it reward) cannot be predetermined until you actually go through the experience. Here the dataset D = {$(state_{i},reward_{i})$} is not same everytime you play the game. RL provides a formalism for learning optimal decision making. This will become clear when we see some concrete examples and code. But combing these techniques with Neural networks has given us general algorithms that learn to play atari games, beat world champions at Go and train robots to learn simple tasks. . Problem . Consider the Following optimization problem. Find the shortest path from state $s_0$ to goal $g$,where the edges indicate the cost/distance ? . . Here taking a greedy approach will fail as actions will have long-term consequences. Solving the above problem efficiently requires realizing that the distance to any node along the shortest path from source to destination is also shortest path. In the above example the shortest path to g should either be through d or f.(one of the incoming edges). Let the shortest distance from a node s to g be given by $v^{*}(s)$. Then the last edge of the shortest path should come from evaluating $v^{*}(f))$ and $v^{*}(d)$, where $v^{*}(f) = 1 + v^{*}(g)$ and $v^{*}(d) = min(3+v^{*}(g),1+v^{*}(f))$. This backtracking or dynamic programming approach of finding one edge at a time is illustrated below. . . Now, for each action we take, let us also add a transition probability that defines the likelihood of ending up in any of the available states given the action. Here, for states c and e we added some randomness. This will let us model more realistic scenarios. Consider an RL agent driving your car. The consequences of any action (eg: Turning the steering to the right given the visual view of the road.) can only be modeled probabilistically. . . Here by weighting w.r.t the transition probabilities we can recover $v^{*}()$ for all states. Optimal policy is again achieved by acting greedily w.r.t $v^{*}()$. For example , $v^{*}(c) = min(4+0.3*v^{*}(e)+0.7*v^{*}(d),2+v^{*}(e))$. In RL, we call this Bellmann Equation. . . . Modern Deep RL deals with solving this stochastic version of the problem when the transition probabilities are not available and the number of possible states is very large. Consider the following video game playing scenario. The pixels on the screen at any timestamp can be taken as state. The game rules provides list of possible actions and the corresponding reward(increase in score). . . By the end of this module we will develop all the machinery to understand the algorithms that learn optimal(or good) policies for this general case. . Bellmann Equation and MDP&#39;s . In this section we will devolop some theory. First let&#39;s define an object called Markov Decision Process (MDP),given by the tuple of $(S,A,P,R, gamma)$. Here discounting factor $ gamma$ is largely a mathematical convenience as it helps to bound the total reward. Any reward $r_t$ received at timestamp t is multiplied by $ gamma^{t-1}$, that is we prefer immediate rewards over faraway ones. The term Markov refers to the fact that given present state s and action taken from there a, next state is independent of the past trajectory. Consider the example of stochastic shortest path but with the goal state g infinitely far away. In that setting we want to maximize the average reward we will get starting from any state. . The only knob agent has is policy $ pi : S rightarrow A$.(which of the possible actions to take). The environment dynamics (P and R) are not under agent&#39;s control. For simplicity,let&#39;s assume that rewards are bounded and positive. Let&#39;s say that the agent has a policy $ pi$ and starts to intreract with the environment. The value function v(s) refers to the average reward starting from state s and following policy $ pi$. Then v(s) for all the states satisfies a recursive definition as shown below. . Thus finding $v^{ pi}(s)$ amounts to solving system of linear equation or in matrix notation finding the inverse of a matrix. But, we still need the proof for the existence of the inverse for $I - gamma.P^{ pi}$. Before going further, it&#39;s important to thoroughly understand the bellmann equation : $V^{ pi} = R^{ pi} + gamma.P^{ pi}.V^{ pi}$. The deriviation involves observing the fact that once the agent takes initial action from state s, the transition probability will dictate it&#39;s next state s&#39;. From there, the average reward by definition is given by v(s&#39;), leaving a recursive definition. . . Below i have give the proof for the existence of inverse. If we recall, a matrix A($n * n$) multiplied by a column vector X ($n * 1$) merely takes a linear combination of all the column vectors in A. Existence of inverse to A means that there doesnot exist a non-zero vector X, that can collapse A to a null vector. Using this fact and the traingular inequality on $I - gamma.P^{ pi}$ completes the proof. . . Search for optimal State Values. . Now let&#39;s define $v^{*}(s)$ as the maximum expected reward that we can get from state s under any policy. Note that the above equation is defined on a particular policy or when the action from each state is fixed or if state transition probabilities are independent of action taken. Once we are able to extract these values, optimal policy becomes obvious. Value Iteration methods try to apprimate this $V^{*}(s)$. They start with some arbitrary function like $f(s) = 0 forall s$ and iteratively bring $f$ closer to $V^{*}$. Let&#39;s also define $Q^{ pi}(s,a)$ as the expected reward under the policy $ pi$,when we take action a at state s and subsequently sample actions according to $ pi$. Here $ pi$ is just a function that takes state s and action a and returns the corresponding probability a for taking that action. Similarly $Q^{*}(s,a)$ is also defined as the maximum Q that can be achieved under any policy. We often want $Q^{*}$ values over $V^{*}$ as simply choosing greedily w.r.t $Q^{*}$ gives the optimal policy. To see the advantage more clearly, Imagine an oracle that can give you $V^{*}$ for any s. Now, the agent starts at state s and is looking to take optimal action. It first has to take all possible actions from that state to see where it would end up. And for each subsequent state s&#39; , it has to query the oracle to get $V^{*}(s&#39;)$. Only after this we can determine the best action from initial state s as $max_{a in A}(r_1 + v^{*}(s&#39;))$. But having $Q^{*}$ values let&#39;s us choose this action directly by evaluating $Q_{a in A}^{*}(s,a)$. There exists a proof that for discounted infinite horizon MDP&#39;s there exists a stationary and deterministic optimal policy for all states simultaneously. Let&#39;s call this $ pi^{*}$. It is easy to see that both $V^{*}$ and $Q^{*}$ will also satisfy a similar recursive relation called Bellmann Optimality equations. . . Value Iteration . Till now we have seen how we can solve for V for all states given a policy. But, Our objective is to find V* - the maximum return that can be achieved under any policy. The claim is that if we start with random values of Q and then do the following - . Act greedily with respect to these values. | Update these Q values using bellmann update rule. | Repeat the process for some large number of times - H | . Will give us a policy $Q^{*,H}$(greedy policy w.r.t Q values after H Steps) that is close to true Optimal Policy. In what follows is the proof of the claim. . . . . . Policy Iteration (PI) . We also have an alternative algorithm that also converges to optimal policy called Policy Iteration. . Note : PI strictly converges to optimal policy after some steps which is not true for VI as it only get&#39;s closer but never quite equals. . The following derivation requires some explanation. First Here&#39;s the claim of policy iteration algorithms. . Start out with random policy. | Evaluate $Q^{ pi_{0}}$ using bellmann update rule. (BOX 1). | Greedy policy w.r.t to new Q values becomes your new policy. | . Performing above steps for large number of iterations would give us Optimal Policy - Claim. (In fact the claim is more subtle as given by Policy Improvement Theorem. . Policy Improvement Theorem: As given in below slide suggests that after every round of PI, the new policy has higher(or equal) V compared to old for all states. . The proof involves manipulating $ tau^{ pi}$ (bellmann operator). We start out by consicely writing down PI algorithm - . $Q^{ pi_{k}} = tau^{ pi_{k}}.Q^{ pi_{k}}$ . Note that bellmann optimality operator $ tau$(involves taking max in next state) is defferent from bellmann operator $ tau^{ pi}$. And substituting the latter with former would always result in a value higher or equal by definition. Since according to PI algorithm the new policy is the Greedy one w.r.t updated Q values, we have - $ tau.Q^{ pi_{k}} = tau^{ pi_{k+1}}.Q^{ pi_{k}}$. Later by recursively expanding $Q^{ pi_{k}}$, we reach the fixed point of the bellmann operator. . . Alternative and more Intuitive proof: | . . . . .",
            "url": "https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html",
            "relUrl": "/research/2021/02/21/Deep-Learning-Useful-Ideas.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Estimating Distributions",
            "content": "Introduction . Here, I will cover methods that leverage deep learning to estimate distributions. Ideally, they should also allow us to sample new data-points aka generative models. Supervised learning has already shown great success. But, when labels are not available we still want to be able to use raw perceptual data (videos,images,raw text etc) to capture rich patterns in the data. Broadly, I will cover the basics of the following approaches : . Autoregressive Models | Flow Based Models | Latent Variable Models | Generative Adversarial Networks (GAN&#39;s) | . Consider a distribution X, of dog images (24*24 images). We want a model that can give . a. high probability values for any dog image in the training data | b. high probability for any other similar dog image (Generalization) | c. Can generate novel dogs (Generative model). | . The recipe for satisfying conditions a.,b. seems straight forward - train on the training data maximizing log-likelihood of each image. An architecture like CNN would encode good prior(translational invariance and parameter sharing) to enable generalization. To be clear, what i&#39;m suggesting is for each image, we will have a CNN processing followed by the output of size 256*24*24. ( For each pixel,we will have 256 possible values and the label comes from the actual value of that pixel in that image.). So,we are essentially maximizing the log probability of each pixel corresponding to all the images in the training data. . But, Once we have trained on our data what we end up is 24*24 histograms. What exactly will be P(x_i) ? Can we say that the sigma(p(xi)) = 1.(definition of probability distribution). Let&#39;s say we define P(x_i) as product of all the outputs. Here,all that we are ensuring is each histogram corresponding to each pixel sums to 1.(via softmax over outputs). So, each of the methods tries to approach this problem in different ways with some tradeoff in ease of sampling new values and training. .",
            "url": "https://vinayvarma.work/reinforcement%20learning/2020/11/10/Estimating-Distributions.html",
            "relUrl": "/reinforcement%20learning/2020/11/10/Estimating-Distributions.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Accept all your drives",
            "content": "Acknowledge and accept all human drives. The tension between ‘Culturally-accepted’ and ‘evolutionarily-primed’ behaviors create unnecessary fatigue and misery. Evolutionary forces create strong drives that are to be suppressed or accepted based on the imaginative reality of the culture. Breaching those norms leaves us feeling ashamed and guilty. We seek to adjust our thoughts and behaviors to fit into the culture.This often happens in very subtle ways. Mentally note or record in a journal the instances you felt the slightest tinge of guilt or indulged in behaviors that you are not happy. Now, accept that this is by definition is part of human nature. Sometimes mere social baggage creates unnecessary friction against evolutionary drives. Maybe the advantage of sticking to cultural imagination is not worth it. Even, noticing vast differences in the cultural norms across the globe can give a different perspective.Guilt is a bad feeling and it cannot be endured for long. So, our brains invent reasons to escape this feeling. . All behaviours that are against our wellbeing are sustained either by this escape-mechanism or through our complete unawareness of their effects. Only an inquisitive and open mind has the luxury of ever discovering the behaviours in the second category. The creation of the ego is an apt example. Raising our awareness is the only way to open the possibility of improving our well-being. .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/09/08/Accept-your-drives.html",
            "relUrl": "/think%20on%20these/2020/09/08/Accept-your-drives.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "A different dimension",
            "content": "Heightened experience of life doesn’t come from grabbing better beliefs and ideologies. Beliefs offer certainty. Our brains are ill-equipped to deal with probabilities. So, a well-formed belief or opinion might serve as a good heuristic. But, Beliefs concerning oneself create an identity.It only creates angst and unnecessary dissatisfaction. It also has the side-effect of pushing life in the background. It feeds on the stability of its beliefs. It works only in the realm of thoughts – It strives to engross and fit everything in its tiny bubble. Any amount of brooding or theorization over this only can burst one to creates newer and more subtle ones. Non-judgemental awareness of this process loosens the hold. At this heightened level of perception, life happens on a different dimension .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/09/05/different-dimension.html",
            "relUrl": "/think%20on%20these/2020/09/05/different-dimension.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Continual Learning",
            "content": "Introduction . The objective of this work is to provide a coherent picture of Continual Learning in the context of Neural Networks. I hope that this would serve as a launch pad for further investigations of the reader. . Continual Learning . Modern Deep Learning systems,in general, deal exclusively with i.i.d. data. If our grand objective is to build General Intelligence,then it is mandatory for a single system to learn multiple tasks. Here , We will start with the general setting of continual learning and then begin to investigate how various features of the problem are solved by current methods. . Note: None of the known methods satisfactorily solve Continual Learning problem in it&#8217;s General Setting. . Continual Learning : The General Setting Optimize a defined loss function under Infinite Stream of Data,without any explicit distinction between tasks. Note : Here &#39;task&#39; is artificially introduced to represent variation in data distribution and even loss function. . The following are some of the desirable features of the learning scheme 1: . Fixed Memory : Memory requirements cannot grow with each additional task. . | No Task Boundary : Learn from the input data without explicitly defining task boundary increases the flexibility of the method and also reflects real-world setting. . | Online Learning : No offline storing of data.(i.e training with large batches for multiple epochs) . | Forward Transfer : Previous tasks should ease the learning on new tasks. . | Backward Transfer : Upon Learning future tasks, performance on correspondingly related past tasks should improve. . | Problem Agnostic : Should be flexible enough to work with different loss functions. . | . Important: Humans do not exactly operate in the General setting of &#8217;Continual Learning&#8217;. We often have local control over the stream of tasks. We decide,apriori the sequence of tasks to perform in order to reach a desired goal. We also assign ourselves tasks(thus access to task labels) in order to optimize our goals. . Implicit in the above definition,we expect the learning procedure to learn flexible representations. Traditional machine learning paradigm aims to learn a specifit task - machine translation,Image recognition,Speech synthesis - end to end,often from scratch. This is facilitated by training on large amounts of data,specific to the task.Consider Classical MNIST and CIFAR-10 Datesets,that have propelled progress in Computer vision. Each of these have 60,000 Images split equally between ten classes. This is in sharp contrast to how a human experiences learning. Our learning involves learning large number of classes,with few examples from each class.(say 6000 classes with 10 examples for each). Thus,our learning procedure is optimized for this setting and we are forced to learn concepts and abstractions that let us learn quickly and efficiently with less data. Later in this section,we will see techniques that tries to alleviate this problem in modern neural networks. . Let&#39;s consider three scenarios,which will better help us to better parse the existing literature in the field. . $e^{i pi} + 1 = 0$ . $$ sum_{i=0}^n i^2 = frac{(n^2+n)(2n+1)}{6}$$ . Approaches . . The approaches taken can be broadly categorized into three groups 2: . Modify the Update Rule . | Replay methods . | Use Semi-distributed representations . | Meta-Approaches . | Here we will cover some representative works from each of the above categories. . 1. Modify the Update Rule . Instead of updating all the weights on a new task, the most important weights are retained. . 4. Meta-Approaches . First, We will try to understand the various incarnations of these approaches and finally conclude with some recent works that try to leverage Meta-Approaches for Continual Learning. . Meta-Learning is a general paradigm that aims to condition the learning rule itself based on pre-defined properties of the learned representations. This will become clear when we see concrete examples. But,for our task of continual learning, Meta-Approaches offer a way away from hand-engineered approaches to learning-based solutions. Let us explore few foundational works in this field,before we see a concrete example for Continual Learning. . . Note: I will not cover the complete spectrum of Meta-Learning approaches,just enough to appreciate how this paradigm allows for a more flexible approach towards solving Continual Learning. But,for a good strating point check Lil&#8217;Log&#8217;s Blog . MODEL-AGNOSTIC META- LEARNING (MAML) 3 . Notation : . T : The task, corresponds to the objective or loss function,domain,environment. . p(T): The distribution of tasks from which a new task is sampled. . f&theta; : A model(neural net) parametrized by $ theta$ . {Ti} ~ p(T) : Tasks used for meta training . {Tj} ~ p(T) : Tasks used for meta testing . {D$T_i$} : Meta-training set . {D$T_j$} : Meta-testing set . {DTtr} : Traning data for task T . {DTtest} : Test data for task T . LTi&lt;/sub&gt; : Loss or feedback from the sampled task Ti&lt;/p&gt; Given that we want our model to quickly adapt to a wide range of tasks, can we incorporate this inductive bias explicitly into our training ? . PROBLEM STATEMENT : . Learn the weights of a neural net that can quickly learn to perform well on a new task. . Method : . The intuition is to learn the right set of initial parameters that which upon few gradient steps on a new task,would result in good performance. . Consider a task Ti drawn out of task distribution p(T). Now, from this Ti we can sample training dataset Ditr,the loss/feedback on which is given by LTi&lt;/sub&gt;. This loss is parameterized by f&theta;. Suppose we take few gradient steps on this $ theta$ to give $ phi$. The error on test data set ,Ditest sampled from Ti,acts as training signal to update orginal f&theta;.&lt;/p&gt; What makes this process different from traditional training of neural networks is that the gradient is taken over meta-parameters $ phi$ through $ theta$. Consider the case of taking one gradient over LTi&lt;/sub&gt; to arrive at $ phi$.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; $$ phi_i = theta - alpha nabla_ theta mathscr{L}( theta,D_{T_i}^{tr})$$ . Now,the loss on $ phi$i (taken on $ {D_{T_i}^{test}} $ ) is backpropagated through $ theta$. . $$ min_{ theta} Sigma_{{T_i} thicksim p(T)} L( phi_i,D_{T_i}^{test}) = min_{ theta} Sigma_{{T_i} thicksim p(T)} mathscr{L}( theta - alpha nabla_ theta mathscr{L}( theta,D_{T_i}^{tr}),D_{T_i}^{test}) $$ . $ alpha : Learning rate(hyper-parameter)$ . Note that the meta-optimization is performed over the model parameters $ theta$, whereas the objective is computed using the updated model parameters $ phi$. In effect, the method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task. . . Now, We will see how this approach can be applied to Continual learning by just tweaking the problem statement. . $ mathscr{L}$ . For example, here is a footnote 1. And another 2 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ . h&theta;(x) = &theta;o x + &theta;1x . h&theta;(x) = &theta;o x + &theta;1x . 1. arXiv:1909.08383!↩ . 2. arXiv:1905.12588!↩ . 3. arXiv:1703.03400!↩ . 4. arXiv:1905.12588!↩ . &lt;/div&gt; .",
            "url": "https://vinayvarma.work/research/artificial%20general%20intelligence/2020/08/08/Continual-learning.html",
            "relUrl": "/research/artificial%20general%20intelligence/2020/08/08/Continual-learning.html",
            "date": " • Aug 8, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Basics of Deep Learning",
            "content": "About . This notebook is a walkthrough of the amazing book by Jeremy howard and Slyvian Gugger. To buy the orginal book : Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD. I am using their free git-hub version . Download files and unzip .",
            "url": "https://vinayvarma.work/jupyter/2020/07/02/fastai-book-walkthrough.html",
            "relUrl": "/jupyter/2020/07/02/fastai-book-walkthrough.html",
            "date": " • Jul 2, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Citations in Fastpages via BibTeX and jekyll-scholar",
            "content": "Link to newer version of this document. . (N.B.: The following was created as a Markdown file in _posts/. For Jupyter notebooks, the same things apply; one simply enters the Liquid codes into Markdown cells. Quick Jupyter example.) . How to Cite . For demonstration purposes, I’ll take the liberty of citing a couple of my recent papers, namely the first SignalTrain paper [1] . Instead of using the LaTeX code cite{ &lt;whatever&gt; }, I use the Liquid code {% cite &lt;whatever&gt; %}. For example, the first citation above is written as “{% cite signaltrain %}” in the Markdown file that generates this HTML page. . The two citation markings above point to the References section at the end of this post where the full references are printed out in the bibliography style of my choice. . Drawing from the Bibliography . In the main blog directory, create a new directory called _bibliography/, and place your BibTeX file there as references.bib. In the case of this demo, the references file looks like this: . @conference{signaltrain, title = {Profiling Audio Compressors with Deep Neural Networks}, author = {Hawley, Scott H. and Colburn, Benjamin and Mimilakis, Stylianos Ioannis}, booktitle = {Audio Engineering Society Convention 147}, month = {Oct}, year = {2019}, url = {http://www.aes.org/e-lib/browse.cfm?elib=20595} } @article{billy_signaltrain2, title={Exploring Quality and Generalizability in Parameterized Neural Audio Effects}, author={William Mitchell and Scott H. Hawley}, journal={ArXiv}, year={2020}, volume={abs/2006.05584} url = {https://arxiv.org/abs/2006.05584} } . Note that this (single) references file is for your entire blog. The great thing about this is that all your Jupyter notebooks and Markdown posts will draw from this same file, which could be hundreds of references long, and jekyll-scholar will only include the ones you need for each post. . Finally, at the end of your post, you signal the creation of the list of references by using the Liquid tag . {% bibliography --cited %} . …so I’ll put that at the very bottom of this file. (Currently that’ll generate an error, because we haven’t enabled jekyll-scholar yet, but we’ll do that next.) The optional argument --cited means it’ll only list the references cited in your post. . Enabling Jekyll-Scholar . To enable jekyll-scholar, all we need to do is make the following two changes, and perhaps a third. . In _config.yml, add “ - jekyll-scholar” to the list of plugins:. . | Edit the Gemfile to include gem &#39;jekyll-scholar&#39; where the other plugins are listed. . | Optional: The default citation format is “apa”. If you want to change that, you can add the following to your _config.yml file: . scholar: style: &lt;name&gt; . …naming one of the styles in the CSL style repository (but leaving off the .csl ending). Tip from the CSL maintainers: . To quickly search the styles in the GitHub CSL style repository by file name, press “t” to activate GitHub’s File Finder and start typing. . Note however that the csl-styles Gem package used by jekyll-scholar lags behind the official CSL style repository, so some names you choose may not work. In that case, you can supply a CSL file yourself. For this demo, I found the file physical-review-d.csl, added it to my main blog directory, and then specified the style name physical-review-d in _config.yml. This produced the bracketed-number citation markers above, and the reference format you see below in the References section. . | The convenience of this BibTeX/jekyll-scholar approach is that instead of having to manually edit references on each individual page – say, if you wanted to change citation formats (or alternatively, update information about a paper cited in multiple posts) – now you only change one line in _config.yml (or update one spot in references.bib) and the system “builds out” the change “everywhere.” . Happy blogging! . References . [1]S. H. Hawley, B. Colburn, and S. I. Mimilakis, Profiling Audio Compressors with Deep Neural Networks, in Audio Engineering Society Convention 147 (2019). |",
            "url": "https://vinayvarma.work/2020/07/01/Citations-Via-Bibtex.html",
            "relUrl": "/2020/07/01/Citations-Via-Bibtex.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Title",
            "content": "trying neil gaiman&#39;s technique of blocking out fixed periods of time daily where i&#39;m allowed to not write if i wish but not allowed to do anything else.(30 mins).For the time being the objective of this practice in improve my personal writing skills - i.e write a readable prose,extra points if also delightful.Hoping to eventually tease out more concrete practice session...aka with some measurable metrics.Some important areas to work on: | 1.Clarity in writing.(as it reflects,clarity of thought) | . Summary of the day: . I have tried out a more engaging approach to learning mathematics - solve problems.I have found a nice book on analysis and an accompanying problem book.I seem to have enjoyed teasing out some problems from the first chapter.Surely,this is more engaging. . Personal interview : How my philosophy of health has changed over the years? . Ok, let me reel back to as far into past I can. I think I don&#39;t have the slightest clue during my high school for sure. I used to eat a typical south Indian diet which involves feeding in copious amounts of refined flour(rice, wheat.).And the protein intake is laughable at best. As expected, I used to feel hungry right after eating. I felt good about it. I somehow felt that it translates as growth. During my high-school, especially in the later years, after class 10th, I hardly had time to even surf the internet casually. We were busy preparing for the entrance exam called IIT-JEE(consider them the ivy league of Indian universities just 10 times more competitive). The psychological turmoil during that period is for some other time. I have always had an addiction to reading. I read compulsively. Well, on anything with the slight influence of my mood to steer. That&#39;s when I first read Tim Ferriss and some folks on his podcast like peter Attia. I don&#39;t remember how I stumbled on the bulletproof guy, but I kind of fancied his books as they seem to have the right amount of &#39;look I&#39;m different and also right&#39; kind of air about them. So, I picked up on keto and read further. (David Perlmutter-Author of Grain Brain).Well, presently I take two meals a day, with no grains or flours. Unlimited amount of unadulterated foods. (meat, veggies, and fruit) conditioned on availability and taste. Some times I randomly skip meals all day. The next day, I ensure to start with an intense workout followed by a ravenous feeding session. One good rule of thumb is don&#39;t eat anything that came into existence in the last 100 years or so and hasn&#39;t been before - all packaged and processed foods, sugar. . &quot;weight&quot;+str(&quot;&quot;) target.model . &#39;weight&#39; . i=1 &quot;hi{str(i)}&quot;.format(i) . KeyError Traceback (most recent call last) &lt;ipython-input-8-8197ad1f9fec&gt; in &lt;module&gt; 1 i=1 -&gt; 2 &#34;hi{str(i)}&#34;.format(i) KeyError: &#39;str(i)&#39; . import pandas as pd . out = pd.DataFrame.from_dict({&#39;weig&#39;:[0,9]}) .",
            "url": "https://vinayvarma.work/2020/05/06/thoughts.html",
            "relUrl": "/2020/05/06/thoughts.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Deep Reinforcement Learning - Practice(Q-learning Algorithms)",
            "content": "Introduction . Let&#39;s start with humans learning from experience.Say we are trying to learn to bicycle. We are driven by a goal to stay balanced and pedal.Along the way we fall.Now we need to start again.Somewhere in the gap between each iteration,we are learning and improving.Let&#39;s hash out the features of this process,if we are to model this : . Past actions influence future output: There is no immediate feedback.Each micro-action(exerting more pressure on the pedal,..) along the way either leads to falling-off balance or keep moving. | Computational Problem : How to assign credit to actions when they are not temporally connected ? . Outcomes might not be deterministic: There are features of the environment(road,weather etc) that we do not fully understand that can effect outcome of the action. | Computational Problem: How to make inference about the properties of a system under uncertainity ? . Since out of the above two the latter seems to be simpler,let&#39;s start by building our intuition about Non-deterministic systems : Let&#39;s say we are given a Non-deterministic system whose properties we are unaware of but can only query it.How can we build our knowledge about the properties of this system ? Can we come up with a systematic way(algorithm) to estimate it&#39;s properties ? . query(),query(),query() #Fires different measurements each time. . (1.830642819404602, -18.865480422973633, 0.9646076709032059) . Now to understand about any of the properties of this &#39;query&#39;,we need to start with making some assumptions - say the numbers are coming from some unknown distribution with some stable(constant) mean. Let&#39;s start estimating this. . . Note: 1. How will your estimate of this mean change with each query ? . How does your confidence on this mean change with increasing number of queries $n$ ? | def estimate_mean(n): &quot;&quot;&quot;Returns list of estimated means after each query repeated for n times.&quot;&quot;&quot; list_n = [] # keep track of all the outputs from our slot machine list_mean = [] # collect the means after every sample. for i in range(n): out = query() list_n.append(out) list_mean.append(sum(list_n)/len(list_n)) return list_mean . Now let&#39;s say after each query we update our estimate of running mean. . estimate_mean(10) # The list of estimates for n = 1...10 . [-3.2570073008537292, 2.4307329952716827, -3.6120274662971497, -6.871547028422356, -6.159279823303223, -2.986470659573873, -2.4990574217268398, -1.9037501001730561, -2.7559810421533055, -1.0134802050888538] . #let&#39;s plot these means list_means = estimate_mean(100) plt.hist(list_means) . (array([ 1., 0., 1., 32., 59., 4., 2., 0., 0., 1.]), array([-4.82607206, -3.65700102, -2.48792998, -1.31885894, -0.1497879 , 1.01928314, 2.18835417, 3.35742521, 4.52649625, 5.69556729, 6.86463833]), &lt;a list of 10 Patch objects&gt;) . We can see that the calculated mean vary a lot but seem to be closer to $0$ most often..But how does our estimate itself depend on $n$(number of queries) ? . #let&#39;s measure the stability of our estimates by taking the difference of each successive estimates. diff_means = [list_means[i]-list_means[i-1] for i in range(1,len(list_means))] plt.plot(diff_means) . [&lt;matplotlib.lines.Line2D at 0x7f96ba5e1350&gt;] . Our estimate of the mean don&#39;t seem to change much after a while.This is interesting.This aligns with our intuition - with more samples, we can be more confident.We can even go about proclaiming that whatever the dynamics of the system it&#39;s mean might be constant ? . . Important: Remember this insight..for any system with stable mean we can be sure that after a while our estimate itself becomes stable. . Neural Nets with Q-LEARNING . Reference : Playing Atari with Deep Reinforcement Learning . Key Questions : . 1.Can we use neural nets in RL setting where the observations are correlated and Non-stationary(non-iid) ? . solution : Experiance Replay . Context . The key algorithm introduced in the paper is summarized below but we need necessary context before naively implementing it. Let us start with our knowledge of using Deep Nets in Supervised setting, and try to formulate the problem in similar context : . From each state the agent encounters we want the agent to output a probability distribution over all possible actions, which when sampled from results in optimal return. So, if we can have access to the labels(optimal actions) from each state we can easily create a loss function and proceed with SGD over all the states. . Here&#39;s a simple approach for bootstrapping the labels: . Play the episode multiple times. . | Record the returns . | Choose the episodes with highest return. These might be the ones that the agent chanced upon good actions,so train on them. . | Cross-Entropy method . Here&#39;s how the environment looks like: . import gym import torch import numpy as np env_name = &quot;CartPole-v0&quot; env = gym.make(env_name) def moving_average(x, w): return np.convolve(x, np.ones(w), &#39;valid&#39;) / w . Here we will use gym&#39;s cartpole environment. Let&#39;s quickly check the average return under random actions. . #collapse-show def play_episode(env,obs_acts,sampler): ob = env.reset() done = False rewards = 0 while not done: acts = obs_acts(ob) actn = sampler(acts) next_obs, reward,done, _ = env.step(actn) rewards += reward ob = next_obs return rewards . . Average return for 1000 episodes . np.mean([play_episode(env,lambda x : env.action_space.n,np.random.choice) for i in range(1000)]) . 22.295 . Now that&#39;s the average return of a policy taking random actions at each state. In previous section, we have seen how properties of any distributin can be estimated in a step-wise manner,with each interaction resulting in more accurate estimate. But, here the reward from the environment depends on our actions(In this case moving left and right) at each state,with each action resulting in a different mean reward. Our task is to learn this mean reward corresponding to each action at a given state. . #collapse-show import torch import torch.nn as nn obs_size = env.observation_space.shape[0] hidden_size = 128 n_actions = env.action_space.n net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) . . # Bootsrapping with the cross-entropy loss of best episodes. from collections import namedtuple import torch.optim as optim obs_size = env.observation_space.shape[0] hidden_size = 128 n_actions = env.action_space.n sft_max = nn.Softmax(dim=1) net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) def train(net,data,loss,optmizer): optimizer.zero_grad() keys,labels = data pred_vals = net(keys) loss_v = loss(pred_vals,labels) print(f&#39;loss before update {loss_v.item()}&#39;) loss_v.backward() optimizer.step() print(f&#39;loss after update {loss(net(keys),labels).item()}&#39;) def categorical_sampler(): def sampler(probs): return np.random.choice(len(probs),p=probs) return sampler def play_episode(env,net,sampler): episode_step = namedtuple(&#39;episode_step&#39;,field_names=[&#39;obs&#39;,&#39;actn&#39;]) lis_steps = [] episode = namedtuple(&#39;episode&#39;,field_names=[&#39;tot_return&#39;,&#39;lis_steps&#39;]) ob = env.reset() done = False rewards = 0 while not done: acts = sft_max(net(torch.FloatTensor([ob]))) #import pdb;pdb.set_trace() actn = sampler(acts.data.numpy()[0]) lis_steps.append(episode_step(ob,actn)) next_obs, reward,done, _ = env.step(actn) rewards += reward ob = next_obs return episode(rewards,lis_steps) . batch_size = 20 percentile = 75 sampler = categorical_sampler() def get_batch(env,net,batch_size,sampler): batch = [] while len(batch) &lt; batch_size: episode = play_episode(env,net,sampler) batch.append(episode) return batch . batch = get_batch(env,net,10,sampler) lis_returns = list(map(lambda x: x.tot_return,batch)) np.mean(lis_returns) . 27.0 . Now,let&#39;s train . #collapse-show mean_returns = [] loss = nn.CrossEntropyLoss() optimizer = optim.Adam(params=net.parameters(),lr=0.01) while True: batch = get_batch(env,net,10,sampler) lis_returns = list(map(lambda x: x.tot_return,batch)) mean_return = np.mean(lis_returns) mean_returns.append(mean_return) ret_threshold = np.percentile(lis_returns,percentile) obs = [] actns = [] filtered_episodes = filter(lambda x : x.tot_return &gt;= ret_threshold,batch) if mean_return &gt; 199: print(&#39;training complete&#39;) break else: for episode in filtered_episodes: obs.extend(list(map(lambda x: x.obs,episode.lis_steps))) actns.extend(list(map(lambda x: x.actn,episode.lis_steps))) print(f&#39;{len(mean_returns)} mean return = {mean_return},threshold = {ret_threshold}&#39;) train(net,(torch.FloatTensor(obs),torch.LongTensor(actns)),loss,optimizer) import matplotlib.pyplot as plt plt.plot(moving_average(mean_returns,10)) . . [&lt;matplotlib.lines.Line2D at 0x12107b950&gt;] . It&#39;s cool that a simple approach derived out of our intuition actually worked. Now let&#39;s check the performance of the same algorithm on FrozenLake environment. . env_fl = gym.make(&#39;FrozenLake-v0&#39;) . Home Work 1 : . Solve the above environment with the above method. For details of the environment refer FrozenLake . CrossEntropy Method : Why it Works ? . Due to the frequent use of CrossEntropy method in Supervised learning setting we have not spent time dealing with it&#39;s formulation. Here&#39;s the necessary deep dive : . Deep-Q Learning . Prerequisites . Psuedo-code . Code . #collapse-show from collections import deque batch_size = 100 batch = deque([],batch_size) min_batch_size = 30 env_name = &quot;CartPole-v0&quot; env = gym.make(env_name) obs_size = env.observation_space.shape[0] hidden_size = 128 n_actions = env.action_space.n epsilon = 0.2 net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) def e_greedy_sampler(epsilon): def sampler(qf): nped_qf = qf.detach().numpy().squeeze() if np.random.uniform(0,1) &lt;= epsilon: return np.random.choice(len(nped_qf)) else: return np.argmax(nped_qf) return sampler def get_pred_q(episode,gamma=0.99): rewards = [step.r for step in episode] pred_q = [step.qa for step in episode] #appending &#39;0&#39; for terminal state. pred_q.append(np.array(0)) labels = [] for i in range(len(rewards)): labels.append(rewards[i]+gamma*pred_q[i+1].max().item()) return labels class QAgent(): def __init__(self,net): self.net = net def act(self,ob,fn = lambda x: x): return fn(self.net(ob)) def train(self,data,optimizer,loss_type =&#39;squared&#39;): q_actions = [] actions = [] labels = [] for step,label in data: labels.append(label) q_actions.append(step.qa) actions.append([step.a]) q_tensor = torch.cat(q_actions,0) a_tensor = torch.tensor(actions) labels_tensor = torch.tensor(labels) q_actions_taken = torch.gather(q_tensor,1,a_tensor) if loss_type == &#39;squared&#39;: loss = nn.MSELoss() loss = loss(q_actions_taken.squeeze(-1),labels_tensor) #print(f&#39;loss:{loss}&#39;) loss.backward(retain_graph=True) optimizer.step() agent = QAgent(net) # The same network we used in cross-entropy method. #change our previous play_episode to store (ob,a,q,r,next_ob) def play_episode(agent,env,sampler,n_steps=1): epi_step = namedtuple(&#39;epi_step&#39;,field_names=[&#39;ob&#39;,&#39;a&#39;,&#39;qa&#39;,&#39;r&#39;]) episode = [] pred_q = [] tot_return = 0 ob = env.reset() done = False while not done: qf = agent.act(torch.FloatTensor([ob])) action = sampler(qf) #import pdb;pdb.set_trace() next_ob,r,done,_ = env.step(action) tot_return += r episode.append(epi_step(ob,action,qf,r)) ob = next_ob pred_q = get_pred_q(episode) return tot_return,[(episode_step,pred_q[i]) for i,episode_step in enumerate(episode)] sampler = e_greedy_sampler(epsilon) optimizer = optim.Adam(params=net.parameters(),lr=0.01) threshold = 199 def train(sampler,optimizer,threshold,agent,env): return_vals = [] counter = 0 while True: episodic_ret,lis_step_label = play_episode(agent,env,sampler,n_steps=1) counter += 1 return_vals.append(episodic_ret) #print(f&#39;{counter} return == {episodic_ret}&#39;) if episodic_ret &gt; threshold: #print(&#39;solved.&#39;) break batch.extend(lis_step_label) if len(batch) &gt; min_batch_size: idx = np.random.choice(len(batch),min_batch_size) optimizer.zero_grad() loss = agent.train([batch[id] for id in idx],optimizer,loss_type=&#39;squared&#39;) return return_vals return_vals = train(sampler,optimizer,threshold,agent,env) . . plt.plot(return_vals) . [&lt;matplotlib.lines.Line2D at 0x1a28cc8f50&gt;] . That took about 700 episodes and there&#39;s almost no general trend.Let&#39;s see if could replicate : . # Run this when you got lot of free time. lis_return_vals = [] for i in range(20): net = nn.Sequential(nn.Linear(obs_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, n_actions)) agent = QAgent(net) lis_return_vals.append(train(sampler,optimizer,threshold,agent,env)) . #but here&#39;s a sample plot : plt.plot(lis_return_vals[0]) . [&lt;matplotlib.lines.Line2D at 0x1a2b106910&gt;] . Home Work 2 : . Though we seem to have solved the environment,the training(episodic rewards) process seems too unstable. . Can you make it stable ? | List some of the possible reasons for instability ? | Bag of Tricks : . . Tip: SOME OBVIOUS TECHNIQUES . In our implementation of Deep-Q learning agent we have kept epsilon constant throughout the training. We can anneal it towards a lower value as the training proceeds. | Our update was Q(i) = r(i) + gammaQ(i+1) . So after every batch of episodes(after the update) we would expect our Q(i)&#39;s closer to the labels (r(i) + gammaQ(i+1)) but due to the proximity of Q(i) and Q(i+1) updating Q(i) also changes Q(i+1). Unlike supervised setting, our labels here are a not constant,thus introducing instability in training.We can keep our labels constant for a couple of batches ( by storing the weights and using them to produce labels). | Well, DeepMind pushed this a whole lot further by adding several other techniques in this paper : . Rainbow: Combining Improvements in Deep Reinforcement Learning .",
            "url": "https://vinayvarma.work/reinforcement%20learning/2020/05/06/Q-Learning.html",
            "relUrl": "/reinforcement%20learning/2020/05/06/Q-Learning.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Policy Gradient Algorithms",
            "content": "",
            "url": "https://vinayvarma.work/reinforcement%20learning/2020/05/06/Policy-Gradients.html",
            "relUrl": "/reinforcement%20learning/2020/05/06/Policy-Gradients.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Collected Thoughts",
            "content": "Intention vs Expectation . There is a subtle difference between intention and expectation. We can have intention without any expectation. Intention comes without any bondage. Expectation pulls us into unnecessary drama – our life will depend on the currents of situations, drawing out life energies. Through intent and purpose, we express ourselves. Life is nothing but an expression of energy. . Well-being . Seeking acknowledgment and validation from others creates psychological dependency. Longing to be around people arises out of this compulsion to validate. We seek to be cared, respected, included. These primordial drives have kept us alive. All compulsions seem to arise out of a center and work to keep it alive. This is a live process that is happening beneath our awareness. Raising awareness towards this process brings well-being. . Beyond chatter . We have incredibly complex minds. Capturing it in a Self-image or ego is futile. Ever refreshing attention to the present is all that’s needed to capture the enormity of life. Ego constricts.Beyond chatter, there is a profound experience. . Inner Exploration . We are our minds.All that we call life arises out of it. Most of the time, our conscious brain runs on auto-pilot – compulsively rolling from thought to thought. Paradoxically, without any force but by just observing these thoughts and sensations as they arise in consciousness seems to have an immediate calming effect. The experience of paying attention to the contents of consciousness brings about fundamental changes in the very perception of life. It would be a disaster to go through life without ever exploring the depths of it. . Experience . Seek experiences. Personal transformation arises under experiences bereft of any thought or rationalization. These leave a deep impression on us that makes change inevitable. If you fantasize a life of leisure and travel – take a small vacation, Curious about spirituality – take a day off and meditate. We are capable of following irrational group norms based on beliefs and ideals alone. I would rather explore life grounded in reality than led by the figments of cultural imagination. Poke around your boundaries, seek experiences. . On Impermanence . Thoughts, feelings, emotions, beliefs change continuously. An appreciation of this fact is needed for a balanced life. Almost nothing of me remained from a decade ago…the contents of my consciousness and body have changed completely. But, there is an element in me that stitches a sense of continuity. The grip of the ephemeral loosens when one becomes aware. Not believe or theorize. . On happiness . It all starts with doubting human condition. Why is it that happiness comes after fulfilling a desire and lasts only for a while? So we are bound to hop from a desire to desire, accumulating whatever angst along the way. Can there be joy untethered to desire ? If not, then a sensible solution would be to have easily achievable goals. This makes the whole hopping thing seem more continuous stream of happiness. But, do we have control over our desires ? . On Love . We all seem to share a longing to expand. To become more. This drive manifests as a desire for wealth, status, recognition or freedom from all these. Our minds are not satisfied with the boundaries of our physical bodies – there is an element in us that is seeking to be boundless. What is stopping us from loving unconditionally ? . On Education . Education can be better. A complete human being unsmitten by the relentless need to compare, capable of love is a rarity. When a child passes through a decade or so of education – his capacity towards life should be enhanced. Awareness brings about inner harmony that resolves the need for religion, government, authority, morality. Raising the awareness of our species is the best engineering solution against war, crime, pollution, and self-induced suffering. . Stretch your limits . Thought is limited to the past. It can’t operate without memory. And memory is limited and fallible. Belief is restricted thought. Collection of beliefs about oneself creates Ego. When a free thought disturbs the certainty of a belief – Ego shatters. Ego feeds on the stability of identity. Seeking to stretch your limits eventually renders Ego useless. . Joyful life . We are the creatures of limited time and energy. What we choose to make out of these is entirely our choice. Awareness of these basic constants leads to a responsible life. Complete responsibility equals absolute freedom. When we realize our ability to respond to life, events, thoughts will live a joyful life. . The Vicious cycle . We Sense the world. And then we associate. We then segregate them into good and bad. Any further perception is limited by these associations. . Discipline . It takes some effort to stick to a regimen. But it’s the only way to transform oneself. A sense of personal growth seems to be very fundamental for a good life. We all have this longing to expand and grow beyond ourselves. Whatever the means one may choose, discipline is the tool. . Keep it simple . Life is simple. Catch yourself, whenever you are trying to make it otherwise. Do not escape reality. Acknowledge your feelings and desires without judgment. Do not hurry. There is no compulsion to reach conclusions. Appreciate uncertainty. We can take better actions under this framework. Forcing reality is a loser’s game. Keep it simple. . Pay the dues . Pay attention to your desires and fears. We are complex creatures. Often we are unsure of what we want. It takes deliberate struggle to evade dissatisfaction and boredom. So let it be. Pay the dues. . Take Charge . Imagine. Consciously, not impulsively imagine what your ideal life should be like. Imagine freely, the kind of family, the nature of work and the world you would like to live in.Feel it to the bone. Now, do What you should be doing today to move in that direction. You can either live intentionally or succumb to the compulsive desires sold to you. There is always somebody dictating you the norms, selling you their fears and insecurities. Taking charge involves paying enough attention to these superficial desires, so they lose their grip. To connect with your deepest longings, you need to imagine freely and act. The feedback can only come through action. So, it’s better to imagine yourself, take charge and act. . Anger . It’s absorbing. But, it damages both the ends. The physical sensations are not pleasurable,often the actor feels more pain than the recipient. A moment’s awareness settles everything down. Somewhere along the line,control is being dropped. It is a fun exercise to actively take note of these situations , and seek to involve in them. To non-judgmentally observe your response. It becomes impossible to be both conscious and angry. .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/05/05/collection.html",
            "relUrl": "/think%20on%20these/2020/05/05/collection.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Thoughts on Building Things",
            "content": "Start early, start now . Start early. Wherever you are, now is the time. Those who made it might not tell you but luck is a big factor. The earlier you start, the more can you fail. It’s an odd’s game. Don’t be fooled by the myth of ‘Genius’ nor the ‘Hard work’. Cowards subscribe for ‘the genius’ while the winners and aspiring ones go for ‘the ‘Hard work’.Both are overly result-oriented. Planning for the worst and giving your all is the name of the game. And having time on your side lets you play it for a tad longer. Start now. . Intensity, at will . Spend some time with yourself. Remove all the distractions – thoughts, compulsions. Cultivating a craft where you can be completely absorbed, will open up a new dimension. When you remove distractions, life intensifies. This percolates into all areas of life. You will become capable of intensity, at will. . Nobody to please . Learn to disagree. Compulsively worrying to satisfy and please others at the stake of personal wellbeing is a pathological condition. Being spiteful and nasty is not an option. But, not negotiating for your well-being is pathetic. Over time, the effects percolate. Personal beliefs are altered to fit the reality. True capacities are shunned and never realized. Agreeable people are paid less. Their position in social hierarchy remains mediocre. There is no moral obligation to please anybody. . Create a routine . Create a routine. Else you will be put in one. Exercise your freedom to choose what you want to do. Repeating the same steps every day is the only way to gain mastery. It is by building this reservoir of work you can sustainably stick with it. This is your ticket to freedom. Uncommitted become the slaves of norms. They will always be struggling to escape, always at odds with reality. start with your body. Have an exercise routine that can be maintained. Next, Work. Choose what you want more of and create a routine around it. Do not be tempted to fit too much here. Slowly you will develop expertise and confidence that will let you do more of it. . The most important thing . Cultivate the ability to break-down complex problems into simpler ones. This is the meta-skill that translates into greatness in every field. Accomplishing ridiculously difficult tasks demands the ability to visualize the composing sub-tasks and formulate daily routines around them. On any day, there should always be one thing that takes your complete attention, irrespective of the breadth of the project. As an entrepreneur, your sole responsibility is to answer what is the most important thing, now ? . On work . Selling time for money is addictive. It nullifies the odds of accumulating wealth. It makes one blind to possibilities, incapable of imaging the power of leverage. Aspiring entrepreneurs should never work for money. They should actively avoid fixed pay contracts. Only under the circumstance of receiving rare skill, knowledge can they transact their time. Actively seek mild discomfort. Once succumbed to the safety of a job, anything creative sounds dangerous and implausible. The closer you can be to the creative process, the better. For employment, Startups are the best bet. Manipulating your environment is the most direct way of influencing outcomes. Under no circumstance, your time should be spent away from the creative process. There are always better options. . Luck . There is a controllable element to what people refer to as Luck – being in the right place at the right time. Depending upon what you want to do, there are great places(people) around the world. Anything that can structurally avoid complacency, is a great choice. We should look for as much discomfort as possible and the most sensible way is to place yourself in a hard place with hard people. . Consistency . Don’t directly rush into doing something. Take some time and put some labor to find out why you want to create something. Only when you are clear on Why proceed further. The next important question is what you can do right now to get a bit closer? Daily commitments should be so simple that they sound laughably easy.This ensures momentum, as we achieve them daily. We are not naturally good at forecasting the cumulative effects of simple habits. The key to accomplishing ridiculously difficult tasks is consistency, not the quantum of work done on a day. . Progress . Clarity of what you want in the long term is absolutely crucial. Define it. Take some time and write it down. Yes, it might evolve. But you should have a clear understanding of the essence. Ask specific questions like How do you want your day to be? How does your ideal work environment look like? How does your relationships, health, financials look like? Create a practice to develop deeper understanding of your longings? Only with clarity of vision can there be a possibility of progress. . Hard Work . Working more hours is not hard work. Putting your heart to create something is not hard work. Constantly stretching your capabilities is not hard work. Taking responsibility is not hard work. Working for the paycheck, bearing and accepting the inanity of your work, waiting for the weekend, passing through life without intensity is Hard Work. . Every Day . Focus on what you are doing every day. Start with the smallest change that can be incorporated into your routine. Now stick to it every day. This is the secret of accomplishing impossibly difficult tasks. Focus on the routines and habits. Nothing more is needed. . Fear is good . When you are starting something new, the fear of failure is often useful. This forces you to focus on risk. Being able to use this fear to ask the right questions, to strategies and come up with novel ideas is the defining skill of a successful entrepreneur. In the right proportions, it illuminates threats, directs the energies. Fear is good. . Copying Ideas . Don’t get lost in the “next big idea” fallacy. There are two practical ways to come up with successful business ideas. Look for the problems you are facing or Copy from the solutions other entrepreneurs successfully implemented. The first category involves listing down all the activities in your day while thinking how each one of them can be made better. Now, once you choose a few, write down how much you are willing to pay and the kind of service or the product you want. If an idea filters through this process successfully, it can be considered further. Don’t fool yourself here,if you aren’t willing to pay chances are nobody will. The second category is much simpler. Choose an industry. (better you already got some idea about it.).Now find out what’s happening within it in other countries. Read their magazines, news sites, etc. Now the question boils down to, What successful business products and models can you take inspiration from? Yes, that’s called copying ideas. Hopefully, you are doing all this to create some value, not justify your genius. . Little by little . We lack the ability to foresee the effects of small changes accumulated over time. Habits are remarkable ways to leverage the power of compounding. Introspect how your daily time is spent. Add or substitute a habit at a time. There is no hurry. This is a long term game. Keep the changes to absolute minimum.In the initial stages, momentum is the key. Whatever your ideal time or effort, reduce it by half – this should be your initial commitment. When creating habits, setting ridiculously easy goals so that it’s impossible to fail is the key. Consider any extra effort put as the bonus and feel great about it. Little by little your life will be transformed. . Attention . Attention is limited. Any practice or technique that allows you to direct your energies will have a compounding effect on your life. Measure your day by how much of it is intentional. There are always people and things constantly attempting to grab your attention – the News, Internet, fancy toys. It’s a skill that can be trained. .",
            "url": "https://vinayvarma.work/entrepreneurship/self-empowerment/2020/05/05/On-Buliding-things.html",
            "relUrl": "/entrepreneurship/self-empowerment/2020/05/05/On-Buliding-things.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Dissatisfaction",
            "content": "Dissatisfaction. This is the most subtle and malignant force within us. The majority of life’s efforts are to escape this feeling. Our desires come out of deep dissatisfaction with the present and hope to escape from it in the future. But this is a ceaseless process, always keeps us tethered to the future. It’s funny that life always happens now. .",
            "url": "https://vinayvarma.work/think%20on%20these/2020/05/05/Dissatisfaction.html",
            "relUrl": "/think%20on%20these/2020/05/05/Dissatisfaction.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Create a routine",
            "content": "Create a routine. Else you will be put in one. Exercise your freedom to choose what you want to do. Repeating the same steps every day is the only way to gain mastery. It is by building this reservoir of work you can sustainably stick with it. This is your ticket to freedom. Uncommitted become the slaves of norms. They will always be struggling to escape, always at odds with reality. start with your body. Have an exercise routine that can be maintained. Next, Work. Choose what you want more of and create a routine around it. Do not be tempted to fit too much here. Slowly you will develop expertise and confidence that will let you do more of it. .",
            "url": "https://vinayvarma.work/productivity/2020/05/05/Create-routine.html",
            "relUrl": "/productivity/2020/05/05/Create-routine.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://vinayvarma.work/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "On Belief",
            "content": "Belief is the easy thing. It demands no cognitive load. Decisions are made almost unconsciously. Doubt takes deliberation. Belief is common. It brings together people and offers safety. Groups welcome or ostracize individuals based on common beliefs.Evolution further selects these individuals. The desire to be identified with a group is an ever operating natural force. This raises the odds of our survival in the wild. While our well-being lurks on the fine balance of Doubt and Belief. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/25/On-Belief.html",
            "relUrl": "/think%20on%20these/2019/09/25/On-Belief.html",
            "date": " • Sep 25, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Love",
            "content": "A heart brimming with love and a radiant body are necessary to experience life at its fullest. Love doesn’t require anything from outside one’s agency. It doesn’t demand people to be less mean nor for us to constantly look for justification in their actions. It doesn’t arise out of any expectation from oneself or others. Whoever is around you, the habit of having good intentions for their life creates immediate wellbeing. Giving your best to the task, say in playing football for your team, might involve strategizing against your opponent. But, nowhere does it require you to change your intentions. It only becomes a zero-sum game when you rest your source of joy outside yourself – like in winning a game or getting a promotion. Then, inevitably subtle forms of hate arise helping us maneuver through dissatisfaction. But, this leaves us incapable of Joy. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/25/Love.html",
            "relUrl": "/think%20on%20these/2019/09/25/Love.html",
            "date": " • Sep 25, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "Inefficiency",
            "content": "Life is mostly inefficient. We end up doing things that in retrospect offer no value. We regret. Yet, this trial and error process is the only way to discover. All our expectations, thoughts, longings are based on our present level of awareness. We cannot act like our future selves.Realizing this is self-empowering. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/23/Inefficiency.html",
            "relUrl": "/think%20on%20these/2019/09/23/Inefficiency.html",
            "date": " • Sep 23, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Now",
            "content": "It’s a strange realization. There is only now. All our dreams, if realized will have to dawn on some now. There is no escape from now. All the sensations that we derive from indulging in work, thoughts will be experienced now. So the scope of our life depends on our capacity to experience now. It’s a strange realization . Life is happening now. Whether I plan for future, muse on my past or feel the burning pain of my wound , It all happens right now and that’s all I got. A moment arises within the cessation of the previous one. There is no way to latch. In thinking about the future – all my sensations whether the excitement or the dread happens in the present. So it follows whether can I keep myself joyful right now – without the necessity of a person, thing, belief or ideology. That would be cool. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/17/Now.html",
            "relUrl": "/think%20on%20these/2019/09/17/Now.html",
            "date": " • Sep 17, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "Move",
            "content": "Life is in motion. Stagnation is death. Take action. That’s the only way to interact and influence. Time is ticking away. There is nothing to be preserved. So move into every moment - without baggage and drama. .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/09/03/move.html",
            "relUrl": "/think%20on%20these/2019/09/03/move.html",
            "date": " • Sep 3, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Mental states",
            "content": "The joy of achievement, the pain of losing a loved one are fundamentally different mental states, invoked under certain conditions. The desired mental states we all seek are similar. In life, we try to line it up with certain mental states while avoiding others. This is the origin of all our desires. So it seems pertinent to find out the controller of these states? From where do they arise ? Does pre-conditions matter to get into a certain state ? .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/08/28/Mental-states.html",
            "relUrl": "/think%20on%20these/2019/08/28/Mental-states.html",
            "date": " • Aug 28, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "Action",
            "content": "Only tools we are in complete charge are our body, mind. All life happens through them. It pays to think in terms of this simple model. Much of the unnecessary suffering arises out of confusion with other minds. We have no access to the consciousness of other humans. So we cannot control their likes, dislikes, affection towards you. But, we seem to be in constant conflict with this reality. Our relation to the world occurs in pairs of action and expectation. But, can there be action without a desire or exception? .",
            "url": "https://vinayvarma.work/think%20on%20these/2019/08/19/Action.html",
            "relUrl": "/think%20on%20these/2019/08/19/Action.html",
            "date": " • Aug 19, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Bio . I want to build intelligent systems. In past, I’ve automated financial trading using Deep Reinforcement Learning at Niveshi. Currently on a break , studying mathematics and computerscience. Below are some sample posts: . Life : Collected Thoughts | Entreprenuership :Thoughts on Building Things | Intelligent Systems : Deep Reinforcement Learning - Theory | . Links . Twitter . | Github . | LinkedIn . | .",
          "url": "https://vinayvarma.work/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vinayvarma.work/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}