{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Distributions\n",
    "> A tutorial introduction to density estimation methods with deep learning.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Reinforcement learning]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here, I will cover methods that leverage deep learning to estimate distributions. Ideally, they should also allow us to sample new data-points aka generative models. Supervised learning has already shown great success. But, when labels are not available we still want to be able to use raw perceptual data (videos,images,raw text etc) to capture rich patterns in the data. Broadly, I will cover the basics of the following approaches :\n",
    "\n",
    "- Autoregressive Models\n",
    "- Flow Based Models\n",
    "- Latent Variable Models\n",
    "- Generative Adversarial Networks (GAN's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a distribution X, of dog images (24\\*24 images). We want a model that can give\n",
    "- a. high probability values for any dog image in the training data \n",
    "- b. high probability for any other similar dog image (Generalization) \n",
    "- c. Can generate novel dogs (Generative model). \n",
    "\n",
    "The recipe for satisfying conditions a.,b. seems straight forward - train on the training data maximizing log-likelihood of each image. An architecture like CNN would encode good prior(translational invariance and parameter sharing) to enable generalization. To be clear, what i'm suggesting is for each image, we will have a CNN processing followed by the output of size 256\\*24\\*24. ( For each pixel,we will have 256 possible values and the label comes from the actual value of that pixel in that image.). So,we are essentially maximizing the log probability of each pixel corresponding to all the images in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, Once we have trained on our data what we end up is 24\\*24 histograms. What exactly will be P(x_i) ? Can we say that the sigma(p(xi)) = 1.(definition of probability distribution). Let's say we define P(x_i) as product of all the outputs. Here,all that we are ensuring is each histogram corresponding to each pixel sums to 1.(via softmax over outputs). So, each of the methods tries to approach this problem in different ways with some tradeoff in ease of  sampling new values and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
