{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson learnt while building a Deep Reinforcement learning based automated stock trading service.\n",
    "> or How We made DeepRL Work in the real world\n",
    "- toc: true \n",
    "- comments: true\n",
    "- categories: [Research]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of today, I have been working at `Niveshi` for nearly two-and-half years. We Started out in late 2018, with nothing but a rough idea at best to automate `strategy generation` using DeepRL. I will spare other details but I believe we have succeeded in building an automated pipeline from getting raw data to ready-to-deploy startegies. Here I'm logging some lossons learned along the way. These will be useful for folks trying to make research ideas in Artificial Intelligence work in the real world, especially Deep RL. I will take `stock trading` as a running example,to illustrate some general principles. Some familiarity with RL is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Objective` : What are we trying to solve ?\n",
    "\n",
    "Find statistical regularities in the historical data and identify profitable trading rules. For simplicity, let's assume the data includes only historical price and volume. From RL perspective, given a set of time-series features learn to trade profitably. This amounts to outputting appropriate actions at every time-stamp, in order to maximize profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Usually techniques are sampled from traditional machine learning,signal processing and statistics. The recipe is as follows :\n",
    "\n",
    "- Hire a bunch of smart folks. \n",
    "- Give them the data and ask them to make money for the fund.\n",
    "- Incentivize them appropriately. \n",
    "\n",
    "Apart from the laundry list of problems that come with managing people, this approach is quite costly, and doesn't scale well. Markets quickly absorb any obvious edges. Consequently,any successful pattern,however sophisticated, will have a short shelf-life. This keeps the team busy continuosly searching for novel ways to leverage the data and extract profitble stratagies. DeepRL, atleast in theory, promises to automate all this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes to my-self** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to gaurentee success with Neural Networks ?\n",
    "\n",
    " - Collect Large , Clean and Varied ( interesting samples,edge cases) data.\n",
    " - Train a large enough network neural network on it.\n",
    " \n",
    " For whatever reason if your problem can't meet the above conditions, get ready for a painful ride.\n",
    " \n",
    "\n",
    "2. How to deal with less data ?\n",
    " \n",
    " signal = (amout of data) * (signal from each data point).\n",
    "\n",
    "Roughly,the amount of signal provided per data-point follow `Self-supervision > Supervised > Reinforcement Learning`.  \n",
    "\n",
    " - Consider learning the structure of the data before attempting to formulate as the RL problem.\n",
    " \n",
    " - Add auxillary losses. These might either fasten learning or can be used as standalone signals to improve the robustness of the base signal.\n",
    " \n",
    " - When you are limited by the scale of data, ensemble of simple architectures can beat the effort over novel architectures.\n",
    "\n",
    "\n",
    "3. Don't Fiddle with network-architectures yet.\n",
    "\n",
    "When You begin workin on a new problem, Start with the simplest yet bespoke architucture you can find. At the outset,you have too many variables to deal with and the `network-architecture` is the least important of all.\n",
    "\n",
    "> Note: Best architecture is not only function of the domain but also the scale of data. The inductive bias of CNN's and RNN's might not be necessary or even optimal under the limit of infinite data. This also explains the recent surge in Multi-head Attention based architectures, which are closer to Multi-Layer Perceptrons than CNN's. These architectures work better with lots of data. Otherwise, you better stick with your CNN's.\n",
    "\n",
    "4. Don't do AutoML.\n",
    "\n",
    "  We have never done blind hyper-parameter search. Imagine working with a peculiar dataset where almost there is no useful published work to start with. And you just managed to train the model. How would you go from here to betting your capital on its outputs ?\n",
    "  \n",
    "Ablation studies and Hypothesis-testing is the way to go.\n",
    "  \n",
    "Based on our understanding of the algorithm and the domain we made hypotheses and tested them on at a time. Note that at this point we only managed to fit on `training data`. Performance on out of sample is out of question. To give an example, We have spent inordinate amounts of time thinking and empirically testing out the effects of say changing any particular hyperparameter like `gamma` or `entropy factor` would effect some aspect of the resulting strategies like `Average holding time`. This would not only develop your understanding of the problem but gives you `levers` to control the specific atttributes of your `strategies` at the later stage.\n",
    "\n",
    "\n",
    "4. Many useful things are not useful in practice.\n",
    "\n",
    "  In theory any marginal improvement is useful. In practice, marginal improvement cannot be justified at the cost of adding algorithmic/implementation complexity to the system. This throws majority of reasearch papers out-of-the window.\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "1. Don't Learn from pixels. Please consider adding any auxillary information about the dynamics.State Representation is really important.\n",
    "\n",
    "2. Reduce the dimensionality of `Action` and `State` space.\n",
    "\n",
    "  Focus on explicitly reducing the noise from the `state`. If you don't have a good set of minimal features required for the task, which is often the case, focus on building a pipeline to choose relevant features from you global set. Classical techniques may be of little use here, It is worthwhile to see if we can use the orginal task and the model to glean feature importance.\n",
    "\n",
    "  Here `state` simply refers to the features fed as input to the network. Theoritically,it seems network should be able to do feature selection and engineering implicitly. But, We have found drastic improvements by reducing noise from the input features before feeding to the network. In stock markets, many patterns found through training data don't generalize. It is intuitive to rely on `Occam's Razor`- Among two similarly performing strategies, prefer the one that used less features. Many RL algorithms relies on having access to a hashtable storing visitation frequencies and other metrics of the state-space. A neural network acts as a drop-in replacement in modern DeepRL methods with potentially infinite state-spaces. This might also contribute to why the performance is improved by drastically reducing the dimensionality of the state. \n",
    "  \n",
    "  And reducing Action space has more to do with decreasing the complexity of the modeling problem.\n",
    "  \n",
    "\n",
    " \n",
    "  \n",
    "  \n",
    "  \n",
    " 5. Engineering Environment\n",
    "  \n",
    "  \n",
    " 6. Reward Engineering\n",
    " \n",
    " \n",
    "  \n",
    "  \n",
    " 9. Develop checks for checking robustness.\n",
    " \n",
    " For other domains it translates to developing techniques of accessing overfitting apart from checking run-on-test.\n",
    " \n",
    "10. You can use machine learning to understand ML models.\n",
    "\n",
    "11. No single metric to optimize\n",
    "\n",
    "12. Carefully Engineer the environment.\n",
    "Reward engineering and desigining environment are intimately tied. Eg: Agent got stuck in high portfolio states. For instance if the global trend is up..it learned to get some high portfolio and stay there. Correlation vs causation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Notes \n",
    "\n",
    "1. \n",
    "\n",
    "2. Focus on gathering Large amounts of Clean Data. This should be the first priority.\n",
    "\n",
    "\n",
    "4. Focus on solving the most general instantiation of the problem. This system can later be used as a backbone to specialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Philosophical musings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not to join in a startup ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm assuming you are an employee, not the founder.\n",
    "- Lack of proper mentorship. Startups hire just enough people to get things done. And often times you are the only one working on an area. During my time at Niveshi, I was the only machine learning engineer at the company. To grow, you need fast feedback loops. Nothing replaces learning from an experienced mentor.\n",
    "\n",
    "- You won't get rich by working at a startup.You will get rich by deploying large sums of capital over multiple startups. Capital is cheaper than time. You can only work in so many startups over your career.Given abysmal success rate, it is stupid to diversify your time this way. Moreover,as an employee you will own 10 to 30 times lower equity than the founders. This is true even if you are the founding engineer of an early stage startup. Considering that you are just starting out your career,your market value is low. You will be paid minimal equity and compensation possible, even if they are generous. A better alternative is to work towards increasing your reputation early in your career. Earn a safety net of Career capital and money by working in big companies. Then,later start your own company or even work at a startup when you are potentially overvalued. Diversifying your bets among several low-risk and high-risk has better return profile than  medium-risk ones.[https://en.wikipedia.org/wiki/Barbell_strategy]\n",
    "\n",
    "- You will likely learn some bad engineering practices. Believe it or not we managed to get away without having any kind of unittesting. Our code get's pushed only by means of peer review.\n",
    "\n",
    "- There are no established practices to deal with common painpoints. Resolving conflict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
