{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "> A catalogue of not so deep ideas in Deep Learning.\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Research]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is a collection of techniques for finding functions that can approximate arbitrary input-output mappings, while capturing enough structure of the problem to be able to use them in practical applicatons.language-translation systems,Image classification,Movie Recommendation systems are Some notable applications where they have become defacto-choice. This is by no means an exhaustive treatment of the field. Here I provide concise summaries,followed by a simple implementation of some of the most interesting ideas in the field. I believe that many things in the field are unnecessarily complicated by lengthy treatment where a readable code and short explanation will suffice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note` : This is not intended to be the first introduction to deep learning. Here I wanted to succintly catalogue some the latest advancements in the field. But, respecting tradition the first module starts from the basics. Only hard prerequisite is to have a good intuition of `matrix multiplication` notion of `taking a derivative`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many deep learning models follow a simple recipe:\n",
    "\n",
    "    1. Gather the data.\n",
    "    2. Define learnable parameters. And specify how they will interact with the data.(architecture)\n",
    "    3. Define a loss function to minimize.\n",
    "    4. Adjust the parameters until satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a linear regression model using gradient-descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will see how we can perform all the above steps starting with the most barebones implementation. Note that the procedure outlined here is general purpose - meaning the way we adjust `parameters` is going to remain same irrecpective of the modality of the data, details of the loss function or the architecture.\n",
    "\n",
    "Step 1. Gather the data\n",
    "\n",
    "Let's generate some fake data.Let's assume that the data is coming from $y = 2*x1 - 4.2*x2 + 1 + noise(measurement error)$. This can be more succintly represented in vector notation :\n",
    "$$y = \\begin{bmatrix} x1 \\\\ x2 \\end{bmatrix} . \\begin{bmatrix} 2 \\\\ -4.2 \\end{bmatrix} + 1$$\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_data(*params,const=None,rows=1000):\n",
    "    #number of features in the input\n",
    "    dim = len(params)\n",
    "    x = np.random.normal(0,0.3,(rows,dim))\n",
    "    y = x@np.array([params]).T\n",
    "    if const:\n",
    "        y += np.array([const])\n",
    "    return x,y\n",
    "\n",
    "x,y = get_data(2,-4.2,const=1)\n",
    "\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Define learnable parameters. And specify how they will interact with the data.(architecture)\n",
    "\n",
    "Now we aim to learn the right coefficients to approximate the data generation process. First let's look at some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.121630476687219"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with a random guess that respects the sanctity of the data.i.e our inputs are of dimension 1000*2 \n",
    "# outputs are 1000*1. Multiplying inputs by a 2*1 matrix(weights) and adding a constant(bias) is the simplest way\n",
    "# to ensure an output of 1000*1. \n",
    "\n",
    "# initial guess\n",
    "init_weights = np.array([[0.,-1.]]) #shape -> 1*2\n",
    "init_bias = np.array([0.])\n",
    "\n",
    "#expected output\n",
    "def give_expected_output(inpt,weights,bias):\n",
    "    return ((inpt@weights.T) + bias)\n",
    "\n",
    "out = give_expected_output(x,init_weights,init_bias)#shape -> 1000*1\n",
    "\n",
    "def get_error(out,expected_out):\n",
    "    return np.mean((expected_out - out)**2)\n",
    "\n",
    "get_error(out,y) # IF WE CAN DRIVE THIS NUMBER DOWN TO ZERO VIA A GENERAL PURPOSE PROCESS,WE ARE GOOD TO GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grads(weights,bias,x,y,loss_func='squared_loss'):\n",
    "    if loss_func == 'squared_loss':\n",
    "        weights_grad = 2*np.mean((x@weights.T + bias - y)*weights)\n",
    "        bias_grad = 2*np.mean((x@weights.T + bias - y))\n",
    "    else:\n",
    "        print(\"Sorry I'm not yet scalable enough for arbitrary loss functions\")\n",
    "    return weights_grad,bias_grad\n",
    "        \n",
    "\n",
    "\n",
    "grad_init_weights,grad_bias = get_grads(init_weights,init_bias,x,y,loss_func = 'squared_loss')\n",
    "\n",
    "def learn(x,y,init_weights,init_bias,loss_func,lr=0.001,epochs=5000):\n",
    "    out = give_expected_output(x,init_weights,init_bias)\n",
    "    error = loss_func(out,y)\n",
    "    #print(f'initial error, epoch 0: {error}')\n",
    "    errors = [error]\n",
    "    pres_lr = lr\n",
    "    for i in range(epochs):\n",
    "        weight_grad,bias_grad = get_grads(init_weights,init_bias,x,y)\n",
    "        init_weights -= weight_grad*pres_lr\n",
    "        init_bias -= bias_grad*pres_lr\n",
    "        out = give_expected_output(x,init_weights,init_bias)\n",
    "        error = loss_func(out,y)\n",
    "        if np.mean(weight_grad)<0.0001:\n",
    "            pres_lr = pres_lr*2\n",
    "        errors.append(error)\n",
    "    return errors,init_weights,init_bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2be3ea9df0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors,final_weights,final_bias = learn(x,y,init_weights,init_bias,get_error)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.82604061, -1.82604061]]), array([0.97582166]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_weights,final_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above code what happens if 'lr' is not dynamically adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Inductive biases - convolutions and recurrent networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below feedforward network each neuron is connected to all the neurons in the previous layer. No inductive biases in the connectivity.\n",
    "\n",
    "![FeedForward](my_icons/fc.png \"Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/\")\n",
    "\n",
    "`Locality` : For almost all natural signals it is easier to predict the future using recent past compared to any earlier versions. `Locality` allows for the sparsity of weights. We can put faraway weights to zero. In the below figure the `15` weights of the first layer is reduced to `9`. It's also important to be aware of the concept of `receptive field`(RF). `RF` of layer `a` w.r.t `b` is simply the number of neurons in `a` that influence the outputs of layer b.\n",
    "\n",
    "![](my_icons/sparsity.png \"Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/\")\n",
    "\n",
    "\n",
    "\n",
    "`Stationarity` : Same patterns are repeated again and again.\n",
    "We don't need connections from the inputs far down. `Stationarity` implies weight sharing. \n",
    "\n",
    "\n",
    "![](my_icons/parashare.png \"Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/\")\n",
    "\n",
    "`Compositionality`: There are hierarchies. Letters make up words,words make up sentences and so on. `Compisitionality` implies deeper networks. \n",
    "\n",
    "Now We will see how popular building blocks like CNN'S and RNN'S leverage these properties. `CNN`'s are a linear layer with lots of `weight sharing` and `sparsity`. RNN's just use `weight sharing` but BPTT takes the `locality` into account.\n",
    "\n",
    "CNN  = linear layer + weight sharing + sparsity.\n",
    "\n",
    "Consider convolving over a 4 * 4 inputs with a 3 * 3 kernel with a unit stride as presented below.\n",
    "![](my_icons/cnn.png \"Credit:https://arxiv.org/pdf/1603.07285.pdf\")\n",
    "\n",
    "If I stack the 2-d input into a 1-d vector by unrolling left to right and top to bottom, the convolution can be represented as a matrix multiplication.\n",
    "![](my_icons/conv_mat.png \"Credit:https://arxiv.org/pdf/1603.07285.pdf\")\n",
    "\n",
    "The `zeros` along the columns encode `locality` while the replication of same weights along the rows account for `stationarity`. If the above properties doesn't make sense for your input , then CNN's aren't the right choice.\n",
    "\n",
    "RNN = linear layer + weight sharing + BPTT(Back-prop through time)\n",
    "\n",
    "![](my_icons/rnn.png \"Credit:https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-3/\")\n",
    "\n",
    "RNN's are used for sequence data. In the above figure the `arrow` indicates matrix multiplication. The hidden state `h(t)` at time `t` is equal to Affine_transform(x(t)) + Affine_transform(h(t-1)).(`Note`: Affine_transform refers to matrix multiplication). The following code will makes it all clear.\n",
    "\n",
    "\n",
    "Consider the following sequence : \n",
    "\n",
    "'Hey Jude, don't make it bad.\n",
    "\n",
    "Take a sad song and make it better.\n",
    "\n",
    "Remember to let her into your heart,\n",
    "\n",
    "Then you can start to make it better.\n",
    "'\n",
    "\n",
    "Now we would want to classify it into either positive or negative sentiment. Ideally we would have a collection of such sequences with their corresponding labels. Here note that the number of words in each sequence need not be same. A naive approach would be to string all the words in a sequence to a single column,and run it through a fullyconnected network.(variable sequence length still poses a problem.) Let's see how an RNN can accomplish this with much less parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,i_sz,h_sz,out_sz):\n",
    "        super().__init__()\n",
    "        self.i_sz = i_sz\n",
    "        self.h_sz = h_sz\n",
    "        self.out_sz = out_sz\n",
    "        self.input_hidden = nn.Linear(self.i_sz,self.h_sz)\n",
    "        self.hidden_hidden = nn.Linear(self.h_sz,self.h_sz)\n",
    "        self.hidden_output = nn.Linear(self.h_sz,self.out_sz)\n",
    "    def forward(self,x):\n",
    "        h = 0\n",
    "        bs,seq_len,emb_sz = x.shape\n",
    "        for i in range(seq_len):\n",
    "            # This just adds the hidden representation which is a function of all the words fed until t-1 to the \n",
    "            #word at t\n",
    "            h = h + self.input_hidden(x[:,i,:])\n",
    "            # Stores the hidden representation for next  word in seq.\n",
    "            h = F.relu(self.hidden_hidden(h))\n",
    "        return self.hidden_output(h)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 101\n",
    "seq_len = 10\n",
    "vector_len = 100\n",
    "seq_1 = torch.rand(bs,seq_len,vector_len)\n",
    "rnn = RNN(i_sz = vector_len,h_sz=20,out_sz=1)\n",
    "out = rnn(seq_1)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be seen as a two layer network.The last layer converts `h_sz` to the output dimension, while the first layer maps `i_sz` to `h_sz`. But, the first layer only uses weight matrices of size 100 * 20,20 * 20. Totalling of around 2400 parameters. Our naive version would have seq_len * i_sz * h parameters.(around 20000). Moreover, In RNN number of paramenters is independent of\n",
    "sequence length.\n",
    "\n",
    "The above model is just an instantiation of `weight sharing` for sequential data. We haven't still leveraged the `locality` aspect (`sparsity`).\n",
    "\n",
    "Moreover,eventhough we only have 3 different weight matrices,the actual number of layers is proportional to the `size` of the `for loop`. Aside from being very slow and memory intensive, gradients of loss w.r.t initial operations( i = 0) very unlikely to be stable.(According to the chain rule of derivatives the gradient of loss w.r.t first matrix multiplication would involve multiplying atleast `seq_len` of partial derivatives. The resultant can easily explode or vanish.) What if we only take gradients for the last `n` operations. This is also called `Truncated BPTT`. Here's the modified `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,x):\n",
    "        h = 0\n",
    "        bs,seq_len,emb_sz = x.shape\n",
    "        for i in range(seq_len):\n",
    "            # This just adds the hidden representation which is a function of all the words fed until t-1 to the \n",
    "            #word at t\n",
    "            h = h + self.input_hidden(x[:,i,:])\n",
    "            # Stores the hidden representation for next  word in seq.\n",
    "            h = F.relu(self.hidden_hidden(h))\n",
    "            if i%3 == 0:\n",
    "                h = h.detach()\n",
    "        return self.hidden_output(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just flushes the memory for the backward pass after every 3 steps. Aside from solving obvious practical problems,this also has regularizing effects. We are implicitly encoding our bias - you need not look past the last 3 points in the sequence - a.k.a `locality`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sequence of vectors ($x_1$,$x_2$..$x_n$). It helps to imagine them as vectors corresponding to sequence of words. If you can bear with me,the following operation converts them into another sequence of vectors ($y_1$,$y_2$..$y_n$) of same dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([4, 4]), torch.Size([3, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two column vectors(each with dimension of 3*1)\n",
    "import torch\n",
    "import torch.tensor as tensor\n",
    "X = torch.cat([tensor([[1.],[2.],[3.]]),tensor([[4.],[5.],[6.]]),tensor([[7.],[8.],[9.]]),tensor([[10.],[11.],[12.]])],dim=-1)\n",
    "matrix = torch.rand(4,4) # a random matrix\n",
    "Y = X@matrix\n",
    "X.shape,matrix.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,let's generate the same `V` with a fancier set of operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = X.T@X\n",
    "Y_new = X@temp.T\n",
    "Y_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `temp.T` is acting as `matrix`. This also removes the need for additional initialization. This operation also lends to the following intution:\n",
    "\n",
    "- `temp` is the dot product of each column vector in U with all the vector within it. The captures the measure of similarity between the vectors.\n",
    "- `Y_new` is just a linear combination of U with the corresponding weights from the `temp`.\n",
    "\n",
    "- In other words, $y_i =\\sum_{j} w_{ij}.x_{j}$ where `w`'s are taken frow the rows of `temp`.\n",
    "\n",
    "\n",
    "In the above figure each vector $x_{i}$ is used three times. Let's take $x_{2}$ for illustration:\n",
    "\n",
    "- To get $w_{22}$\n",
    "- Similarly to get weight's required for all the other outputs $y_1$,$y_3$ and $y_4$ \n",
    "- $x_2$ is also used in linear weighting with `w`'s to get $y_2$\n",
    "\n",
    "But, In the whole compution we are not learning any weights. Everything is being generated from the input. We can introduce three different set of `x`'s for each of the above operations. Let's initialize three square matrices $W_k$,$W_q$,$W_v$, each of size (4,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_k,w_q,w_v = torch.rand(4,4),torch.rand(4,4),torch.rand(4,4) # learnable parameters\n",
    "\n",
    "keys,queries,values = X@w_k,X@w_q,X@w_v\n",
    "# This is the naming convention used in the literature.\n",
    "temp = queries.T@keys\n",
    "Y_new =values@temp.T\n",
    "Y_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. `Self-attention` refers to performing above operations. We additionally normalize the weights in the `temp` with `softmax`. Further, We can also use sets of matrices ($W_k$,$W_q$,$W_v$),essentially replicationg self-attention with different weight matrices. The resulting outputs can be concatenated and be passed through a linear layer to get back the orginal dimension.(This is called `multi-head attention`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,feat_sz,n_heads=1):\n",
    "        super().__init__()\n",
    "        # for n_heads we need the corresponding number of weight matrices of size feat_sz*feat_sz to get new\n",
    "        #set of (keys,queries,values),Computationally this can be fused inside a single linear operation.\n",
    "        self.heads = n_heads\n",
    "        self.get_keys = nn.Linear(feat_sz,feat_sz*n_heads,bias=False)\n",
    "        self.get_queries = nn.Linear(feat_sz,feat_sz*n_heads,bias=False)\n",
    "        self.get_values = nn.Linear(feat_sz,feat_sz*n_heads,bias=False)\n",
    "        self.comb_heads = nn.Linear(n_heads*feat_sz,feat_sz)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # typically data is fed with features along the `columns`.\n",
    "        bs,n_seq,feat_sz = x.size()\n",
    "        keys = self.get_keys(x).view(bs,n_seq,self.heads,feat_sz)\n",
    "        queries = self.get_queries(x).view(bs,n_seq,self.heads,feat_sz)\n",
    "        values = self.get_values(x).view(bs,n_seq,self.heads,feat_sz)\n",
    "        # `torch.bmm` performs matrix multiplication for a given batch.It is efficient to squeeze n_heads along\n",
    "        # with batches and perform the calculation at once.\n",
    "        keys = keys.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz)\n",
    "        queries = queries.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz)\n",
    "        values = values.transpose(1,2).contiguous().view(bs*self.heads,n_seq,feat_sz)\n",
    "        dot = torch.bmm(queries,keys.transpose(1,2))\n",
    "        #Rescaling the elements to control the scale\n",
    "        dot = dot/math.sqrt(feat_sz)\n",
    "        dot = F.softmax(dot,dim=2)\n",
    "        out = torch.bmm(dot,values).view(bs,self.heads,n_seq,feat_sz)\n",
    "        #reshaping\n",
    "        out = out.transpose(1,2).contiguous().view(bs,n_seq,self.heads*feat_sz)\n",
    "        # passing through a linear layer to combine all the heads\n",
    "        out = self.comb_heads(out)# gives bs,n_seq,n_heads\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 3]), torch.Size([1, 4, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = X.unsqueeze(0).transpose(1,2) # input with features arranged in columns(shape = [1, 4, 3])\n",
    "\n",
    "sa = SelfAttention(feat_sz=3)\n",
    "Y = sa(input_)# simple attention\n",
    "\n",
    "mha = SelfAttention(feat_sz=3,n_heads=6)\n",
    "Y_6 = sa(input_)\n",
    "\n",
    "Y.shape,Y_6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Attention is first introduced to deal with sequences in the context of natural language. But, nothing in the above implementation handles `order`. It just maps a `set of vectors` to another. To enforce order, we can simply add a `position vector` to our inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above ideas mark the end of the theoretical minimum needed. Below you would find the notched up version of the above content by solving simple but practical problems end-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RL Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning allows for learning generalizable mappings between input and output. In supervised setting we are given a `fixed dataset` D = {$(x_{i},y_{i})$},and are tasked to predict $y_{i}$ using $x_{i}$. Thus, we know groud truth for all the input data. This let's us formulate a `loss` that reflects our dissatisfaction with the predicted `outputs` and optimize over it. But, consider how we learn to perform any new task ? The dataset and the learning signal comes sequentially and is also dependent on our actions. We don't have prepared datasets in real life. We learn from experience. `RL` deals with learning under this natural setting. The key difference to note is that the dataset is not `constant`, It changes everytime, contingent on your actions and the stochasticity in the environment. To make this clear, consider learning to play a video game. The pixels (call it `state`) and the score you receive ( call it `reward`) cannot be predetermined until you actually go through the experience. Here the dataset D = {$(state_{i},reward_{i})$} is not same everytime you play the game. `RL` provides a formalism for learning optimal decision making. This will become clear when we see some concrete examples and code. But combing these techniques with `Neural networks` has given us general algorithms that `learn to play atari games`, `beat world champions at Go` and `train robots to learn simple tasks`.\n",
    "![](my_icons/rlsuc.png \"Credit: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Following optimization problem. Find the shortest path from state $s_0$ to goal $g$,where the edges indicate the cost/distance ?\n",
    "\n",
    "![](my_icons/sp.png \"Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf\")\n",
    "\n",
    "Here taking a greedy approach will fail as actions will have long-term consequences. Solving the above problem efficiently requires realizing that `the distance to any node along the shortest path from source to destination is also shortest path`. In the above example the shortest path to `g` should either be through `d` or `f`.(one of the incoming edges). Let the shortest distance from a node `s` to `g` be given by $v^{*}(s)$. Then the last edge of the shortest path should come from evaluating $v^{*}(f))$ and $v^{*}(d)$, where $v^{*}(f) = 1 + v^{*}(g)$ and $v^{*}(d) = min(3+v^{*}(g),1+v^{*}(f))$. This backtracking or dynamic programming approach of finding one edge at a time is illustrated below.\n",
    "\n",
    "\n",
    "![](my_icons/op.png \"Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf\")\n",
    "\n",
    "\n",
    "Now, for each action we take, let us also add a `transition probability` that defines the likelihood of ending up in any of the available states given the action. Here, for states `c` and `e` we added some randomness. This will let us model more realistic scenarios. Consider an `RL agent` driving your car. The consequences of any action (eg: Turning the steering to the right given the visual view of the road.) can only be modeled probabilistically.\n",
    "\n",
    "![](my_icons/ssp.png \"Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf\")\n",
    "\n",
    "\n",
    "Here by weighting w.r.t the transition probabilities we can recover $v^{*}()$ for all states. Optimal policy is again achieved by acting greedily w.r.t $v^{*}()$. For example , $v^{*}(c) = min(4+0.3*v^{*}(e)+0.7*v^{*}(d),2+v^{*}(e))$. In RL, we call this `Bellmann Equation`.\n",
    "\n",
    "![](my_icons/sspb.png \"Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf\")\n",
    "\n",
    "![](my_icons/be.png \"Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf\")\n",
    "\n",
    "\n",
    "Modern `Deep RL` deals with solving this stochastic version of the problem when the transition probabilities are not available and the number of possible states is very large. Consider the following video game playing scenario. The pixels on the screen at any timestamp can be taken as `state`. The game rules provides list of `possible actions` and the corresponding `reward`(increase in score).\n",
    "\n",
    "![](my_icons/vid.png \"Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf\")\n",
    "\n",
    "By the end of this module we will develop all the machinery to understand the algorithms that learn optimal(or good) policies for this general case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellmann Equation and MDP's\n",
    "\n",
    "In this section we will devolop some theory. First let's define an object called Markov Decision Process (MDP),given by the tuple of $(S,A,P,R,\\gamma)$. Here discounting factor $\\gamma$ is largely a mathematical convenience as it helps to bound the `total reward`. Any reward $r_t$ received at timestamp `t` is multiplied by $\\gamma^{t-1}$, that is we prefer immediate rewards over faraway ones. The term `Markov` refers to the fact that given present state `s` and action taken from there `a`, next state is independent of the past trajectory. Consider the example of `stochastic shortest path` but with the goal state `g` infinitely far away. In that setting we want to maximize the average reward we will get starting from any state.\n",
    "\n",
    "\n",
    "The only `knob` agent has is policy $\\pi : S \\rightarrow A$.(which of the possible actions to take). The environment dynamics (P and R) are not under agent's control. For simplicity,let's assume that rewards are bounded and positive. Let's say that the agent has a policy $\\pi$ and starts to intreract with the environment. The value function `v(s)` refers to the average reward starting from state `s` and following policy $\\pi$. Then `v(s)` for all the states satisfies a recursive definition as shown below.\n",
    "\n",
    "![](my_icons/bellmann.png \"\")\n",
    "\n",
    "\n",
    "Thus finding $v^{\\pi}(s)$ amounts to solving system of linear equation or in matrix notation finding the inverse of a matrix. But, we still need the proof for the existence of the inverse for $I - \\gamma.P^{\\pi}$. Before going further, it's important to thoroughly understand the bellmann equation : $V^{\\pi} = R^{\\pi} + \\gamma.P^{\\pi}.V^{\\pi}$. The deriviation involves observing the fact that once the agent takes initial action from state `s`, the transition probability will dictate it's next state `s'`. From there, the average reward by definition is  given by `v(s')`, leaving a recursive definition.\n",
    "\n",
    "\n",
    "![](my_icons/matform.png \"\")\n",
    "\n",
    "Below i have give the proof for the existence of inverse. If we recall, a matrix A($n * n$) multiplied by a column vector X ($n * 1$) merely takes a linear combination of all the column vectors in A. Existence of inverse to `A` means that there doesnot exist a non-zero vector X, that can collapse A to a null vector. Using this fact and the traingular inequality on $I - \\gamma.P^{\\pi}$ completes the proof.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/proof.jpg \"\")\n",
    "\n",
    "\n",
    "Now let's define $v^{*}(s)$ as the maximum expected reward that we can get from state `s` under `any policy`. Once we are able to extract these values, optimal policy becomes obvious. `Value Iteration` methods try to apprimate this $V^{*}(s)$. They start with some arbitrary function like $f(s) = 0 \\forall s$ and iteratively bring $f$ closer to $V^{*}$. Let's also define $Q^{\\pi}(s,a)$ as the expected reward under the policy $\\pi$,when we take action `a` at state `s` and sebsequently sample actions according to $\\pi$. Similarly $Q^{*}(s,a)$ is also defined as the maximum Q that can be achieved under any policy. We often want $Q^{*}$ values over $V^{*}$ as simply choosing greedily w.r.t $Q^{*}$ gives the optimal policy. To see the advantage more clearly, Imagine an oracle that can give you $V^{*}$ for any `s`. Now, the agent starts at state `s` and is looking to take optimal action. It first has to take all possible actions from that state to see where it would end up. And for each subsequent state `s'` , it has to query the oracle to get $V^{*}(s')$. Only after this we can determine the best action from initial state s as $max_{a\\in A}(r_1 + v^{*}(s'))$. But having $Q^{*}$ values let's us choose this action  directly by evaluating $Q_{a\\in A}^{*}(s,a)$. There exists a proof that for discounted infinite horizon MDP's there exists a stationary and deterministic optimal policy for all states simultaneously. Let's call this $\\pi^{*}$. It is easy to see that both $V^{*}$ and $Q^{*}$ will also satisfy a similar recursive relation called `Bellmann Optimality equations`.\n",
    "\n",
    "\n",
    "![](my_icons/b.jpg \"\")\n",
    "\n",
    "\n",
    "We can also define bellmann operator that let's us concisely reprent the equations. Next.\n",
    "\n",
    "\n",
    "![](my_icons/b2.jpg \"\")\n",
    "\n",
    "![](my_icons/vi1.jpg \"\")\n",
    "\n",
    "![](my_icons/vi2.jpg \"\")\n",
    "\n",
    "![](my_icons/vi3.jpg \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
