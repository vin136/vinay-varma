<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Reinforcement Learning - Theory (Chapter 1) | Home</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Deep Reinforcement Learning - Theory (Chapter 1)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Basics from first principles." />
<meta property="og:description" content="Basics from first principles." />
<link rel="canonical" href="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:url" content="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:site_name" content="Home" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-21T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Basics from first principles.","@type":"BlogPosting","headline":"Deep Reinforcement Learning - Theory (Chapter 1)","dateModified":"2021-02-21T00:00:00-06:00","datePublished":"2021-02-21T00:00:00-06:00","url":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://vinayvarma.work/feed.xml" title="Home" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Reinforcement Learning - Theory (Chapter 1) | Home</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Deep Reinforcement Learning - Theory (Chapter 1)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Basics from first principles." />
<meta property="og:description" content="Basics from first principles." />
<link rel="canonical" href="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:url" content="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:site_name" content="Home" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-21T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Basics from first principles.","@type":"BlogPosting","headline":"Deep Reinforcement Learning - Theory (Chapter 1)","dateModified":"2021-02-21T00:00:00-06:00","datePublished":"2021-02-21T00:00:00-06:00","url":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://vinayvarma.work/feed.xml" title="Home" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Home</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Reinforcement Learning - Theory (Chapter 1)</h1><p class="page-description">Basics from first principles.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-21T00:00:00-06:00" itemprop="datePublished">
        Feb 21, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      20 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Research">Research</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/vin136/vinay-varma/tree/master/_notebooks/2021-02-21-Deep-Learning-Useful-Ideas.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/vin136/vinay-varma/master?filepath=_notebooks%2F2021-02-21-Deep-Learning-Useful-Ideas.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/vin136/vinay-varma/blob/master/_notebooks/2021-02-21-Deep-Learning-Useful-Ideas.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Basics">Basics </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Train-a-linear-regression-model-using-gradient-descent.">Train a linear regression model using gradient-descent. </a></li>
<li class="toc-entry toc-h3"><a href="#Adding-Inductive-biases---convolutions-and-recurrent-networks.">Adding Inductive biases - convolutions and recurrent networks. </a></li>
<li class="toc-entry toc-h3"><a href="#Self-Attention">Self-Attention </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Deep-RL---Theory">Deep RL - Theory </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Motivation">Motivation </a></li>
<li class="toc-entry toc-h3"><a href="#Problem">Problem </a></li>
<li class="toc-entry toc-h3"><a href="#Bellmann-Equation-and-MDP's">Bellmann Equation and MDP&#39;s </a></li>
<li class="toc-entry toc-h3"><a href="#Search-for-optimal-State-Values.">Search for optimal State Values. </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Value-Iteration">Value Iteration </a></li>
<li class="toc-entry toc-h2"><a href="#Policy-Iteration-(PI)">Policy Iteration (PI) </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-21-Deep-Learning-Useful-Ideas.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I plan on to write series of posts explaining key ideas in Modern Reinforcement Learning(RL). Current post covers basics of deep learning,followed by an introduction to RL Theory.(Thourougly proves two key algorithms for learning in RL setting - Value Iteration and Policy Iteration.). If you prefer a more practical introduction refer <a href="https://vinayvarma.work/reinforcement%20learning/2020/05/06/Q-Learning.html">this</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Note</code> : This is not intended to be the first introduction to deep learning. Here I just provide a self-contained summary. Only hard prerequisite is to have good intuitions for <code>matrix multiplication</code> and the notion of <code>taking a derivative</code>. Otherwise refer to <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of linear algebra</a> and <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">Calculus</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Basics">
<a class="anchor" href="#Basics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Basics<a class="anchor-link" href="#Basics"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Many deep learning models follow a simple recipe:</p>

<pre><code>1. Gather the data.
2. Define learnable parameters. And specify how they will interact with the data.(architecture)
3. Define a loss function to minimize.
4. Adjust the parameters until satisfied.</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Train-a-linear-regression-model-using-gradient-descent.">
<a class="anchor" href="#Train-a-linear-regression-model-using-gradient-descent." aria-hidden="true"><span class="octicon octicon-link"></span></a>Train a linear regression model using gradient-descent.<a class="anchor-link" href="#Train-a-linear-regression-model-using-gradient-descent."> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we will see how we can perform all the above steps starting with the most barebones implementation. Note that the procedure outlined here is general purpose - meaning the way we adjust <code>parameters</code> is going to remain same irrecpective of the modality of the data, details of the loss function or the architecture.</p>
<p>Step 1. Gather the data</p>
<p>Let's generate some fake data.Let's assume that the data is coming from $y = 2*x1 - 4.2*x2 + 1 + noise(measurement error)$. This can be more succintly represented in vector notation :

$$y = \begin{bmatrix} x1 \\ x2 \end{bmatrix} . \begin{bmatrix} 2 \\ -4.2 \end{bmatrix} + 1$$
</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">,</span><span class="n">const</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">rows</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1">#number of features in the input</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,(</span><span class="n">rows</span><span class="p">,</span><span class="n">dim</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="nd">@np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="n">const</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">const</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>

<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mf">4.2</span><span class="p">,</span><span class="n">const</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((1000, 2), (1000, 1))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Step 2. Define learnable parameters. And specify how they will interact with the data.(architecture)</p>
<p>Now we aim to learn the right coefficients to approximate the data generation process. First let's look at some code.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Start with a random guess that respects the sanctity of the data.i.e our inputs are of dimension 1000*2 </span>
<span class="c1"># outputs are 1000*1. Multiplying inputs by a 2*1 matrix(weights) and adding a constant(bias) is the simplest way</span>
<span class="c1"># to ensure an output of 1000*1. </span>

<span class="c1"># initial guess</span>
<span class="n">init_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">]])</span> <span class="c1">#shape -&gt; 1*2</span>
<span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>

<span class="c1">#expected output</span>
<span class="k">def</span> <span class="nf">give_expected_output</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">inpt</span><span class="nd">@weights</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">give_expected_output</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">)</span><span class="c1">#shape -&gt; 1000*1</span>

<span class="k">def</span> <span class="nf">get_error</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">expected_out</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">expected_out</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">get_error</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># IF WE CAN DRIVE THIS NUMBER DOWN TO ZERO VIA A GENERAL PURPOSE PROCESS,WE ARE GOOD TO GO</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2.121630476687219</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_grads</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="s1">'squared_loss'</span><span class="p">):</span>
    <span class="c1"># Just taking the mathematical gradient as defined by the model.</span>
    <span class="k">if</span> <span class="n">loss_func</span> <span class="o">==</span> <span class="s1">'squared_loss'</span><span class="p">:</span>
        <span class="n">weights_grad</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x</span><span class="nd">@weights</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">bias_grad</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x</span><span class="nd">@weights</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Sorry I'm not yet scalable enough for arbitrary loss functions"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights_grad</span><span class="p">,</span><span class="n">bias_grad</span>
        


<span class="n">grad_init_weights</span><span class="p">,</span><span class="n">grad_bias</span> <span class="o">=</span> <span class="n">get_grads</span><span class="p">(</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">loss_func</span> <span class="o">=</span> <span class="s1">'squared_loss'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">loss_func</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">give_expected_output</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="c1">#print(f'initial error, epoch 0: {error}')</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">[</span><span class="n">error</span><span class="p">]</span>
    <span class="n">pres_lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">weight_grad</span><span class="p">,</span><span class="n">bias_grad</span> <span class="o">=</span> <span class="n">get_grads</span><span class="p">(</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">init_weights</span> <span class="o">-=</span> <span class="n">weight_grad</span><span class="o">*</span><span class="n">pres_lr</span>
        <span class="n">init_bias</span> <span class="o">-=</span> <span class="n">bias_grad</span><span class="o">*</span><span class="n">pres_lr</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">give_expected_output</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weight_grad</span><span class="p">)</span><span class="o">&lt;</span><span class="mf">0.0001</span><span class="p">:</span>
            <span class="n">pres_lr</span> <span class="o">=</span> <span class="n">pres_lr</span><span class="o">*</span><span class="mi">2</span>
        <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">errors</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">errors</span><span class="p">,</span><span class="n">final_weights</span><span class="p">,</span><span class="n">final_bias</span> <span class="o">=</span> <span class="n">learn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">get_error</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&lt;matplotlib.lines.Line2D at 0x7f2be3ea9df0&gt;]</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">final_weights</span><span class="p">,</span><span class="n">final_bias</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(array([[-0.82604061, -1.82604061]]), array([0.97582166]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the above code what happens if 'lr' is not dynamically adjusted ? Now, imagine an arbitrary architecture (interaction between parameters and data). Can we have a general purpose <code>get_grads</code> function that efficiently evaluates the gradient for this network ?. For many such practical concerns, I suggest going through this free course from <a href="https://course.fast.ai/">fast.ai</a>. Modern libraries like Pytorch provide simple abstractions to ignore all such details. Now some general intuitions towards coming up with architectures for learning more complex functions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Adding-Inductive-biases---convolutions-and-recurrent-networks.">
<a class="anchor" href="#Adding-Inductive-biases---convolutions-and-recurrent-networks." aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding Inductive biases - convolutions and recurrent networks.<a class="anchor-link" href="#Adding-Inductive-biases---convolutions-and-recurrent-networks."> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the below feedforward network each neuron is connected to all the neurons in the previous layer. No inductive biases in the connectivity.</p>
<p><img src="/images/copied_from_nb/my_icons/fc.png" alt="FeedForward" title="Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/"></p>
<p><code>Locality</code> : For almost all natural signals it is easier to predict the future using recent past compared to any earlier versions. <code>Locality</code> allows for the sparsity of weights. We can put faraway weights to zero. In the below figure the <code>15</code> weights of the first layer is reduced to <code>9</code>. It's also important to be aware of the concept of <code>receptive field</code>(RF). <code>RF</code> of layer <code>a</code> w.r.t <code>b</code> is simply the number of neurons in <code>a</code> that influence the outputs of layer b.</p>
<p><img src="/images/copied_from_nb/my_icons/sparsity.png" alt="" title="Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/"></p>
<p><code>Stationarity</code> : Same patterns are repeated again and again.
We don't need connections from the inputs far down. <code>Stationarity</code> implies weight sharing.</p>
<p><img src="/images/copied_from_nb/my_icons/parashare.png" alt="" title="Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/"></p>
<p><code>Compositionality</code>: There are hierarchies. Letters make up words,words make up sentences and so on. <code>Compisitionality</code> implies deeper networks.</p>
<p>Now We will see how popular building blocks like CNN'S and RNN'S leverage these properties. <code>CNN</code>'s are a linear layer with lots of <code>weight sharing</code> and <code>sparsity</code>. RNN's just use <code>weight sharing</code> but BPTT takes the <code>locality</code> into account.</p>
<p>CNN  = linear layer + weight sharing + sparsity.</p>
<p>Consider convolving over a 4 <em> 4 inputs with a 3 </em> 3 kernel with a unit stride as presented below.
<img src="/images/copied_from_nb/my_icons/cnn.png" alt="" title="Credit:https://arxiv.org/pdf/1603.07285.pdf"></p>
<p>If I stack the 2-d input into a 1-d vector by unrolling left to right and top to bottom, the convolution can be represented as a matrix multiplication.
<img src="/images/copied_from_nb/my_icons/conv_mat.png" alt="" title="Credit:https://arxiv.org/pdf/1603.07285.pdf"></p>
<p>The <code>zeros</code> along the columns encode <code>locality</code> while the replication of same weights along the rows account for <code>stationarity</code>. If the above properties doesn't make sense for your input , then CNN's aren't the right choice.</p>
<p>RNN = linear layer + weight sharing + BPTT(Back-prop through time)</p>
<p><img src="/images/copied_from_nb/my_icons/rnn.png" alt="" title="Credit:https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-3/"></p>
<p>RNN's are used for sequence data. In the above figure the <code>arrow</code> indicates matrix multiplication. The hidden state <code>h(t)</code> at time <code>t</code> is equal to Affine_transform(x(t)) + Affine_transform(h(t-1)).(<code>Note</code>: Affine_transform refers to matrix multiplication). The following code will makes it all clear.</p>
<p>Consider the following sequence :</p>
<p>'Hey Jude, don't make it bad.</p>
<p>Take a sad song and make it better.</p>
<p>Remember to let her into your heart,</p>
<p>Then you can start to make it better.
'</p>
<p>Now we would want to classify it into either positive or negative sentiment. Ideally we would have a collection of such sequences with their corresponding labels. Here note that the number of words in each sequence need not be same. A naive approach would be to string all the words in a sequence to a single column,and run it through a fullyconnected network.(variable sequence length still poses a problem.) Let's see how an RNN can accomplish this with much less parameters.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i_sz</span><span class="p">,</span><span class="n">h_sz</span><span class="p">,</span><span class="n">out_sz</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i_sz</span> <span class="o">=</span> <span class="n">i_sz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span> <span class="o">=</span> <span class="n">h_sz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_sz</span> <span class="o">=</span> <span class="n">out_sz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">i_sz</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">out_sz</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">emb_sz</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># This just adds the hidden representation which is a function of all the words fed until t-1 to the </span>
            <span class="c1">#word at t</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_hidden</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:])</span>
            <span class="c1"># Stores the hidden representation for next  word in seq.</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_hidden</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_output</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">101</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">vector_len</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">seq_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">vector_len</span><span class="p">)</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">i_sz</span> <span class="o">=</span> <span class="n">vector_len</span><span class="p">,</span><span class="n">h_sz</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">out_sz</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">seq_1</span><span class="p">)</span>

<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([101, 1])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This can be seen as a two layer network.The last layer converts <code>h_sz</code> to the output dimension, while the first layer maps <code>i_sz</code> to <code>h_sz</code>. But, the first layer only uses weight matrices of size 100 <em> 20,20 </em> 20. Totalling of around 2400 parameters. Our naive version would have seq_len <em> i_sz </em> h parameters.(around 20000). Moreover, In RNN number of paramenters is independent of
sequence length.</p>
<p>The above model is just an instantiation of <code>weight sharing</code> for sequential data. We haven't still leveraged the <code>locality</code> aspect (<code>sparsity</code>).</p>
<p>Moreover,eventhough we only have 3 different weight matrices,the actual number of layers is proportional to the <code>size</code> of the <code>for loop</code>. Aside from being very slow and memory intensive, gradients of loss w.r.t initial operations( i = 0) very unlikely to be stable.(According to the chain rule of derivatives the gradient of loss w.r.t first matrix multiplication would involve multiplying atleast <code>seq_len</code> of partial derivatives. The resultant can easily explode or vanish.) What if we only take gradients for the last <code>n</code> operations. This is also called <code>Truncated BPTT</code>. Here's the modified <code>forward</code> function.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">emb_sz</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># This just adds the hidden representation which is a function of all the words fed until t-1 to the </span>
            <span class="c1">#word at t</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_hidden</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:])</span>
            <span class="c1"># Stores the hidden representation for next  word in seq.</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_hidden</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="k">3</span> == 0:
                <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_output</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This just flushes the memory for the backward pass after every 3 steps. Aside from solving obvious practical problems,this also has regularizing effects. We are implicitly encoding our bias - you need not look past the last 3 points in the sequence - a.k.a <code>locality</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Self-Attention">
<a class="anchor" href="#Self-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Self-Attention<a class="anchor-link" href="#Self-Attention"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider a sequence of vectors ($x_1$,$x_2$..$x_n$). It helps to imagine them as vectors corresponding to sequence of words. If you can bear with me,the following operation converts them into another sequence of vectors ($y_1$,$y_2$..$y_n$) of same dimension.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Two column vectors(each with dimension of 3*1)</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.tensor</span> <span class="k">as</span> <span class="nn">tensor</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">],[</span><span class="mf">3.</span><span class="p">]]),</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">4.</span><span class="p">],[</span><span class="mf">5.</span><span class="p">],[</span><span class="mf">6.</span><span class="p">]]),</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">7.</span><span class="p">],[</span><span class="mf">8.</span><span class="p">],[</span><span class="mf">9.</span><span class="p">]]),</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">10.</span><span class="p">],[</span><span class="mf">11.</span><span class="p">],[</span><span class="mf">12.</span><span class="p">]])],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># a random matrix</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@matrix</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([3, 4]), torch.Size([4, 4]), torch.Size([3, 4]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now,let's generate the same <code>V</code> with a fancier set of operations.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span>
<span class="n">Y_new</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@temp</span><span class="o">.</span><span class="n">T</span>
<span class="n">Y_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 4])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here <code>temp.T</code> is acting as <code>matrix</code>. This also removes the need for additional initialization. This operation also lends to the following intution:</p>
<ul>
<li>
<code>temp</code> is the dot product of each column vector in U with all the vector within it. The captures the measure of similarity between the vectors.</li>
<li>
<p><code>Y_new</code> is just a linear combination of U with the corresponding weights from the <code>temp</code>.</p>
</li>
<li>
<p>In other words, $y_i =\sum_{j} w_{ij}.x_{j}$ where <code>w</code>'s are taken frow the rows of <code>temp</code>.</p>
</li>
</ul>
<p>In the above figure each vector $x_{i}$ is used three times. Let's take $x_{2}$ for illustration:</p>
<ul>
<li>To get $w_{22}$</li>
<li>Similarly to get weight's required for all the other outputs $y_1$,$y_3$ and $y_4$ </li>
<li>$x_2$ is also used in linear weighting with <code>w</code>'s to get $y_2$</li>
</ul>
<p>But, In the whole compution we are not learning any weights. Everything is being generated from the input. We can introduce three different set of <code>x</code>'s for each of the above operations. Let's initialize three square matrices $W_k$,$W_q$,$W_v$, each of size (4,4).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">w_k</span><span class="p">,</span><span class="n">w_q</span><span class="p">,</span><span class="n">w_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># learnable parameters</span>

<span class="n">keys</span><span class="p">,</span><span class="n">queries</span><span class="p">,</span><span class="n">values</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@w_k</span><span class="p">,</span><span class="n">X</span><span class="nd">@w_q</span><span class="p">,</span><span class="n">X</span><span class="nd">@w_v</span>
<span class="c1"># This is the naming convention used in the literature.</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">T</span><span class="nd">@keys</span>
<span class="n">Y_new</span> <span class="o">=</span><span class="n">values</span><span class="nd">@temp</span><span class="o">.</span><span class="n">T</span>
<span class="n">Y_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 4])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's it. <code>Self-attention</code> refers to performing above operations. We additionally normalize the weights in the <code>temp</code> with <code>softmax</code>. Further, We can also use sets of matrices ($W_k$,$W_q$,$W_v$),essentially replicationg self-attention with different weight matrices. The resulting outputs can be concatenated and be passed through a linear layer to get back the orginal dimension.(This is called <code>multi-head attention</code>)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>


<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">n_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># for n_heads we need the corresponding number of weight matrices of size feat_sz*feat_sz to get new</span>
        <span class="c1">#set of (keys,queries,values),Computationally this can be fused inside a single linear operation.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_keys</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="o">*</span><span class="n">n_heads</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_queries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="o">*</span><span class="n">n_heads</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_values</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="o">*</span><span class="n">n_heads</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">comb_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_heads</span><span class="o">*</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># typically data is fed with features along the `columns`.</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_keys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_queries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_values</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="c1"># `torch.bmm` performs matrix multiplication for a given batch.It is efficient to squeeze n_heads along</span>
        <span class="c1"># with batches and perform the calculation at once.</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span><span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
        <span class="c1">#Rescaling the elements to control the scale</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">dot</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="c1">#reshaping</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="o">*</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="c1"># passing through a linear layer to combine all the heads</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comb_heads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="c1"># gives bs,n_seq,n_heads</span>
        
        <span class="k">return</span> <span class="n">out</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># input with features arranged in columns(shape = [1, 4, 3])</span>

<span class="n">sa</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">feat_sz</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">sa</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span><span class="c1"># simple attention</span>

<span class="n">mha</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">feat_sz</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">n_heads</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">Y_6</span> <span class="o">=</span> <span class="n">sa</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>

<span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">Y_6</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([1, 4, 3]), torch.Size([1, 4, 3]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>Attention is first introduced to deal with sequences in the context of natural language. But, nothing in the above implementation handles <code>order</code>. It just maps a <code>set of vectors</code> to another. To enforce order, we can simply add a <code>position vector</code> to our inputs.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above ideas mark the end of the theoretical minimum. These will be needed only when you are trying to solve any of the interesting applications discussed below with RL.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-RL---Theory">
<a class="anchor" href="#Deep-RL---Theory" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep RL - Theory<a class="anchor-link" href="#Deep-RL---Theory"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Motivation">
<a class="anchor" href="#Motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation<a class="anchor-link" href="#Motivation"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep Learning allows for learning generalizable mappings between input and output. In supervised setting we are given a <code>fixed dataset</code> D = {$(x_{i},y_{i})$},and are tasked to predict $y_{i}$ using $x_{i}$. Thus, we know the groud truth for all input data. This let's us formulate a <code>loss</code> that reflects our dissatisfaction with the predicted <code>outputs</code> and optimize over it. But, consider how we humans learn to perform any new task ? The dataset and the learning signal comes sequentially and is also dependent on our actions. We don't have prepared datasets in real life. We learn from experience. <code>RL</code> deals with learning under this natural setting. The key difference here is that the dataset is not <code>constant</code>, It changes everytime, contingent on your actions and the stochasticity in the environment. To make this clear, consider learning to play a video game. The pixels (call it <code>state</code>) and the score you receive ( call it <code>reward</code>) cannot be predetermined until you actually go through the experience. Here the dataset D = {$(state_{i},reward_{i})$} is not same everytime you play the game. <code>RL</code> provides a formalism for learning optimal decision making. This will become clear when we see some concrete examples and code. But combing these techniques with <code>Neural networks</code> has given us general algorithms that <code>learn to play atari games</code>, <code>beat world champions at Go</code> and <code>train robots to learn simple tasks</code>.
<img src="/images/copied_from_nb/my_icons/rlsuc.png" alt="" title="Credit: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Problem">
<a class="anchor" href="#Problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem<a class="anchor-link" href="#Problem"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the Following optimization problem. Find the shortest path from state $s_0$ to goal $g$,where the edges indicate the cost/distance ?</p>
<p><img src="/images/copied_from_nb/my_icons/sp.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Here taking a greedy approach will fail as actions will have long-term consequences. Solving the above problem efficiently requires realizing that <code>the distance to any node along the shortest path from source to destination is also shortest path</code>. In the above example the shortest path to <code>g</code> should either be through <code>d</code> or <code>f</code>.(one of the incoming edges). Let the shortest distance from a node <code>s</code> to <code>g</code> be given by $v^{*}(s)$. Then the last edge of the shortest path should come from evaluating $v^{*}(f))$ and $v^{*}(d)$, where $v^{*}(f) = 1 + v^{*}(g)$ and $v^{*}(d) = min(3+v^{*}(g),1+v^{*}(f))$. This backtracking or dynamic programming approach of finding one edge at a time is illustrated below.</p>
<p><img src="/images/copied_from_nb/my_icons/op.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Now, for each action we take, let us also add a <code>transition probability</code> that defines the likelihood of ending up in any of the available states given the action. Here, for states <code>c</code> and <code>e</code> we added some randomness. This will let us model more realistic scenarios. Consider an <code>RL agent</code> driving your car. The consequences of any action (eg: Turning the steering to the right given the visual view of the road.) can only be modeled probabilistically.</p>
<p><img src="/images/copied_from_nb/my_icons/ssp.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Here by weighting w.r.t the transition probabilities we can recover $v^{*}()$ for all states. Optimal policy is again achieved by acting greedily w.r.t $v^{*}()$. For example , $v^{*}(c) = min(4+0.3*v^{*}(e)+0.7*v^{*}(d),2+v^{*}(e))$. In RL, we call this <code>Bellmann Equation</code>.</p>
<p><img src="/images/copied_from_nb/my_icons/sspb.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p><img src="/images/copied_from_nb/my_icons/be.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Modern <code>Deep RL</code> deals with solving this stochastic version of the problem when the transition probabilities are not available and the number of possible states is very large. Consider the following video game playing scenario. The pixels on the screen at any timestamp can be taken as <code>state</code>. The game rules provides list of <code>possible actions</code> and the corresponding <code>reward</code>(increase in score).</p>
<p><img src="/images/copied_from_nb/my_icons/vid.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>By the end of this module we will develop all the machinery to understand the algorithms that learn optimal(or good) policies for this general case.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bellmann-Equation-and-MDP's">
<a class="anchor" href="#Bellmann-Equation-and-MDP's" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bellmann Equation and MDP's<a class="anchor-link" href="#Bellmann-Equation-and-MDP's"> </a>
</h3>
<p>In this section we will devolop some theory. First let's define an object called Markov Decision Process (MDP),given by the tuple of $(S,A,P,R,\gamma)$. Here discounting factor $\gamma$ is largely a mathematical convenience as it helps to bound the <code>total reward</code>. Any reward $r_t$ received at timestamp <code>t</code> is multiplied by $\gamma^{t-1}$, that is we prefer immediate rewards over faraway ones. The term <code>Markov</code> refers to the fact that given present state <code>s</code> and action taken from there <code>a</code>, next state is independent of the past trajectory. Consider the example of <code>stochastic shortest path</code> but with the goal state <code>g</code> infinitely far away. In that setting we want to maximize the average reward we will get starting from any state.</p>
<p>The only <code>knob</code> agent has is policy $\pi : S \rightarrow A$.(which of the possible actions to take). The environment dynamics (P and R) are not under agent's control. For simplicity,let's assume that rewards are bounded and positive. Let's say that the agent has a policy $\pi$ and starts to intreract with the environment. The value function <code>v(s)</code> refers to the average reward starting from state <code>s</code> and following policy $\pi$. Then <code>v(s)</code> for all the states satisfies a recursive definition as shown below. 
<img src="/images/copied_from_nb/my_icons/bellmann.png" alt=""></p>
<p>Thus finding $v^{\pi}(s)$ amounts to solving system of linear equation or in matrix notation finding the inverse of a matrix. But, we still need the proof for the existence of the inverse for $I - \gamma.P^{\pi}$. Before going further, it's important to thoroughly understand the bellmann equation : $V^{\pi} = R^{\pi} + \gamma.P^{\pi}.V^{\pi}$. The deriviation involves observing the fact that once the agent takes initial action from state <code>s</code>, the transition probability will dictate it's next state <code>s'</code>. From there, the average reward by definition is  given by <code>v(s')</code>, leaving a recursive definition.</p>
<p><img src="/images/copied_from_nb/my_icons/matform.png" alt=""></p>
<p>Below i have give the proof for the existence of inverse. If we recall, a matrix A($n * n$) multiplied by a column vector X ($n * 1$) merely takes a linear combination of all the column vectors in A. Existence of inverse to <code>A</code> means that there doesnot exist a non-zero vector X, that can collapse A to a null vector. Using this fact and the traingular inequality on $I - \gamma.P^{\pi}$ completes the proof.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/copied_from_nb/my_icons/proof.jpg" alt=""></p>
<h3 id="Search-for-optimal-State-Values.">
<a class="anchor" href="#Search-for-optimal-State-Values." aria-hidden="true"><span class="octicon octicon-link"></span></a>Search for optimal State Values.<a class="anchor-link" href="#Search-for-optimal-State-Values."> </a>
</h3>
<p>Now let's define $v^{*}(s)$ as the maximum expected reward that we can get from state <code>s</code> under <code>any policy</code>. 
Note that the above equation is defined on a particular policy or when the action from each state is fixed or if state transition probabilities are independent of <code>action taken</code>. Once we are able to extract these values, optimal policy becomes obvious. <code>Value Iteration</code> methods try to apprimate this $V^{*}(s)$. They start with some arbitrary function like $f(s) = 0 \forall s$ and iteratively bring $f$ closer to $V^{*}$. Let's also define $Q^{\pi}(s,a)$ as the expected reward under the policy $\pi$,when we take action <code>a</code> at state <code>s</code> and subsequently sample actions according to $\pi$. Here $\pi$ is just a function that takes state <code>s</code> and action <code>a</code> and returns the corresponding probability <code>a</code> for taking that action. Similarly $Q^{*}(s,a)$ is also defined as the maximum Q that can be achieved under any policy. We often want $Q^{*}$ values over $V^{*}$ as simply choosing greedily w.r.t $Q^{*}$ gives the optimal policy. To see the advantage more clearly, Imagine an oracle that can give you $V^{*}$ for any <code>s</code>. Now, the agent starts at state <code>s</code> and is looking to take optimal action. It first has to take all possible actions from that state to see where it would end up. And for each subsequent state <code>s'</code> , it has to query the oracle to get $V^{*}(s')$. Only after this we can determine the best action from initial state s as $max_{a\in A}(r_1 + v^{*}(s'))$. But having $Q^{*}$ values let's us choose this action  directly by evaluating $Q_{a\in A}^{*}(s,a)$. There exists a proof that for discounted infinite horizon MDP's there exists a stationary and deterministic optimal policy for all states simultaneously. Let's call this $\pi^{*}$. It is easy to see that both $V^{*}$ and $Q^{*}$ will also satisfy a similar recursive relation called <code>Bellmann Optimality equations</code>.</p>
<p><img src="/images/copied_from_nb/my_icons/b.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Value-Iteration">
<a class="anchor" href="#Value-Iteration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Value Iteration<a class="anchor-link" href="#Value-Iteration"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Till now we have seen how we can solve for <code>V</code> for all states given a policy. But, Our objective is to find <code>V*</code> - the maximum return that can be achieved under any policy. The claim is that if we start with random values of <code>Q</code> and then do the following -</p>
<ul>
<li>Act greedily with respect to these values.</li>
<li>Update these <code>Q</code> values using bellmann update rule.</li>
<li>Repeat the process for some large number of times - H </li>
</ul>
<p>Will give us a policy $Q^{*,H}$(greedy policy w.r.t Q values after H Steps) that is close to true Optimal Policy. In what follows is the proof of the claim.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/copied_from_nb/my_icons/b2.jpg" alt=""></p>
<p><img src="/images/copied_from_nb/my_icons/vi1.jpg" alt=""></p>
<p><img src="/images/copied_from_nb/my_icons/vi2.jpg" alt=""></p>
<p><img src="/images/copied_from_nb/my_icons/vi3.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Policy-Iteration-(PI)">
<a class="anchor" href="#Policy-Iteration-(PI)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Policy Iteration (PI)<a class="anchor-link" href="#Policy-Iteration-(PI)"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also have an alternative algorithm that also converges to optimal policy called Policy Iteration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Note</code> : PI strictly converges to optimal policy after some steps which is not true for VI as it only get's closer but never quite equals.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following derivation requires some explanation. First Here's the claim of policy iteration algorithms.</p>
<ul>
<li>Start out with random policy.</li>
<li>Evaluate $Q^{\pi_{0}}$ using bellmann update rule. (BOX 1). </li>
<li>Greedy policy w.r.t to new Q values becomes your new policy.</li>
</ul>
<p>Performing above steps for large number of iterations would give us Optimal Policy  - <code>Claim.</code> (In fact the claim is more subtle as given by <code>Policy Improvement Theorem</code>.</p>
<p><code>Policy Improvement Theorem</code>: As given in below slide suggests that after every round of PI, the new policy has higher(or equal) <code>V</code> compared to old <code>for all states</code>.</p>
<p>The proof involves manipulating $\tau^{\pi}$ (bellmann operator). We start out by consicely writing down PI algorithm -</p>
<p>$Q^{\pi_{k}} = \tau^{\pi_{k}}.Q^{\pi_{k}}$</p>
<p>Note that bellmann optimality operator $\tau$(involves taking <code>max</code> in next state) is defferent from bellmann operator $\tau^{\pi}$. And substituting the latter with former would always result in a value higher or equal by definition. Since according to PI algorithm the new policy is the Greedy one w.r.t updated Q values, we have  - 
$\tau.Q^{\pi_{k}} = \tau^{\pi_{k+1}}.Q^{\pi_{k}}$. Later by recursively expanding $Q^{\pi_{k}}$, we reach the fixed point of the <code>bellmann operator</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/copied_from_nb/my_icons/pi1.jpg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Alternative and more Intuitive proof:</li>
</ul>
<p><img src="/images/copied_from_nb/my_icons/pia1.jpeg" alt=""></p>
<p><img src="/images/copied_from_nb/my_icons/pia2.jpeg" alt=""></p>
<p><img src="/images/copied_from_nb/my_icons/pia3.jpeg" alt=""></p>
<p><img src="/images/copied_from_nb/my_icons/pia4.jpeg" alt=""></p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="vin136/vinay-varma"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/research/2021/02/21/Deep-Learning-Useful-Ideas.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Thoughts on Science and Life.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/vin136" target="_blank" title="vin136"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/vinnuvinay008" target="_blank" title="vinnuvinay008"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
