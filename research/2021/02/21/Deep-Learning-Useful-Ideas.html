<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Learning | Home</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Deep Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A catalogue of not so deep ideas in Deep Learning." />
<meta property="og:description" content="A catalogue of not so deep ideas in Deep Learning." />
<link rel="canonical" href="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:url" content="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:site_name" content="Home" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-21T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Deep Learning","dateModified":"2021-02-21T00:00:00-06:00","datePublished":"2021-02-21T00:00:00-06:00","description":"A catalogue of not so deep ideas in Deep Learning.","mainEntityOfPage":{"@type":"WebPage","@id":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html"},"@type":"BlogPosting","url":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://vinayvarma.work/feed.xml" title="Home" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Learning | Home</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Deep Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A catalogue of not so deep ideas in Deep Learning." />
<meta property="og:description" content="A catalogue of not so deep ideas in Deep Learning." />
<link rel="canonical" href="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:url" content="https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html" />
<meta property="og:site_name" content="Home" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-21T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Deep Learning","dateModified":"2021-02-21T00:00:00-06:00","datePublished":"2021-02-21T00:00:00-06:00","description":"A catalogue of not so deep ideas in Deep Learning.","mainEntityOfPage":{"@type":"WebPage","@id":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html"},"@type":"BlogPosting","url":"https://vinayvarma.work/research/2021/02/21/Deep-Learning-Useful-Ideas.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://vinayvarma.work/feed.xml" title="Home" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Home</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Learning</h1><p class="page-description">A catalogue of not so deep ideas in Deep Learning.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-21T00:00:00-06:00" itemprop="datePublished">
        Feb 21, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      16 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Research">Research</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/vin136/vinay-varma/tree/master/_notebooks/2021-02-21-Deep-Learning-Useful-Ideas.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/vin136/vinay-varma/master?filepath=_notebooks%2F2021-02-21-Deep-Learning-Useful-Ideas.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/vin136/vinay-varma/blob/master/_notebooks/2021-02-21-Deep-Learning-Useful-Ideas.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Basics">Basics </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Train-a-linear-regression-model-using-gradient-descent.">Train a linear regression model using gradient-descent. </a></li>
<li class="toc-entry toc-h3"><a href="#Adding-Inductive-biases---convolutions-and-recurrent-networks.">Adding Inductive biases - convolutions and recurrent networks. </a></li>
<li class="toc-entry toc-h3"><a href="#Self-Attention">Self-Attention </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Deep-RL-Methods">Deep RL Methods </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Motivation">Motivation </a></li>
<li class="toc-entry toc-h3"><a href="#Problem">Problem </a></li>
<li class="toc-entry toc-h3"><a href="#Bellmann-Equation-and-MDP's">Bellmann Equation and MDP&#39;s </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-21-Deep-Learning-Useful-Ideas.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep Learning is a collection of techniques for finding functions that can approximate arbitrary input-output mappings, while capturing enough structure of the problem to be able to use them in practical applicatons.language-translation systems,Image classification,Movie Recommendation systems are Some notable applications where they have become defacto-choice. This is by no means an exhaustive treatment of the field. Here I provide concise summaries,followed by a simple implementation of some of the most interesting ideas in the field. I believe that many things in the field are unnecessarily complicated by lengthy treatment where a readable code and short explanation will suffice.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Note</code> : This is not intended to be the first introduction to deep learning. Here I wanted to succintly catalogue some the latest advancements in the field. But, respecting tradition the first module starts from the basics. Only hard prerequisite is to have a good intuition of <code>matrix multiplication</code> notion of <code>taking a derivative</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Basics">
<a class="anchor" href="#Basics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Basics<a class="anchor-link" href="#Basics"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Many deep learning models follow a simple recipe:</p>

<pre><code>1. Gather the data.
2. Define learnable parameters. And specify how they will interact with the data.(architecture)
3. Define a loss function to minimize.
4. Adjust the parameters until satisfied.</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Train-a-linear-regression-model-using-gradient-descent.">
<a class="anchor" href="#Train-a-linear-regression-model-using-gradient-descent." aria-hidden="true"><span class="octicon octicon-link"></span></a>Train a linear regression model using gradient-descent.<a class="anchor-link" href="#Train-a-linear-regression-model-using-gradient-descent."> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we will see how we can perform all the above steps starting with the most barebones implementation. Note that the procedure outlined here is general purpose - meaning the way we adjust <code>parameters</code> is going to remain same irrecpective of the modality of the data, details of the loss function or the architecture.</p>
<p>Step 1. Gather the data</p>
<p>Let's generate some fake data.Let's assume that the data is coming from $y = 2*x1 - 4.2*x2 + 1 + noise(measurement error)$. This can be more succintly represented in vector notation :

$$y = \begin{bmatrix} x1 \\ x2 \end{bmatrix} . \begin{bmatrix} 2 \\ -4.2 \end{bmatrix} + 1$$
</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">,</span><span class="n">const</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">rows</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1">#number of features in the input</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,(</span><span class="n">rows</span><span class="p">,</span><span class="n">dim</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="nd">@np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="n">const</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">const</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>

<span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mf">4.2</span><span class="p">,</span><span class="n">const</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((1000, 2), (1000, 1))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Step 2. Define learnable parameters. And specify how they will interact with the data.(architecture)</p>
<p>Now we aim to learn the right coefficients to approximate the data generation process. First let's look at some code.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Start with a random guess that respects the sanctity of the data.i.e our inputs are of dimension 1000*2 </span>
<span class="c1"># outputs are 1000*1. Multiplying inputs by a 2*1 matrix(weights) and adding a constant(bias) is the simplest way</span>
<span class="c1"># to ensure an output of 1000*1. </span>

<span class="c1"># initial guess</span>
<span class="n">init_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">]])</span> <span class="c1">#shape -&gt; 1*2</span>
<span class="n">init_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>

<span class="c1">#expected output</span>
<span class="k">def</span> <span class="nf">give_expected_output</span><span class="p">(</span><span class="n">inpt</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">inpt</span><span class="nd">@weights</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">give_expected_output</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">)</span><span class="c1">#shape -&gt; 1000*1</span>

<span class="k">def</span> <span class="nf">get_error</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">expected_out</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">expected_out</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">get_error</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># IF WE CAN DRIVE THIS NUMBER DOWN TO ZERO VIA A GENERAL PURPOSE PROCESS,WE ARE GOOD TO GO</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2.121630476687219</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_grads</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">loss_func</span><span class="o">=</span><span class="s1">'squared_loss'</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">loss_func</span> <span class="o">==</span> <span class="s1">'squared_loss'</span><span class="p">:</span>
        <span class="n">weights_grad</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x</span><span class="nd">@weights</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">bias_grad</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x</span><span class="nd">@weights</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Sorry I'm not yet scalable enough for arbitrary loss functions"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights_grad</span><span class="p">,</span><span class="n">bias_grad</span>
        


<span class="n">grad_init_weights</span><span class="p">,</span><span class="n">grad_bias</span> <span class="o">=</span> <span class="n">get_grads</span><span class="p">(</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">loss_func</span> <span class="o">=</span> <span class="s1">'squared_loss'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">loss_func</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">give_expected_output</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="c1">#print(f'initial error, epoch 0: {error}')</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">[</span><span class="n">error</span><span class="p">]</span>
    <span class="n">pres_lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">weight_grad</span><span class="p">,</span><span class="n">bias_grad</span> <span class="o">=</span> <span class="n">get_grads</span><span class="p">(</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">init_weights</span> <span class="o">-=</span> <span class="n">weight_grad</span><span class="o">*</span><span class="n">pres_lr</span>
        <span class="n">init_bias</span> <span class="o">-=</span> <span class="n">bias_grad</span><span class="o">*</span><span class="n">pres_lr</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">give_expected_output</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">weight_grad</span><span class="p">)</span><span class="o">&lt;</span><span class="mf">0.0001</span><span class="p">:</span>
            <span class="n">pres_lr</span> <span class="o">=</span> <span class="n">pres_lr</span><span class="o">*</span><span class="mi">2</span>
        <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">errors</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">errors</span><span class="p">,</span><span class="n">final_weights</span><span class="p">,</span><span class="n">final_bias</span> <span class="o">=</span> <span class="n">learn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">init_weights</span><span class="p">,</span><span class="n">init_bias</span><span class="p">,</span><span class="n">get_error</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&lt;matplotlib.lines.Line2D at 0x7f2be3ea9df0&gt;]</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">final_weights</span><span class="p">,</span><span class="n">final_bias</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(array([[-0.82604061, -1.82604061]]), array([0.97582166]))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># In the above code what happens if 'lr' is not dynamically adjusted.</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Adding-Inductive-biases---convolutions-and-recurrent-networks.">
<a class="anchor" href="#Adding-Inductive-biases---convolutions-and-recurrent-networks." aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding Inductive biases - convolutions and recurrent networks.<a class="anchor-link" href="#Adding-Inductive-biases---convolutions-and-recurrent-networks."> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the below feedforward network each neuron is connected to all the neurons in the previous layer. No inductive biases in the connectivity.</p>
<p><img src="/images/copied_from_nb/my_icons/fc.png" alt="FeedForward" title="Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/"></p>
<p><code>Locality</code> : For almost all natural signals it is easier to predict the future using recent past compared to any earlier versions. <code>Locality</code> allows for the sparsity of weights. We can put faraway weights to zero. In the below figure the <code>15</code> weights of the first layer is reduced to <code>9</code>. It's also important to be aware of the concept of <code>receptive field</code>(RF). <code>RF</code> of layer <code>a</code> w.r.t <code>b</code> is simply the number of neurons in <code>a</code> that influence the outputs of layer b.</p>
<p><img src="/images/copied_from_nb/my_icons/sparsity.png" alt="" title="Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/"></p>
<p><code>Stationarity</code> : Same patterns are repeated again and again.
We don't need connections from the inputs far down. <code>Stationarity</code> implies weight sharing.</p>
<p><img src="/images/copied_from_nb/my_icons/parashare.png" alt="" title="Credit: https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/"></p>
<p><code>Compositionality</code>: There are hierarchies. Letters make up words,words make up sentences and so on. <code>Compisitionality</code> implies deeper networks.</p>
<p>Now We will see how popular building blocks like CNN'S and RNN'S leverage these properties. <code>CNN</code>'s are a linear layer with lots of <code>weight sharing</code> and <code>sparsity</code>. RNN's just use <code>weight sharing</code> but BPTT takes the <code>locality</code> into account.</p>
<p>CNN  = linear layer + weight sharing + sparsity.</p>
<p>Consider convolving over a 4 <em> 4 inputs with a 3 </em> 3 kernel with a unit stride as presented below.
<img src="/images/copied_from_nb/my_icons/cnn.png" alt="" title="Credit:https://arxiv.org/pdf/1603.07285.pdf"></p>
<p>If I stack the 2-d input into a 1-d vector by unrolling left to right and top to bottom, the convolution can be represented as a matrix multiplication.
<img src="/images/copied_from_nb/my_icons/conv_mat.png" alt="" title="Credit:https://arxiv.org/pdf/1603.07285.pdf"></p>
<p>The <code>zeros</code> along the columns encode <code>locality</code> while the replication of same weights along the rows account for <code>stationarity</code>. If the above properties doesn't make sense for your input , then CNN's aren't the right choice.</p>
<p>RNN = linear layer + weight sharing + BPTT(Back-prop through time)</p>
<p><img src="/images/copied_from_nb/my_icons/rnn.png" alt="" title="Credit:https://atcold.github.io/pytorch-Deep-Learning/en/week06/06-3/"></p>
<p>RNN's are used for sequence data. In the above figure the <code>arrow</code> indicates matrix multiplication. The hidden state <code>h(t)</code> at time <code>t</code> is equal to Affine_transform(x(t)) + Affine_transform(h(t-1)).(<code>Note</code>: Affine_transform refers to matrix multiplication). The following code will makes it all clear.</p>
<p>Consider the following sequence :</p>
<p>'Hey Jude, don't make it bad.</p>
<p>Take a sad song and make it better.</p>
<p>Remember to let her into your heart,</p>
<p>Then you can start to make it better.
'</p>
<p>Now we would want to classify it into either positive or negative sentiment. Ideally we would have a collection of such sequences with their corresponding labels. Here note that the number of words in each sequence need not be same. A naive approach would be to string all the words in a sequence to a single column,and run it through a fullyconnected network.(variable sequence length still poses a problem.) Let's see how an RNN can accomplish this with much less parameters.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i_sz</span><span class="p">,</span><span class="n">h_sz</span><span class="p">,</span><span class="n">out_sz</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">i_sz</span> <span class="o">=</span> <span class="n">i_sz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span> <span class="o">=</span> <span class="n">h_sz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_sz</span> <span class="o">=</span> <span class="n">out_sz</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">i_sz</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_sz</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">out_sz</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">emb_sz</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># This just adds the hidden representation which is a function of all the words fed until t-1 to the </span>
            <span class="c1">#word at t</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_hidden</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:])</span>
            <span class="c1"># Stores the hidden representation for next  word in seq.</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_hidden</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_output</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">101</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">vector_len</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">seq_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">vector_len</span><span class="p">)</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">i_sz</span> <span class="o">=</span> <span class="n">vector_len</span><span class="p">,</span><span class="n">h_sz</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">out_sz</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">seq_1</span><span class="p">)</span>

<span class="n">out</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([101, 1])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This can be seen as a two layer network.The last layer converts <code>h_sz</code> to the output dimension, while the first layer maps <code>i_sz</code> to <code>h_sz</code>. But, the first layer only uses weight matrices of size 100 <em> 20,20 </em> 20. Totalling of around 2400 parameters. Our naive version would have seq_len <em> i_sz </em> h parameters.(around 20000). Moreover, In RNN number of paramenters is independent of
sequence length.</p>
<p>The above model is just an instantiation of <code>weight sharing</code> for sequential data. We haven't still leveraged the <code>locality</code> aspect (<code>sparsity</code>).</p>
<p>Moreover,eventhough we only have 3 different weight matrices,the actual number of layers is proportional to the <code>size</code> of the <code>for loop</code>. Aside from being very slow and memory intensive, gradients of loss w.r.t initial operations( i = 0) very unlikely to be stable.(According to the chain rule of derivatives the gradient of loss w.r.t first matrix multiplication would involve multiplying atleast <code>seq_len</code> of partial derivatives. The resultant can easily explode or vanish.) What if we only take gradients for the last <code>n</code> operations. This is also called <code>Truncated BPTT</code>. Here's the modified <code>forward</code> function.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">emb_sz</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># This just adds the hidden representation which is a function of all the words fed until t-1 to the </span>
            <span class="c1">#word at t</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_hidden</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:])</span>
            <span class="c1"># Stores the hidden representation for next  word in seq.</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_hidden</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="k">3</span> == 0:
                <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_output</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This just flushes the memory for the backward pass after every 3 steps. Aside from solving obvious practical problems,this also has regularizing effects. We are implicitly encoding our bias - you need not look past the last 3 points in the sequence - a.k.a <code>locality</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Self-Attention">
<a class="anchor" href="#Self-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Self-Attention<a class="anchor-link" href="#Self-Attention"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider a sequence of vectors ($x_1$,$x_2$..$x_n$). It helps to imagine them as vectors corresponding to sequence of words. If you can bear with me,the following operation converts them into another sequence of vectors ($y_1$,$y_2$..$y_n$) of same dimension.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Two column vectors(each with dimension of 3*1)</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.tensor</span> <span class="k">as</span> <span class="nn">tensor</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">],[</span><span class="mf">3.</span><span class="p">]]),</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">4.</span><span class="p">],[</span><span class="mf">5.</span><span class="p">],[</span><span class="mf">6.</span><span class="p">]]),</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">7.</span><span class="p">],[</span><span class="mf">8.</span><span class="p">],[</span><span class="mf">9.</span><span class="p">]]),</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">10.</span><span class="p">],[</span><span class="mf">11.</span><span class="p">],[</span><span class="mf">12.</span><span class="p">]])],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># a random matrix</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@matrix</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([3, 4]), torch.Size([4, 4]), torch.Size([3, 4]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now,let's generate the same <code>V</code> with a fancier set of operations.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span>
<span class="n">Y_new</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@temp</span><span class="o">.</span><span class="n">T</span>
<span class="n">Y_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 4])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here <code>temp.T</code> is acting as <code>matrix</code>. This also removes the need for additional initialization. This operation also lends to the following intution:</p>
<ul>
<li>
<code>temp</code> is the dot product of each column vector in U with all the vector within it. The captures the measure of similarity between the vectors.</li>
<li>
<p><code>Y_new</code> is just a linear combination of U with the corresponding weights from the <code>temp</code>.</p>
</li>
<li>
<p>In other words, $y_i =\sum_{j} w_{ij}.x_{j}$ where <code>w</code>'s are taken frow the rows of <code>temp</code>.</p>
</li>
</ul>
<p>In the above figure each vector $x_{i}$ is used three times. Let's take $x_{2}$ for illustration:</p>
<ul>
<li>To get $w_{22}$</li>
<li>Similarly to get weight's required for all the other outputs $y_1$,$y_3$ and $y_4$ </li>
<li>$x_2$ is also used in linear weighting with <code>w</code>'s to get $y_2$</li>
</ul>
<p>But, In the whole compution we are not learning any weights. Everything is being generated from the input. We can introduce three different set of <code>x</code>'s for each of the above operations. Let's initialize three square matrices $W_k$,$W_q$,$W_v$, each of size (4,4).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">w_k</span><span class="p">,</span><span class="n">w_q</span><span class="p">,</span><span class="n">w_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># learnable parameters</span>

<span class="n">keys</span><span class="p">,</span><span class="n">queries</span><span class="p">,</span><span class="n">values</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@w_k</span><span class="p">,</span><span class="n">X</span><span class="nd">@w_q</span><span class="p">,</span><span class="n">X</span><span class="nd">@w_v</span>
<span class="c1"># This is the naming convention used in the literature.</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">T</span><span class="nd">@keys</span>
<span class="n">Y_new</span> <span class="o">=</span><span class="n">values</span><span class="nd">@temp</span><span class="o">.</span><span class="n">T</span>
<span class="n">Y_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([3, 4])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's it. <code>Self-attention</code> refers to performing above operations. We additionally normalize the weights in the <code>temp</code> with <code>softmax</code>. Further, We can also use sets of matrices ($W_k$,$W_q$,$W_v$),essentially replicationg self-attention with different weight matrices. The resulting outputs can be concatenated and be passed through a linear layer to get back the orginal dimension.(This is called <code>multi-head attention</code>)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>


<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">n_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># for n_heads we need the corresponding number of weight matrices of size feat_sz*feat_sz to get new</span>
        <span class="c1">#set of (keys,queries,values),Computationally this can be fused inside a single linear operation.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_keys</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="o">*</span><span class="n">n_heads</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_queries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="o">*</span><span class="n">n_heads</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_values</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="o">*</span><span class="n">n_heads</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">comb_heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_heads</span><span class="o">*</span><span class="n">feat_sz</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># typically data is fed with features along the `columns`.</span>
        <span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_keys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_queries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_values</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="c1"># `torch.bmm` performs matrix multiplication for a given batch.It is efficient to squeeze n_heads along</span>
        <span class="c1"># with batches and perform the calculation at once.</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span><span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
        <span class="c1">#Rescaling the elements to control the scale</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">dot</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="c1">#reshaping</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span><span class="n">n_seq</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="o">*</span><span class="n">feat_sz</span><span class="p">)</span>
        <span class="c1"># passing through a linear layer to combine all the heads</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comb_heads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="c1"># gives bs,n_seq,n_heads</span>
        
        <span class="k">return</span> <span class="n">out</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># input with features arranged in columns(shape = [1, 4, 3])</span>

<span class="n">sa</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">feat_sz</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">sa</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span><span class="c1"># simple attention</span>

<span class="n">mha</span> <span class="o">=</span> <span class="n">SelfAttention</span><span class="p">(</span><span class="n">feat_sz</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">n_heads</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">Y_6</span> <span class="o">=</span> <span class="n">sa</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>

<span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">Y_6</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([1, 4, 3]), torch.Size([1, 4, 3]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>Attention is first introduced to deal with sequences in the context of natural language. But, nothing in the above implementation handles <code>order</code>. It just maps a <code>set of vectors</code> to another. To enforce order, we can simply add a <code>position vector</code> to our inputs.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above ideas mark the end of the theoretical minimum needed. Below you would find the notched up version of the above content by solving simple but practical problems end-end.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-RL-Methods">
<a class="anchor" href="#Deep-RL-Methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep RL Methods<a class="anchor-link" href="#Deep-RL-Methods"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Motivation">
<a class="anchor" href="#Motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation<a class="anchor-link" href="#Motivation"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep Learning allows for learning generalizable mappings between input and output. In supervised setting we are given a <code>fixed dataset</code> D = {$(x_{i},y_{i})$},and are tasked to predict $y_{i}$ using $x_{i}$. Thus, we know groud truth for all the input data. This let's us formulate a <code>loss</code> that reflects our dissatisfaction with the predicted <code>outputs</code> and optimize over it. But, consider how we learn to perform any new task ? The dataset and the learning signal comes sequentially and is also dependent on our actions. We don't have prepared datasets in real life. We learn from experience. <code>RL</code> deals with learning under this natural setting. The key difference to note is that the dataset is not <code>constant</code>, It changes everytime, contingent on your actions and the stochasticity in the environment. To make this clear, consider learning to play a video game. The pixels (call it <code>state</code>) and the score you receive ( call it <code>reward</code>) cannot be predetermined until you actually go through the experience. Here the dataset D = {$(state_{i},reward_{i})$} is not same everytime you play the game. <code>RL</code> provides a formalism for learning optimal decision making. This will become clear when we see some concrete examples and code. But combing these techniques with <code>Neural networks</code> has given us general algorithms that <code>learn to play atari games</code>, <code>beat world champions at Go</code> and <code>train robots to learn simple tasks</code>.
<img src="/images/copied_from_nb/my_icons/rlsuc.png" alt="" title="Credit: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Problem">
<a class="anchor" href="#Problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem<a class="anchor-link" href="#Problem"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the Following optimization problem. Find the shortest path from state $s_0$ to goal $g$,where the edges indicate the cost/distance ?</p>
<p><img src="/images/copied_from_nb/my_icons/sp.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Here taking a greedy approach will fail as actions will have long-term consequences. Solving the above problem efficiently requires realizing that <code>the distance to any node along the shortest path from source to destination is also shortest path</code>. In the above example the shortest path to <code>g</code> should either be through <code>d</code> or <code>f</code>.(one of the incoming edges). Let the shortest distance from a node <code>s</code> to <code>g</code> be given by $v^{*}(s)$. Then the last edge of the shortest path should come from evaluating $v^{*}(f))$ and $v^{*}(d)$, where $v^{*}(f) = 1 + v^{*}(g)$ and $v^{*}(d) = min(3+v^{*}(g),1+v^{*}(f))$. This backtracking or dynamic programming approach of finding one edge at a time is illustrated below.</p>
<p><img src="/images/copied_from_nb/my_icons/op.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Now, for each action we take, let us also add a <code>transition probability</code> that defines the likelihood of ending up in any of the available states given the action. Here, for states <code>c</code> and <code>e</code> we added some randomness. This will let us model more realistic scenarios. Consider an <code>RL agent</code> driving your car. The consequences of any action (eg: Turning the steering to the right given the visual view of the road.) can only be modeled probabilistically.</p>
<p><img src="/images/copied_from_nb/my_icons/ssp.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Here by weighting w.r.t the transition probabilities we can recover $v^{*}()$ for all states. Optimal policy is again achieved by acting greedily w.r.t $v^{*}()$. For example , $v^{*}(c) = min(4+0.3*v^{*}(e)+0.7*v^{*}(d),2+v^{*}(e))$. In RL, we call this <code>Bellmann Equation</code>.</p>
<p><img src="/images/copied_from_nb/my_icons/sspb.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p><img src="/images/copied_from_nb/my_icons/be.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>Modern <code>Deep RL</code> deals with solving this stochastic version of the problem when the transition probabilities are not available and the number of possible states is very large. Consider the following video game playing scenario. The pixels on the screen at any timestamp can be taken as <code>state</code>. The game rules provides list of <code>possible actions</code> and the corresponding <code>reward</code>(increase in score).</p>
<p><img src="/images/copied_from_nb/my_icons/vid.png" alt="" title="Credit: https://nanjiang.cs.illinois.edu/files/cs598/slides_intro_f20.pdf"></p>
<p>By the end of this module we will develop all the machinery to understand the algorithms that learn optimal(or good) policies for this general case.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bellmann-Equation-and-MDP's">
<a class="anchor" href="#Bellmann-Equation-and-MDP's" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bellmann Equation and MDP's<a class="anchor-link" href="#Bellmann-Equation-and-MDP's"> </a>
</h3>
<p>In this section we will devolop some theory. First let's define an object called Markov Decision Process (MDP),given by the tuple of $(S,A,P,R,\gamma)$. Here discounting factor $\gamma$ is largely a mathematical convenience as it helps to bound the <code>total reward</code>. Any reward $r_t$ received at timestamp <code>t</code> is multiplied by $\gamma^{t-1}$, that is we prefer immediate rewards over faraway ones. The term <code>Markov</code> refers to the fact that given present state <code>s</code> and action taken from there <code>a</code>, next state is independent of the past trajectory. Consider the example of <code>stochastic shortest path</code> but with the goal state <code>g</code> infinitely far away. In that setting we want to maximize the average reward we will get starting from any state.</p>
<p>The only <code>knob</code> agent has is policy $\pi : S \rightarrow A$.(which of the possible actions to take). The environment dynamics (P and R) are not under agent's control. For simplicity,let's assume that rewards are bounded and positive. Let's say that the agent has a policy $\pi$ and starts to intreract with the environment. The value function <code>v(s)</code> refers to the average reward starting from state <code>s</code> and following policy $\pi$. Then <code>v(s)</code> for all the states satisfies a recursive definition as shown below.</p>
<p>Thus finding $v^{\pi}(s)$ amounts to solving system of linear equation or in matrix notation finding the inverse of a matrix. But, we still need the proof for the existence of the inverse for $I - \gamma.P^{\pi}$. Before going further, it's important to thoroughly understand the bellmann equation : $V^{\pi} = R^{\pi} + \gamma.P^{\pi}.V^{\pi}$. The deriviation involves observing the fact that once the agent takes initial action from state <code>s</code>, the transition probability will dictate it's next state <code>s'</code>. From there, the average reward by definition is  given by <code>v(s')</code>, leaving a recursive definition.</p>
<p>Below i have give the proof for the existence of inverse. If we recall, a matrix A($n * n$) multiplied by a column vector X ($n * 1$) merely takes a linear combination of all the column vectors in A. Existence of inverse to <code>A</code> means that there doesnot exist a non-zero vector X, that can collapse A to a null vector. Using this fact and the traingular inequality on $I - \gamma.P^{\pi}$ completes the proof.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="vin136/vinay-varma"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/research/2021/02/21/Deep-Learning-Useful-Ideas.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Thoughts on Science and Life.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/vin136" title="vin136"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/vinnuvinay008" title="vinnuvinay008"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
